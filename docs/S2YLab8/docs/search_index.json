[["index.html", "Lab 8 - Exploring Model Selection 1 Welcome to Lab 8! 1.1 Introduction to model selection 1.2 Stepwise Testing 1.3 Criterion-based approach", " Lab 8 - Exploring Model Selection 1 Welcome to Lab 8! Intended Learning Outcomes: Select predictors using backward elimination, forward selection, and stepwise regression; Select predictors based on optimality criteria, such as \\(R^2_a\\) (\\(R^2\\) adjusted) and AIC. 1.1 Introduction to model selection In this lab, we will consider model selection to refer to variable/predictor selection. The process of selection should answer the question: what are the best predictors in my model? Some might say, the more the merrier. However, models with too many predictors are challenging for many reasons: they can be difficult to manage, assess, compute, and interpret. In general, statisticians prefer parsimonious models: the simplest possible model that does the best job. But... how will we know what is the best model? We consider two basic approaches to model selection: Stepwise testing: strategies that compare models with small differences Criterion-based approach: results in the model that optimises some measure of goodness-of-fit 1.2 Stepwise Testing Backward Elimination Backward elimination begins with a model containing all potential predictors and identifies the one with the largest \\(p\\)-value. This can be done by looking at the \\(p\\)-values for the \\(t\\)-statistics of the \\(\\hat{\\beta}_i, i = 1,...,p-1\\) coefficients using the function summary() or for the \\(F\\)-statistics using the function drop1(). If the variable with the largest \\(p\\)-value is above a predetermined critical value, \\(\\alpha_{\\text{crit}}\\), that variable is dropped. A model with the remaining variables is then fit and the procedure continues until all the \\(p\\)-values for the remaining variables in the model are below \\(\\alpha_{\\text{crit}}\\). In summary, the process is     Step 1: Fit model. For the first iteration, fit a model with all predictor variables. In later iterations, fit the latest selection of variables.     Step 2: Evaluate. Are there variables with \\(p\\)-values larger than \\(\\alpha_{\\text{crit}}\\)? If so, remove the variable with the largest \\(p\\)-value.     Step 3: Re-fit the model. Repeat Steps 1 and 2 until no variables have \\(p\\)-values larger than \\(\\alpha_{\\text{crit}}\\). Forward Selection Forward selection starts with no variables in the model and then adds the variable that produces the smallest \\(p\\)-value below \\(\\alpha_{\\text{crit}}\\) when included in the model. This procedure is continued until no new predictors can be added. A summary of the steps is:     Step 1: Fit model. For the first iteration, fit a model with no predictor variables, only the intercept. In later iterations, fit the latest selection of variables.     Step 2: Evaluate. Are there variables with \\(p\\)-values smaller than \\(\\alpha_{\\text{crit}}\\) if included in the model? If so, add the variable with the smallest \\(p\\)-value.     Step 3: Re-fit the model. Repeat Steps 1 and 2 until no variables have \\(p\\)-values smaller than \\(\\alpha_{\\text{crit}}\\). Stepwise Regression This is a combination of backward elimination and forward selection. This technique allows variables that were either removed or added early in the procedure to reenter or exit the model later in the process. At each stage, a variable can be added or removed. Testing-based procedures are relatively straightforward to implement; however, they do have some drawbacks. One of the chief weaknesses of testing-based procedures is ending up with a model that is overly parsimonious. 1.3 Criterion-based approach In essence, we look for the combination of variables that gives us the best goodness-of-fit (gof) metric. Different gofs focus on different things. Here, we will consider \\(R^2_\\alpha\\) (adjusted \\(R^2\\)), Mallow's \\(C_p\\), Bayes Information Criterion (BIC) and Akaike Information Criterion (AIC). You are not expected to know the formula for Mallow's \\(C_p\\), BIC or AIC. Adjusted \\(R^2\\) (\\(R^2_\\alpha\\)) It is a measure of the variability of the data captured by the model, as \\(R^2\\). However, because more predictors will always increase with more variables, we adjust it to account for the added complexity. It is defined as \\[\\begin{equation} R^2_{\\alpha} = 1- \\frac{SSE/(n-p)}{SST/(n-1)}. \\tag{1.1} \\end{equation}\\] In summary: The higher, the better. Mallow's (\\(C_p\\)) It is defined as \\[\\begin{equation} C_p = \\frac{SSE}{\\hat{\\sigma}^2} + 2p - n. \\tag{1.2} \\end{equation}\\] where \\(\\hat{\\sigma}\\) is from the model with all predictors and \\(SSE\\) is for the model with \\(p\\) predictors. When all predictors are used in the model \\(C_p = p\\). It is helpful to plot \\(C_p\\) against \\(p\\). In summary: Small \\(p\\) and \\(C_p \\leq p\\). The smaller, the better. Bayes Information Criterion (\\(BIC\\)) Recall that \\(\\ln\\mathcal{L}(\\boldsymbol{\\beta}, \\sigma^2|\\mathbf{X})\\) is the log-likelihood function. The BIC is defined as \\[\\begin{equation} BIC = -2\\text{max}(\\ln \\mathcal{L}(\\boldsymbol{\\beta}, \\sigma^2|\\mathbf{X})) + p \\cdot \\text{ln}(n) = n \\text{ln}\\frac{SSE}{n} +p\\text{ln}(n) + \\text{constant}. \\tag{1.3} \\end{equation}\\] The BIC uses the log-likelihood and penalises the number of parameters \\(p\\) in the model. In summary: The smaller BIC, the better. Akaike Information Criterion (\\(AIC\\)) The AIC is defined as \\[\\begin{equation} AIC = -2\\text{max}(\\ln\\mathcal{L}(\\boldsymbol{\\beta}, \\sigma^2|\\mathbf{X})) + 2p = n \\text{ln}\\frac{SSE}{n} + 2p + \\text{constant}. \\tag{1.4} \\end{equation}\\] This is perhaps one of the most common goodness-of-fit metrics. It is good to know. In summary: The smaller AIC, the better. "],["example-1-methods-for-variable-selection.html", "2 Example 1: Methods for Variable Selection 2.1 Backward Elimination and Forward Selection 2.2 Criterion-Based Variable Selection", " 2 Example 1: Methods for Variable Selection We have looked at the data for 78 high school wrestlers taken to understand body fat content measurements using three different body fat measuring techniques: hydrostatic weighing (HWFAT), skinfold measurements (SKFAT), and the Tanita body fat scale (TANFAT). In this example, we will create a regression model to predict the wrestlers' hydrostatic fat HWFAT using other variables in the dataset. The question we answer with model selection is: what are the best predictors of HWFAT? Data: HSwrestler Columns:                       C1: AGE - Age of wrestler                       C2: HT - Height in inches                       C3: WT - Weight in pounds                       C4: ABS - Abdominal fat                       C5: TRICEPS - Tricep fat                       C6: SUBSCAP - Subscapular fat                       C7: HWFAT - Hydrostatic fat                       C8: TANFAT - Tanita fat                       C9: SKFAT - Skin fat Read in the data by using: library(PASWR) data(&quot;HSwrestler&quot;) 2.1 Backward Elimination and Forward Selection Exploratory analysis: use pairs to visualise the data and determine which predictors may be useful in predicting HWFAT. pairs(HSwrestler) We can see that there are many positive relationships with HWFAT, mainly SKFAT, HWFAT, SUBCAP, TRIPCEPS, ABS and WT. The only variables that do not seem to have a positive relationship with HWFAT are AGE and HT. Hint You can get a little more information by calling another function from the car package: library(car) scatterplotMatrix(HSwrestler) Backward elimination starts with all the variables in the model (model.all) and eliminates variables with the largest (least significant) \\(p\\)-values. The period in the short-hand notation HWFAT ~ . tells R to include all of the variables specified in the data argument. In this case, the variables TANFAT and SKFAT are removed using negative indices. Do this in two steps, first fitting the full model (all variables), and then removing the least significant variable.       Step 1: Fit model (full model at the start) model.all &lt;- lm(HWFAT ~ . , data = HSwrestler[,c(-8,-9)]) summary(model.all) # Take note of which are least significant ## ## Call: ## lm(formula = HWFAT ~ ., data = HSwrestler[, c(-8, -9)]) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.162 -1.858 -0.464 2.502 8.177 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.29370 9.63027 1.380 0.1718 ## AGE -0.32893 0.32158 -1.023 0.3098 ## HT -0.06731 0.16051 -0.419 0.6762 ## WT -0.01365 0.02591 -0.527 0.5999 ## ABS 0.37142 0.08837 4.203 7.55e-05 *** ## TRICEPS 0.38743 0.13761 2.815 0.0063 ** ## SUBSCAP 0.11405 0.14193 0.804 0.4243 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.028 on 71 degrees of freedom ## Multiple R-squared: 0.8918, Adjusted R-squared: 0.8827 ## F-statistic: 97.54 on 6 and 71 DF, p-value: &lt; 2.2e-16 The summary table lists the \\(p\\)-value according to the \\(t\\)-test. Alternatively, we could perform the \\(F\\)-test by using drop1(). # Perform test drop1(model.all, test = &quot;F&quot;) # single term deletions ## Single term deletions ## ## Model: ## HWFAT ~ AGE + HT + WT + ABS + TRICEPS + SUBSCAP ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 651.05 179.51 ## AGE 1 9.594 660.64 178.65 1.0463 0.309839 ## HT 1 1.613 652.66 177.70 0.1759 0.676225 ## WT 1 2.546 653.60 177.81 0.2777 0.599879 ## ABS 1 162.000 813.05 194.84 17.6669 7.549e-05 *** ## TRICEPS 1 72.683 723.73 185.76 7.9264 0.006301 ** ## SUBSCAP 1 5.921 656.97 178.21 0.6458 0.424315 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1       Step 2: Test and remove the least significant term Both the \\(t\\)-test and the \\(F\\)-test show that HT is the least significant. In addition, suppose \\(\\alpha_\\text{crit}=0.15\\), then the \\(p\\)-value corresponding to HT is larger than \\(\\alpha_\\text{crit}\\). Therefore we should remove it by using the following code: mod.hsw &lt;- update(model.all, . ~ . - HT)       Step 3: Repeat steps 1 and 2 until all variables meet criteria To see if we have removed enough predictors, perform a test and re-evaluate as many times as necessary. What variables are included in the final model? The next variable to be removed should be AGEWTABSTRICEPSSUBCAP. Should you continue removing variables? YESNO. What variables are included in your final model? AGE is includedis not included, HT is includedis not included, WT is includedis not included, ABS is includedis not included, TRICEPS is includedis not included, and SUBSCAP is includedis not included. Hint drop1(mod.hsw, test = &quot;F&quot;) ## Single term deletions ## ## Model: ## HWFAT ~ AGE + WT + ABS + TRICEPS + SUBSCAP ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 652.66 177.70 ## AGE 1 9.875 662.54 176.87 1.0894 0.300098 ## WT 1 10.554 663.22 176.95 1.1643 0.284169 ## ABS 1 189.072 841.73 195.54 20.8580 1.996e-05 *** ## TRICEPS 1 78.809 731.47 184.59 8.6941 0.004302 ** ## SUBSCAP 1 5.693 658.36 176.38 0.6281 0.430660 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can also perform model selection the other way around by adding one variable at the time. The functions add1() and update() are used to create a model using forward selection. In forward selection, the initial model only has an intercept (no predictors). The steps are the same as in backward selection.       Step 1: Fit model (only intercept at the start) # We define SCOPE - an object to help us keep track of the variables we can add SCOPE &lt;- (~ . + AGE + HT + WT + ABS + TRICEPS + SUBSCAP) # Fit initial model mod.fs &lt;- lm(HWFAT ~ 1, data = HSwrestler) summary(mod.fs) ## ## Call: ## lm(formula = HWFAT ~ 1, data = HSwrestler) ## ## Residuals: ## Min 1Q Median 3Q Max ## -10.655 -5.508 -3.100 1.182 27.655 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.235 1.001 14.22 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 8.84 on 77 degrees of freedom       Step 2: Test and add the most significant term # Perform test add1(mod.fs, scope = SCOPE, test = &quot;F&quot;) ## Single term additions ## ## Model: ## HWFAT ~ 1 ## Df Sum of Sq RSS AIC F value Pr(&gt;F) ## &lt;none&gt; 6017.8 340.97 ## AGE 1 175.0 5842.8 340.67 2.2765 0.1355 ## HT 1 117.8 5900.0 341.43 1.5175 0.2218 ## WT 1 3237.6 2780.2 282.74 88.5045 2.219e-14 *** ## ABS 1 5072.8 945.0 198.57 407.9929 &lt; 2.2e-16 *** ## TRICEPS 1 5056.3 961.5 199.92 399.6462 &lt; 2.2e-16 *** ## SUBSCAP 1 4939.0 1078.8 208.90 347.9456 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The variables ABS, TRICEPS and SUBSCAP all have very small \\(p\\)-values (&lt; 2.2e-16). Therefore in this case, we simply take the first variable. # Add the predictor mod.fs &lt;- update(mod.fs, . ~ . + ABS) summary(mod.fs) ## ## Call: ## lm(formula = HWFAT ~ ABS, data = HSwrestler) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.0920 -2.1788 -0.3144 2.2722 10.7798 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.65369 0.65867 5.547 4.05e-07 *** ## ABS 0.63246 0.03131 20.199 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.526 on 76 degrees of freedom ## Multiple R-squared: 0.843, Adjusted R-squared: 0.8409 ## F-statistic: 408 on 1 and 76 DF, p-value: &lt; 2.2e-16 ABS looks like a good addition. Evaluate your model again and add another variable if appropriate.       Step 3: Repeat steps 1 and 2 until all terms meet criteria Now we repeat steps 1 and 2 until the predictor we add has a \\(p\\)-value greater than \\(\\alpha_{\\text{crit}}\\). The next variable that should be added is TRICEPSAGEWTABSHTSUBCAP. Should you continue adding variables? YESNO. Keep in mind the results from forward and backward selection. What variables are included in your final model? AGE is includedis not included, HT is includedis not included, WT is includedis not included, ABS is includedis not included, TRICEPS is includedis not included, and SUBSCAP is includedis not included. Hint # Test the current model add1(mod.fs, scope = SCOPE, test = &quot;F&quot;) 2.2 Criterion-Based Variable Selection We can also perform variable selection by optimising some criteria. For example, our final model would be comprised of the combination of variables that results in the highest \\(R^2_a\\) (or another goodness-of-fit metric). Use the function regsubsets() from the package leaps to build a regression model where HWFAT is the response using the predictors AGE, HT, WT, ABS, TRICEPS, and SUBSCAP when \\(R^2_a\\) is the criterion used for variable selection. library(leaps) models &lt;- regsubsets(HWFAT ~ ., data = HSwrestler[, -c(8, 9)]) summary(models) ## Subset selection object ## Call: regsubsets.formula(HWFAT ~ ., data = HSwrestler[, -c(8, 9)]) ## 6 Variables (and intercept) ## Forced in Forced out ## AGE FALSE FALSE ## HT FALSE FALSE ## WT FALSE FALSE ## ABS FALSE FALSE ## TRICEPS FALSE FALSE ## SUBSCAP FALSE FALSE ## 1 subsets of each size up to 6 ## Selection Algorithm: exhaustive ## AGE HT WT ABS TRICEPS SUBSCAP ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 6 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; Since we are evaluating adding or dropping terms based on \\(R^2_a\\), we can see the difference in each model. R2adj &lt;- summary(models)$adjr2 R2adj ## [1] 0.8409068 0.8801014 0.8849817 0.8846381 0.8840129 0.8826699 The output indicates the best single predictor model is that with the variable ABS, and that particular model has \\(R^2_a = 0.8409\\), the best model with two predictors is the model with the variables ABS and TRICEPS with \\(R^2_a = 0.8801\\), and so forth. The largest \\(R^2_a\\) is \\(0.8849817\\), which corresponds to the model with predictors AGE, ABS, and TRICEPS. The best models of each size are stored in the object models, and Mallow’s \\(C_p\\) values are extracted from the model. MCP &lt;- summary(models)$cp MCP ## [1] 29.051861 4.641808 2.541953 3.775400 5.175856 7.000000 The smallest Mallow’s \\(C_p\\) value is \\(2.542\\), indicating the best model according to Mallow’s \\(C_p\\) is the one with the predictors AGE, ABS, and TRICEPS. Can you think of a way of selecting the best model according to \\(BIC\\)? Hint You can try bic. Remember, the lower the \\(BIC\\) the better. Just now, we find the best model by looking at the model giving the maximum \\(R^2_a\\) or the minimum Mallow's \\(C_p\\) or BIC and then looking at the predictors corresponding to the best model. A quicker way is to read the results graphically using the function plot. For example plot(models, scale = &quot;adjr2&quot;) plot(models, scale = &quot;Cp&quot;) plot(models, scale = &quot;bic&quot;) In these plots, each row corresponds to a model, with the top row corresponding to the best model. The shaded blocks indicate that the predictors are selected in this model. For example, in the plot of adjusted \\(R^2\\), the top row contains four shaded blocks, meaning that the best model is consisted of intercept, age, abs and triceps. (optional) Verify that the model selected using AIC as a criterion from the function stepAIC() from the MASS package returns the same model as the model created in (e) using the regsubsets() function. Hint Fit a full model. Define SCOPE &lt;- (~.) Use the function stepAIC() instead of add1(). Be sure to check what parameters it takes using ?stepAIC "],["exercise-1-lake-water-quality.html", "3 Exercise 1: Lake water quality", " 3 Exercise 1: Lake water quality The EC Water Framework Directive (WFD) 2000 states that there should be ‘good status’ in all shallow waters by 2016. Therefore, it is of interest for environmental management to investigate lake typology and identify lakes that are susceptible to large amounts of algae. An ecologist has recorded the total biovolume of algae in lakes over the summer months. He also measured the altitude, mean depth, retention time, alkalinity, colour and total phosphorus (TP) of the lakes. He was interested in which of these lake typology variables were related to the total amount of algae in the lakes. So the main question is: which of the lake typology variables are useful predictors of the log total biovolume of algae within lakes? Data: lakes.csv Columns:                       C1: lAltitude - log altitude                       C2: lMeanDepth - log mean depth                       C3: lRettime - log retention time                       C4: lAlkalinity - log alkalinity                       C5: lColour - log colour                       C6: lTotalP - log total phosphorus                       C7: ltotalbio - log total biovolume All variables were transformed using natural logs originally to make the distribution of each variable more symmetric. Read in the data using: lakes &lt;- read.csv(&quot;lakes.csv&quot;) Tasks: Use the pairs() command to produce a matrix plot to explore relationships between all pairs of variables. Fit a multiple linear regression model using the lm command with the response being the log total biovolume of algae (ltotalbio), and the other variables as explanatory variables. Use the regsubsets() command in R to select the best model based on adjusted \\(R^2\\). Hint library(leaps) mod.lm &lt;- regsubsets(ltotalbio ~., data = lakes) R2adj &lt;- summary(mod.lm)$adjr2 R2adj Fit a (multiple) linear regression model with a response of log total biovolume of algae and the predictor variables selected using \\(R^2_a\\). Use the final fitted model to produce a 95% prediction interval for the log total biovolume of a future lake, for the values of the explanatory variables below which correspond to your final model: \\(\\textbf{ lAltitude} = 4.4, \\textbf{ lMeanDepth} = 1.5, \\textbf{ lRettime} = -1.3,\\) \\(\\textbf{ lAlkalinity} = -0.5, \\textbf{ lColour} = 2.7, \\textbf{ lTotalP} = 2.77.\\) Hint This was done in Lab 6. The code hides there! If not, here's a short recap: model.final &lt;- lm(ltotalbio ~ lAltitute + lAlkalinity + lColour + lTotalP, data = lakes) pred.df1 &lt;- data.frame(lAltitude = 4.4, lMeanDepth = 1.5, lRettime = -1.3, lAlkalinity = -0.5, lColour = 2.7, lTotalP = 2.77) predict(model.final, int=&quot;p&quot;, newdata = pred.df1) "],["exercise-2-fuel-efficiency.html", "4 Exercise 2: Fuel Efficiency", " 4 Exercise 2: Fuel Efficiency A sample of 86 cars had their city fuel economy measured in miles per gallon and for each car the size of the engine, horsepower (an indication of how powerful the engine is) and the length of the wheelbase (distance between the centres of the front and rear wheels) were also recorded. It is of interest to model city fuel economy using the engine size, horsepower and wheelbase as predictors. Data: cars.csv Columns:                       C1: cmpg - thickness of inner wall during systole                       C2: EngineSize - in litres                       C3: HorsePower - in hp units                       C4: Wheelbase - in inches Read in the data using: cars &lt;- read.csv(&quot;cars.csv&quot;) Tasks: Use an appropriate exploratory analysis to explore the relationships between cmpg, EngineSize, HorsePower and Wheelbase. Is there anything that might concern you here? Fit a multiple linear (lm) regression model to the data in order to predict cmpg from EngineSize, HorsePower and Wheelbase. Justify your model selection. Choose from backward selection, forward selection, or a criterion-based approach. Using an appropriate regression model, find and interpret a 95% confidence interval for the logarithm of city fuel economy of a future car with: \\(\\textbf{ EngineSize} = 2, \\textbf{ HorsePower} = 65, \\textbf{ Wheelbase} = 95\\) Hint See Hint in Exercise 1. Using the same regression model, find and interpret a 95% prediction interval for the logarithm of city fuel economy of a future car with: \\(\\textbf{ EngineSize} = 2, \\textbf{ HorsePower} = 65, \\textbf{ Wheelbase} = 95\\) Hint See Hint in Exercise 1. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
