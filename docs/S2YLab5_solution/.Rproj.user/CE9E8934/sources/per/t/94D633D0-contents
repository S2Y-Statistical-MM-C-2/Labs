---
title: "Lab 5 - Ordinary Least Squares"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
biblio-style: apalike
link-citations: yes
rmd_files: ["index.Rmd", "Example1.Rmd", "Example2.Rmd", "Exercise1.Rmd", "Exercise2.Rmd","Exercise3.Rmd"]

---

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(webexercises)
library(tinytex)
library(dplyr)
library(knitr)
library(ggplot2)
library(tidyverse)
library(PASWR2)
library(MASS)
RUBBER<- Rubber
FERTILIZE <- FERTILIZE

```


```{r include=FALSE, echo=FALSE}
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

# Welcome to Lab 5!

Intended Learning Outcomes:

1. Introduce and understand the matrix notation of linear models.
2. Obtain least squares estimates (LSE) of model parameters using matrix notation in `R`. 
<!-- 3. Complete a one-sample hypothesis test of the mean in R by yourself. -->

## Introduction to Ordinary Least Squares (OLS)
The primary tool used to model associations among variables is regression (linear or otherwise).
Regression analysis is used to model the relationship between a single variable $Y$ , called
the **response** or **dependent variable**, and one or more **explanatory variables**, also called
**predictor(s)**, **covariates**, or **independent variable(s)**, $x_1, x_2,...,x_p$.

As a first step, the response variable will be continuous, but the predictor variables can be either continuous, discrete, or
categorical. You can predict non-numeric (continuous) variables using regression models, but they will be called **generalised** regression models. Here, we stick to the classical regression models. Parameter estimation for linear models can be carried out via OLS or maximum likelihood (ML). In this lab, we will look at OLS.  

The word “regression” is due to Sir Francis Galton, who in 1885 demonstrated that
o↵spring do not tend toward the size of the parents; rather, o↵spring size tends toward the
mean of the population. That is, there is a “regression toward mediocrity.”

What else has Francis done? **"Whenever you can, count"** and **"London is the epicenter of female beauty, Aberdeen is the opposite..."**. Fun guy. 

<center>
![](Images/sir_francis.png){width=300px}
<center>

## The simplest regression model: Simple Linear Regression (one covariate)##

The simple linear regression model is defined as 

$$
Y_i = \beta_0 + \beta_1x_i + \epsilon_i,
$$
where 

* $Y_i$ is the value of the response variable for the $i$th observation.
* $\beta_0$ and $\beta_1$ are regression parameters.
  + $\beta_0$ is known as the intercept.
  + $\beta_1$ is the regression coefficient of $x$ (the effect of $x$ on $Y$).
* $\epsilon_i$ is a random error term that is assumed to have a $N(o,\sigma^2)$ distribution. 

The "modelling" bit refers to estimating model parameters, $\beta_0$ and $\beta_1$ for the $n$ measurements. It looks like

$$
Y_1 = \beta_0 + \beta_1x_1 + \epsilon_1\\
Y_2 = \beta_0 + \beta_1x_2 + \epsilon_2\\
\vdots\\
Y_n = \beta_0 + \beta_1x_n + \epsilon_n.\\
$$
The model can be written in matrix notation as 
$$
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{\epsilon},
$$
where

$$\mathbf{Y} = \left[\begin{matrix} Y_1\\Y2\\ \vdots \\Y_n \end{matrix}\right] \text{ as a } (n \times 1) \text{ matrix, } \mathbf{X} = \left[\begin{matrix} 1 & x_1\\1 & x2 \\ \vdots&\vdots \\ 1 & x_n \end{matrix}\right] \text{ as } (n \times 2) \text{, } \boldsymbol{\beta} =\left[\begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right] \text{ as } (2 \times 1) \text{.} $$
Note that $\epsilon \sim N(\mathbf{0}, \sigma^2\mathbf{I})$ where $\sigma^2 \mathbf{I}$ is the variance-covariance matrix of the vector of errors. Given that $\mathbf{I}$ is the identity matrix, $\sigma^2$ is only along the diagonals, indicating that $\epsilon_i$ are independent from each other. 

## The next-simplest regression model: Multiple Linear Regression (multiple covariates)##

Multiple linear regression is an extension of simple linear regression, where we have more than one coefficient $x$. $Y$ is still the dependent variable, $\beta_0$ the intercept, but $\beta_1$ is generalised to $\beta_j$ which is now the coefficeint of $x_ij$. The model is written as 

$$
Y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \cdots + \beta_{(p-1)}x_{i(p-1)} \epsilon_i, \text{ for } i = 1, 2,...,n.
$$
The matrix notation is

The model can be written in matrix notation as 
$$
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{\epsilon},
$$

where

$$\mathbf{Y} = \left[\begin{matrix} Y_1\\Y2\\ \vdots \\Y_n \end{matrix}\right] \text{ as a } (n \times 1) \text{ matrix, } \mathbf{X} = \left[\begin{matrix} 1 & x_{11} & \cdots& x_{1(p-1)}\\1 & x_{21} & \cdots& x_{2(p-1)} \\ \vdots&\vdots&&\vdots \\ 1 & x_{n1} & \cdots& x_{n(p-1)} \end{matrix}\right] \text{ as } (n \times p) \text{, } \boldsymbol{\beta} =\left[\begin{matrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_n \end{matrix} \right] \text{ as } (p \times 1), \text{ and } \mathbf{\epsilon} = \left[\begin{matrix} \epsilon_0 \\ \epsilon_1 \\ \vdots \\ \epsilon_n \end{matrix} \right] \text{ as } (n \times 1) \text{.} $$

Note some things

* $\mathbf{X}$ contains the values of particular independent variables (covariates). 
  + $\mathbf{X}$ is assumed to be known constants.
* $\mathbf{Y}$ and $\mathbf{\epsilon}$ are random vectors whose elements are random variables. 
* $\boldsymbol{\beta}$ is a vector of unknown coefficients (what we have to estimate!). 

## Ordinary Least Squares ##

Parameter estimation can be carried out using OLS or ML. The ordinary least squares method of estimating parameters minimizes the sum of the
squared deviations of the Yis from their expected values such that

$$
\epsilon_i = Y_i - E(Y_i) \text{ same as saying } = Y_i - (\beta_0 + \beta_1 x_i). 
$$
The estimates $\hat{\beta}_0$ of $\beta_0$ and $\hat{\beta_1}$ of $\beta_1$ seek to minimise the quantity $\mathcal{Q}$, which represents the sum of the squared residuals (differences) as 

$$
\mathcal{Q} = \sum_{i=1}^{n}\epsilon_i^2 = \sum_{i=1}^n\left((Y_i - (\beta_0 + \beta_{1}x_i)\right)^2,
$$
which, in matrix notation, is the same as saying
$$
\mathcal{Q} = \epsilon'\epsilon = (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}). 
$$
If we want parameter estimates that minimise $\mathcal{Q}, we take the derivative with respect to our parameter

$$
\begin{aligned}
\frac{\delta \mathcal{Q}}{\delta \beta_0} & =2 \sum_{i=1}^n\left(Y_i-\beta_0-\beta_1 x_i\right)(-1) \\
& =-2 \sum_{i=1}^n\left(Y_i-\beta_0-\beta_1 x_i\right) . \\
\frac{\delta \mathcal{Q}}{\delta \beta_1} & =2 \sum_{i=1}^n\left(Y_i-\beta_0-\beta_1 x_i\right)\left(-x_i\right) \\
& =-2 \sum_{i=1}^n\left(Y_i-\beta_0-\beta_1 x_i\right)\left(x_i\right), 
\end{aligned}
$$
 set it to zero, and solve for our parameter $\hat{\beta}_j$. The solutions for $\hat{\beta}_0$ and $\hat{\beta}_1$ are
 $$
 \begin{aligned}
 \hat{\beta}_0 & = \bar{Y} - \hat{\beta}_1\hat{x} \\
 \hat{\beta}_1 & \frac{\sum_{i=1}^n(x_i - \bar{x})(Y_i - \bar{Y})}{\sum_{i=1}^{n}(x_i - \bar{x})^2}.
 
 \end{aligned}
 $$
 
In matrix notation, the estimation we can generalise the estimation of $\mathbf{\beta}$ as 
$$
\boldsymbol{\beta} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}. 
$$
Deterministically speaking, these equations will give you the values for $\hat{\beta}_0$ and $\hat{\beta}_1$ (or $\boldsymbol{\beta}$) that minimise $\mathcal{Q}$. If you would like the proof, let me know!


## Credit where credit is due

The labs in S2 incorporate and adapt materials from:

Ugarte, M. D., Militino, A. F., & Arnholt, A. T. (2008). [Probability and Statistics with R](https://ebookcentral.proquest.com/lib/gla/detail.action?docID=5338596). CRC press.

