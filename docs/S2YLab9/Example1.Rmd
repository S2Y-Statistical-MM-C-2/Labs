
# Example 1: Methods for Variable Selection

We have looked at the data for 78 high school wrestlers taken to understand body fat content measurements using three different body fat measuring techniques: hydrostatic weighing (`HWFAT`), skinfold measurements (`SKFAT`), and the Tanita body fat scale (`TANFAT`). In this example, we will create a regression model to predict the wrestlers' hydrostatic fat `HWFAT` using other variables in the dataset. The question we answer with model selection is: what are the best predictors of `HWFAT`?

**Data:** `HSwrestler`

**Columns:**

|                       **C1:** `AGE` - Age of wrestler

|                       **C2:** `HT` - Height in inches

|                       **C3:** `WT` - Weight in pounds

|                       **C4:** `ABS` - Abdominal fat

|                       **C5:** `TRICEPS` - Tricep fat

|                       **C6:** `SUBSCAP` - Subscapular fat

|                       **C7:** `HWFAT` - Hydrostatic fat

|                       **C8:** `TANFAT` - Tanita fat

|                       **C9:** `SKFAT` - Skin fat

\
Read in the data by using:
```{r, include= TRUE, echo = TRUE, message=FALSE, warning=FALSE, eval=TRUE}
library(PASWR)
data("HSwrestler")
```

<br>

## Backward Elimination and Forward Selection

(a) Exploratory analysis: use `pairs` to visualise the data and determine which predictors may be useful in predicting `HWFAT`.

```{r}
pairs(HSwrestler)
```

We can see that there are many positive relationships with `HWFAT`, mainly `SKFAT`, `HWFAT`, `SUBCAP`, `TRIPCEPS`, `ABS` and `WT`. The only variables that do not seem to have a positive relationship with `HWFAT` are `AGE` and `HT`.


`r hide ("Hint")`
You can get a little more information by calling another function from the `car` package:
```{r, warning=FALSE, message=FALSE, warning=FALSE, eval=FALSE}
library(car)
scatterplotMatrix(HSwrestler)
```
`r unhide ()`

(b) Backward elimination starts with all the variables in the model (`model.all`) and eliminates variables with the largest (least significant) $p$-values. The period in the short-hand notation `HWFAT ~ . ` tells R to include all of the variables specified in the data argument. In this case, the variables `TANFAT` and `SKFAT` are removed using negative indices. Do this in two steps, first fitting the full model (all variables), and then removing the least significant variable.

|       **Step 1: Fit model (full model at the start)**
```{r, include = TRUE, message=FALSE, warning=FALSE, eval=TRUE}
model.all <- lm(HWFAT ~ . , data = HSwrestler[,c(-8,-9)])
summary(model.all) # Take note of which are least significant
```

The summary table lists the $p$-value according to the $t$-test. Alternatively, we could perform the $F$-test by using `drop1()`.

```{r,include = TRUE, message=FALSE, warning=FALSE, eval=TRUE}
# Perform test
drop1(model.all, test = "F") # single term deletions
```

|       **Step 2: Test and remove the least significant term**

Both the $t$-test and the $F$-test show that `HT` is the least significant. In addition, suppose $\alpha_\text{crit}=0.15$, then the $p$-value corresponding to `HT` is larger than $\alpha_\text{crit}$. Therefore we should remove it by using the following code:
```{r}
mod.hsw <- update(model.all, . ~ . - HT)
```

|       **Step 3: Repeat steps 1 and 2 until all variables meet criteria**

To see if we have removed enough predictors, perform a test and re-evaluate as many times as necessary. What variables are included in the final model?

The next variable to be removed should be `r mcq(c("AGE", "WT", "ABS", answer="TRICEPS", "SUBCAP"))`. \

Should you continue removing variables? `r mcq(c(answer="YES", "NO"))`. \

What variables are included in your final model? `AGE` `r mcq(c(answer="is included", "is not included"))`, `HT` `r mcq(c("is included",answer="is not included"))`, `WT` `r mcq(c("is included",answer="is not included"))`, `ABS` `r mcq(c(answer="is included", "is not included"))`, `TRICEPS` `r mcq(c(answer="is included", "is not included"))`, and `SUBSCAP` `r mcq(c("is included",answer="is not included"))`.

`r hide ("Hint")`
```{r}
drop1(mod.hsw, test = "F")
```
`r unhide ()`

(c) We can also perform model selection the other way around by adding one variable at the time. The functions `add1()` and `update()` are used to create a model using forward selection. In forward selection, the initial model only has an intercept (no predictors). The steps are the same as in backward selection.

|       **Step 1: Fit model (only intercept at the start)**

```{r, include = TRUE, message=FALSE, warning=FALSE, eval=TRUE}
# We define SCOPE - an object to help us keep track of the variables we can add
SCOPE <- (~ . + AGE + HT + WT + ABS + TRICEPS + SUBSCAP)

# Fit initial model
mod.fs <- lm(HWFAT ~ 1, data = HSwrestler)
summary(mod.fs)
```

|       **Step 2: Test and add the most significant term**
<!-- `r hide ("Solution")` -->
```{r, include = TRUE, message=FALSE, warning=FALSE, eval=TRUE}
# Perform test
add1(mod.fs, scope = SCOPE, test = "F")
```

The variables `ABS`, `TRICEPS` and `SUBSCAP` all have very small $p$-values (`< 2.2e-16`). Therefore in this case, we simply take the first variable. 
```{r, include = TRUE, message=FALSE, warning=FALSE, eval=TRUE}
# Add the predictor
mod.fs <- update(mod.fs, . ~ . + ABS)
summary(mod.fs)
```
`ABS` looks like a good addition. Evaluate your model again and add another variable if appropriate.


|       **Step 3: Repeat steps 1 and 2 until all terms meet criteria**

Now we repeat steps 1 and 2 until the predictor we add has a $p$-value greater than $\alpha_{\text{crit}}$.\

The next variable that should be added is `r mcq(c(answer="TRICEPS", "AGE", "WT", "ABS", "HT", "SUBCAP"))`. \

Should you continue adding variables? `r mcq(c(answer="YES", "NO"))`. \

Keep in mind the results from forward and backward selection. What variables are included in your final model? `AGE` `r mcq(c(answer="is included", "is not included"))`, `HT` `r mcq(c("is included",answer="is not included"))`, `WT` `r mcq(c("is included",answer="is not included"))`, `ABS` `r mcq(c(answer="is included","is not included"))`, `TRICEPS` `r mcq(c(answer="is included","is not included"))`, and `SUBSCAP` `r mcq(c("is included",answer="is not included"))`.

`r hide ("Hint")`
```{r, eval = FALSE}
# Test the current model
add1(mod.fs, scope = SCOPE, test = "F")
```
`r unhide ()`

<br>

## Criterion-Based Variable Selection

We can also perform variable selection by optimising some criteria. For example, our final model would be comprised of the combination of variables that results in the highest $R^2_a$ (or another goodness-of-fit metric).

(d) Use the function `regsubsets()` from the package `leaps` to build a regression model
where `HWFAT` is the response using the predictors `AGE`, `HT`, `WT`, `ABS`, `TRICEPS`, and `SUBSCAP`
when $R^2_a$ is the criterion used for variable selection.


```{r, eval = T, include = T, echo = T}
library(leaps)
models <- regsubsets(HWFAT ~ ., data = HSwrestler[, -c(8, 9)])
summary(models)
```
Since we are evaluating adding or dropping terms based on $R^2_a$, we can see the difference in each model.
```{r, eval = T, include = T, echo = T}
R2adj <- summary(models)$adjr2
R2adj
```
The output indicates the best single predictor model is that with the variable `ABS`, and that particular model has $R^2_a = 0.8409$, the best model with two predictors is the model with the variables `ABS` and `TRICEPS` with $R^2_a = 0.8801$, and so forth. The largest $R^2_a$ is $0.8849817$, which corresponds to the model with predictors `AGE`, `ABS`, and `TRICEPS`.

(e) The best models of each size are stored in the object models, and Mallow’s $C_p$ values are extracted from the model.

```{r, eval = T, include = T, echo = T}
MCP <- summary(models)$cp
MCP
```
The smallest Mallow’s $C_p$ value is $2.542$, indicating the best model according to Mallow’s $C_p$ is the one with the predictors `AGE`, `ABS`, and `TRICEPS`.

(f) Can you think of a way of selecting the best model according to $BIC$?

`r hide ("Hint")`
You can try `bic`. Remember, the lower the $BIC$ the better.
`r unhide ()`

Just now, we find the best model by looking at the model giving the maximum $R^2_a$ or the minimum Mallow's $C_p$ or BIC and then looking at the predictors corresponding to the best model. A quicker way is to read the results graphically using the function `plot`. For example
```{r, include = T, message = F, eval = T}
plot(models, scale = "adjr2")
plot(models, scale = "Cp")
plot(models, scale = "bic")
```

In these plots, each row corresponds to a model, with the top row corresponding to the best model. The shaded blocks indicate that the predictors are selected in this model. For example, in the plot of adjusted $R^2$, the top row contains four shaded blocks, meaning that the best model is consisted of intercept, age, abs and triceps.

(g) (optional) Verify that the model selected using AIC as a criterion from the function `stepAIC()` from the `MASS` package returns the same model as the model created in (e) using the `regsubsets()` function. 

`r hide ("Hint")`
1. Fit a full model.
2. Define `SCOPE <- (~.)`
3. Use the function `stepAIC()` instead of `add1()`. Be sure to check what parameters it takes using `?stepAIC`
`r unhide ()`

<!-- ## Part 3: PLEASE -->

