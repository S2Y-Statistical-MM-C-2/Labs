[["index.html", "Lab 9 - Transformations and Assumptions 1 Welcome to Lab 9! 1.1 Introduction to variable transformations 1.2 Root 1.3 Reciprocal 1.4 Quadratic 1.5 Logarithmic", " Lab 9 - Transformations and Assumptions 1 Welcome to Lab 9! Intended Learning Outcomes: Produce residual plots and assess the assumptions of a linear model. Fit linear models with transformed variables. 1.1 Introduction to variable transformations The need to transform variables may arise during the exploratory analysis when you realize that your predictor(s) do not have a linear relationship with the response, or during assumption checking when the residuals do not meet assumptions. In either case, a transformation of the response variable, the predictors, or both response and predictors may be necessary. If the model has been fitted using a transformation on the response variable, the results will be given in the transformed scale. It is still possible to report the findings in the original scale, but this requires a back-transformation. How do I back-transform my data, you ask? Simply apply the inverse operation (inverse of the transformation) to your predictions. In this lab, we will explore some simple transformations and how to tell if you need them. All examples are a simple linear regression where \\(Y = \\beta_0 + \\beta_1 x + \\epsilon\\), where \\(Y\\) is the response and \\(x\\) is the predictor. 1.2 Root A root transformation is one where you take a root of the variable. For example, you may take the square or cubic root. Recognizing the need for the transformation is important. Here is a small simulation to give you a taste for a predictor in need of a square root transformation looks like. In this example, \\(\\beta_0 = 1\\) and \\(\\beta_1 = 2\\). par(mfrow = c(1,2)) plot(Y~x, pch = 16, main = &quot;Original Scale&quot;) plot(Y~I(x^0.5), pch = 16, main = &quot;After Square-root Transformation&quot;) Figure 1.1: Left: Scatterplot of Y vs x in the original scale. Right: Scatterplot after the square-root transformation of \\(x\\) As you can see in the plots, the transformation may be required if the predictor displays a convex curved relationship with the response. If you think this is you, you can apply the transformation in the model using: # Model without transformation model &lt;- lm(y ~ x) # Model with transformation model.root &lt;- lm(y ~ I(x^0.5)) 1.3 Reciprocal The reciprocal transformation of \\(x\\) is the same as saying \\(\\frac{1}{x}\\) or \\(x^{-1}\\). We performed another simulation to show you what data in need of the reciprocal transformation look like. Here, \\(\\beta_0 = 1\\) and \\(\\beta_1 = 1.5\\). par(mfrow = c(1,2)) plot(Y~x, pch = 16, main = &quot;Original Scale&quot;) plot(Y~I(x^-1), pch = 16, main = &quot;After Reciprocal Transformation&quot;) Figure 1.2: Left: Scatterplot of Y vs x in the original scale. Right: Scatterplot after the reciprocal transformation of \\(x\\) The figure shows a concave curve that slopes downward. The response is largest when \\(x \\leq 1\\) because \\(\\frac{1}{x} &gt; 1\\) when \\(x &lt; 1\\). Using the same logic, we understand that large values of \\(x\\) result in smaller values of \\(Y\\). You can fit it in a model using: # Model without transformation model &lt;- lm(y ~ x) # Model with transformation model.root &lt;- lm(y ~ I(x^-1)) 1.4 Quadratic The quadratic transformation of \\(x\\) is the same as saying \\(x^2\\). It is the inverse of the square-root transformation. It it a subset of the power transformations. In more general terms, it is \\(x^a\\), where \\(a\\) is anything. The reciprocal can also be thought of in this same category. But what does it look like? The figures below are a simulated example using \\(\\beta_0=1\\) and \\(\\beta_1=2\\). par(mfrow = c(1,2)) plot(Y~x, pch = 16, main = &quot;Original Scale&quot;) plot(Y~I(x^2), pch = 16, main = &quot;After Quadratic Transformation&quot;) Figure 1.3: Left: Scatterplot of Y vs x in the original scale. Right: Scatterplot after the quadratic transformation of \\(x\\) As you can see from the plots, the predictor has a concave relationship with the response previous to the quadratic transformation. You can use it in a model by using the code: # Model without transformation model &lt;- lm(y ~ x) # Model with transformation model.root &lt;- lm(y ~ I(x^2)) 1.5 Logarithmic Last but not least, the logarithmic transformation is equivalent to saying \\(\\ln(x)\\), which in R is the same as log(x). Here is another simulation that shows the original and transformed predictor when the transformation should be logarithmic. par(mfrow = c(1,2)) plot(Y~x, pch = 16, main = &quot;Original Scale&quot;) plot(Y~log(x), pch = 16, main = &quot;After Log Transformation&quot;) Figure 1.4: Left: Scatterplot of Y vs x in the original scale. Right: Scatterplot after the log transformation of \\(x\\) Fitting the transformation in R is straight forward as in the previous transformations. You can use: # Model without transformation model &lt;- lm(y ~ x) # Model with transformation model.root &lt;- lm(y ~ log(x)) Another useful feature of the logarithmic transformation is that it can help when applied to the response variable too so that both predictor and response are log-transformed. In the original scale, the data can look like this: par(mfrow = c(1,2)) plot(Y~x, pch = 16, main = &quot;Original Scale&quot;) plot(log(Y)~log(x), pch = 16, main = &quot;After Log Transformation&quot;) Figure 1.5: Left: Scatterplot of Y vs x in the original scale. Right: Scatterplot after the log transformation of \\(x\\) and \\(Y\\) The main thing to note in these plots is the increasing variance the further away from the origin. Sort of like a \"fanning\" effect. The transformation is easy to fit using: # Model with transformation model.root &lt;- lm(log(y) ~ log(x)) You will not need to know this for the exam, but as a special feature, models that use logarithmic transformations have approximate interpretations without the need for back-transformation. When \\(x\\) has been transformed with a natural log transformation, the change in the \\(\\ln(x)\\) is roughly equal to the change in \\(x\\) provided the changes in \\(x\\) are small. Consider the figure below, which graphically illustrates how changing the \\(x\\) values 3 and 6 by 10% corresponds to an approximate increase in \\(\\ln(x)\\) of about 10%. Small change in \\(x\\) gives a similar small change in \\(\\ln(x)\\) "],["example-1-power-transformation-on-y.html", "2 Example 1: Power transformation on \\(Y\\) 2.1 Exploratory analysis and model fitting 2.2 Assumption checking and interpretation", " 2 Example 1: Power transformation on \\(Y\\) The stopping.csv file contains 63 observations of cars in the process of breaking. Our question of interest is: Can we determine if there is a relationship between the speed of cars and the distance taken to stop? Data: stopping Columns:                       C1: Distance - stopping distance measured in feet                       C2: Speed - Speed of the car as when it breaks in miles per hour Read the data using stopping &lt;- read.csv(&quot;stopping.csv&quot;) 2.1 Exploratory analysis and model fitting The scatterplot of distance (\\(Y\\)) against the speed (\\(x\\)) in Figure 2.1 (left) shows that the variables do not appear to be linearly related. The temptation in this case would be to apply a quadratic transformation for \\(x\\) give that the data are curved and concave. The centre plot in Figure 2.1 shows this transformation. It is linear, but the variance has a fanning effect. For this reason, we consider transforming \\(Y\\) using the square-root transformation (\\(\\sqrt{Y}\\)), which should have a similar effect to our first attempt. The right plot in the figure now shows a linear relationship with constant variance between predictor and response. Figure 2.1: Left: scatterplot of Distance versus Speed. Centre: scatterplot of Distance versus square root of Speed. Right: scatterplot of square root of Distance versus Speed. With a linear relationship, we can more appropriately use our simple linear regression model between \\(\\sqrt{\\text{Distance}}\\) (as new \\(Y\\)) and speed as \\[Y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2), \\quad i = 1,\\ldots, 63.\\] model.sqrt &lt;- lm(I(Distance^0.5) ~ Speed, data = stopping) # or # model.sqrt &lt;- lm(sqrt(Distance) ~ Speed, data = stopping) 2.2 Assumption checking and interpretation Figure 2.2 gives the residual plots after fitting a simple linear regression model to the original variables. Figure 2.2: Residual plots from fitting a simple linear regression model to original variables. Top: Standardised residuals versus fitted values. Bottom: Normal probability (Q-Q) plot. The plots show the problems of curvature, non-constant variance and non-normality, indicating that the wrong type of model was used. Figure 2.3 gives the residual plots after fitting a simple linear regression model to the transformed variables. Figure 2.3: Residual plots from fitting a simple linear regression model to transformed variables. Top: Standardised residuals versus fitted values. Bottom: Normal probability (Q-Q) plot. The curvature disappears and the variance is almost constant across the range of fitted values. The normality assumption, however, remains to be invalid. This is not ideal but, on the positive side, the estimates of parameters will not be affected and hence we can still use the model to describe the relationship between variables and make predictions. summary(model.sqrt) ## ## Call: ## lm(formula = sqrt(Distance) ~ Speed, data = stopping) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.4879 -0.5487 0.0098 0.5291 1.5545 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.918283 0.197406 4.652 1.82e-05 *** ## Speed 0.252568 0.009246 27.317 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7193 on 61 degrees of freedom ## Multiple R-squared: 0.9244, Adjusted R-squared: 0.9232 ## F-statistic: 746.2 on 1 and 61 DF, p-value: &lt; 2.2e-16 The interpretation of the parameters is always done in the scale of the model. For example, we have \\(\\beta_1 = 0.2526\\), so we say that sqrt(Distance) changes \\(0.2526\\) feet for every \\((mile/hour)\\). If you are making predictions, you can back-transform the results to the original scale. You can do this by performing the inverse operation that you used for the transformation. If you used \\(sqrt(Y)\\) for the transformation, then the back-transformation requires you to square the data, \\(\\hat{Y}^2\\), to put it back in the original scale. Tasks Write down the equation of the fitted model. Solution The regression equation is \\[\\sqrt{\\text{Distance}} = 0.918 + 0.253 \\cdot \\text{Speed} \\] Based on the regression equation in (a), comment on the relationship between speed and square root of distance. In addition, pick a speed value yourself and predict the distance for this speed. Solution The estimated parameter of \\(0.253\\) suggests the square root of distance is positively linearly related to speed. As the speed increases by 1 MPH, the expected square root of distance increases by 0.253 feet. When predicting the value of the response, we back transform the variable as \\(\\text{Distance} = (0.918 + 0.253 \\cdot \\text{Speed})^2\\). For example, if the speed is 20 MPH, the predicted distance is \\((0.918+0.252\\cdot 20)^2 \\approx 35.64\\) feet. Note that our model is built only for speed ranging from 4 to 40. It would be unwise to make predictions outside this range in the absence of other information. "],["example-2-animal-brains-and-double-transformations.html", "3 Example 2: Animal brains and double transformations", " 3 Example 2: Animal brains and double transformations Variables are often transformed to ﬁx constant variance or normality assumptions; however, transformations can complicate the interpretation of the model. In this example, we explore transforming both the response and the predictor(s). To illustrate what we mean, we will use the data frame Animals from the MASS package. The data consists of 28 animals, including dinosaurs, and their respective body weight in kilograms and brain weight in grams. Our question of interest is: is a bigger brain required to govern a bigger body? Data: Animals Columns:                       C1: body - body weight given in kilograms                       C2: brain - weight of brains given in grams Read the data using: library(MASS) data(Animals) Exploratory analysis and model fitting As always, it is important to first plot the data to understand the relationship between response and predictor in the original scale. Before you start, make sure to remove the dinosaurs from the data (observations 26 to 28 after sorting), because they are outliers (which we know from previous labs!). # Sort by body size SA &lt;- Animals[order(Animals$body), ] # Remove dinos NoDINO &lt;- SA[-c(26:28),] # Plot the data plot(NoDINO, pch = 16, xlab = &quot;Body&quot;, ylab = &quot;Brain&quot;) From the figure above, we can see the response and predictor have a difficult relationship that is curved (convex) and increases in variability. Transforming both variables may be useful in this case. From the introduction, we know logarithmic transformations are often a good choice. You can fit this model in the same way that you have fitted the other transformations. # Fit the model simple.model &lt;- lm(log(brain) ~ log(body), data = NoDINO) coef(simple.model) ## (Intercept) log(body) ## 2.1504121 0.7522607 Interpretation The predicted log(brain weight) of the jaguar, whose body weight is listed as \\(100 \\text{kg}\\) with the ﬁtted model, is \\(2.1504+0.7523\\times\\text{ln}(100)=5.6147\\). This must be back transformed to get the units of original brain measurement (grams). The brain weight predicted by the model is \\(\\exp(5.6147) = 274.4312 \\text{g}\\). When both the response and the predictors have been transformed with a natural logarithm, one can use the percentage interpretation of \\(\\beta_1\\) and be very close to the actual change given by the model for small changes in the \\(x\\)-variable. If the body weight of an animal increases by \\(1\\%\\), the approximate increase in brain weight is \\((0.01\\times0.7523 = 0.007523 = 0.7523\\%)\\) since \\(\\hat{\\beta}_1= 0.7523\\). If the jaguar were to increase its weight by \\(10\\%\\), the expected increase in brain weight would be approximately \\(7.5\\%\\) for a new weight of \\(1.075 \\times 274.4312 = 295.0136 \\text{g}\\). The actual brain weight change predicted by the model for a body weight of \\(110 \\text{kg}\\) is \\(294.83 \\text{g}\\) and the change in brain weight as predicted from the model is \\(7.4331\\%\\) (see Table 1). Note that for this model, \\(\\hat{\\beta}_1= 0.7523 \\approx \\% \\Delta Y /\\% \\Delta x = 0.0743/0.1 = 0.7433\\). In fact, Table 1. Actual change in jaquar brain weight. "],["exercise-1-identify-the-appropriate-transformation.html", "4 Exercise 1: Identify the appropriate transformation", " 4 Exercise 1: Identify the appropriate transformation The following simulated data contain different non-linear relationships between response and predictor variables. Data: SIMDATAXT and SIMDATAST Columns:                       C1: x1 - predictor 1                       C2: x2 - predictor 2                       C3: x3 - predictor 3 Read in the data using: library(PASWR2) data(&quot;SIMDATAXT&quot;) data(&quot;SIMDATAST&quot;) QUESTIONS: Using the data SIMDATAXT, apply appropriate transformations to \\(x_1\\), \\(x_2\\), and \\(x_3\\) to linearise the relationships between the response and predictors one at a time. It might be best if you break it down and address one predictor at the time. Transform \\(x_1\\) You can begin by examining the relationship between \\(y\\) and \\(x_1\\) at the original scale and in a simple linear model. Hint # Plot relationship plot(y ~ x1, data = SIMDATAXT) # Fit and plot model model1 &lt;- lm(y ~ x1, data = SIMDATAXT) plot(model1,which=c(1)) The relationship between \\(y\\) and \\(x_1\\) is curved and convex. Apply the transformation you think is necessary and fit a model.  From the figures, the appropriate transformation of \\(x_1\\) is the square rootquadraticlogarithmicreciprocalnone . Solution # Before transformation plot(y ~ x1, data = SIMDATAXT) model1 &lt;- lm(y ~ x1, data = SIMDATAXT) plot(model1,which=c(1) # Figures suggest a square root transformation plot(y ~ sqrt(x1), data = SIMDATAXT) model2 &lt;- lm(y ~ sqrt(x1), data = SIMDATAXT) plot(model2,which=c(1)) # Much better! Transform \\(x_2\\) The second predictor can be evaluated in the same way as the first. You can begin by examining the relationship between \\(y\\) and \\(x_2\\) and the relationship between the two in a simple linear model using scatterplots. Hint # Plot relationship plot(y ~ x2, data = SIMDATAXT) # Fit and plot model model3 &lt;- lm(y ~ x2, data = SIMDATAXT) plot(model3,which=c(1)) The relationship seems both curved and concave. Apply the transformation you think is necessary and fit a model. From the figures, the appropriate transformation of \\(x_2\\) is quadraticsquare rootlogarithmicreciprocalnone. Solution # Before transformation plot(y ~ x2, data = SIMDATAXT) model3 &lt;- lm(y ~ x2, data = SIMDATAXT) plot(model3,which=c(1)) # Figures suggest a square root transformation plot(y ~ I(x2^2), data = SIMDATAXT) model4 &lt;- lm(y ~ I(x2^2), data = SIMDATAXT) plot(model4,which=c(1)) Transform \\(x_3\\) Plot \\(y\\) and \\(x_3\\) and the residuals of a simple linear model to assess the relationship between the two. Hint # Plot relationship plot(y ~ x3, data = SIMDATAXT) # Fit and plot model model5 &lt;- lm(y ~ x3, data = SIMDATAXT) plot(model5,which=c(1)) The relationship seems curved and concave. Apply the transformation you think is necessary and fit a model. The figures suggest a reciprocalsquare rootlogarithmicquadraticnone transformation for \\(x_3\\). Solution # Before transformation plot(y ~ x3, data = SIMDATAXT) model5 &lt;- lm(y ~ x3, data = SIMDATAXT) plot(model5) # Figures suggest a square root transformation plot(y ~ I(x3^(-1)), data = SIMDATAXT) model6 &lt;- lm(y ~ I(x2^(-1)), data = SIMDATAXT) plot(model6) Using the data SIMDATAST, figure out the appropriate transformation for \\(x_1\\) and/or \\(y\\). Follow the usual steps by producing a scatterplot of \\(y\\) and \\(x_1\\) and the residuals of the simple linear model at original scale. Consider the possible transformations. The figures suggest using a logarithmicsquare rootreciprocalquadraticnone transformation for Yx1bothnone. Solution # Before transformation plot(y ~ x1, data = SIMDATAST) model1b &lt;- lm(y ~ x1, data = SIMDATAST) plot(model1b) # Figures suggest transforming the response plot(log(y) ~ x1, data = SIMDATAST) model2b &lt;- lm(log(y) ~ x1, data = SIMDATAST) plot(model2b) Using the data SIMDATAST, figure out the appropriate transformation for \\(x_2\\) and/or \\(y\\). Follow the steps outlined in (b). The figures suggest using a reciprocalsquare rootlogarithmicquadraticnone transformation for Yx2bothnone. Solution # Before transformation plot(y ~ x2, data = SIMDATAST) model1c &lt;- lm(y ~ x2, data = SIMDATAST) plot(model1c) # Figures suggest transforming the response plot(log(y) ~ x2, data = SIMDATAST) model2c &lt;- lm(log(y) ~ x2, data = SIMDATAST) plot(model2c) "],["exercise-2-log-transformation-on-x.html", "5 Exercise 2: Log-transformation on \\(x\\) 5.1 Statistical analysis 5.2 Assumption checking", " 5 Exercise 2: Log-transformation on \\(x\\) Rossman (1994) collected information on life expectancy in various countries of the world and the densities of people per television set and of people per physician in those countries. Our focus is to identify how female life expectancy (\\(Y\\), abbreviated to FLE) is related to the number of people per physician (\\(x\\), abbreviated to PPP). Data: LifeExp.csv Columns:                       C1: PPP - umber of people per physician                       C2: FLE - female life expectancy Read the data using life &lt;- read.csv(&quot;LifeExp.csv&quot;) Firstly, produce a scatterplot of FLE against PPP. plot(FLE ~ PPP, data = life) Figure 5.1: Scatterplot of female life expectancy versus the number of people per physician. As the relationship appears to be non-linear, we apply log-transformation to the predictor variable. Transforming the values of \\(x\\) might be the first thing to try if there is a non-linear monotonic (i.e. entirely non-increasing or entirely non-decreasing) trend in the data, and non-linearity is the only problem. In other words, model assumptions (i.e. independence, zero-mean, constant variance, normality) should be met. The scatterplot of female life expectancy against \\(\\log\\)(PPP) is produced using: plot(FLE ~ log(PPP), data = life, xlab = &quot;log(PPP)&quot;) Figure 5.2: Scatterplot of female life expectancy versus log(number of people per physician). Figure 5.2 shows a linear trend after transforming \\(x\\). Therefore, we will model the relationship between FLE and \\(\\log\\)(PPP) using a simple linear regression model. The model is given as: \\[Y_i = \\beta_0 + \\beta_1 \\log(x_i) + \\epsilon_i,\\quad \\epsilon_i \\sim N(0,\\sigma^2), \\quad i = 1,\\ldots, 37.\\] 5.1 Statistical analysis A simple linear regression model can be fitted by using the lm command: model1 &lt;- lm(FLE ~ log(PPP), data = life) The summary table and ANOVA table of this model can be produced by using summary() and anova(): summary(model1) ## ## Call: ## lm(formula = FLE ~ log(PPP), data = life) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.065 -3.489 1.143 2.663 7.674 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 109.9498 3.6860 29.83 &lt; 2e-16 *** ## log(PPP) -5.4893 0.5036 -10.90 8.48e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.286 on 35 degrees of freedom ## Multiple R-squared: 0.7724, Adjusted R-squared: 0.7659 ## F-statistic: 118.8 on 1 and 35 DF, p-value: 8.484e-13 anova(model1) ## Analysis of Variance Table ## ## Response: FLE ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## log(PPP) 1 2182.34 2182.34 118.81 8.484e-13 *** ## Residuals 35 642.91 18.37 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 5.2 Assumption checking As seen in previous examples and exercises, model assumptions can be assessed graphically by producing a plot of the residuals versus the fitted values and a normal probability plot (Q-Q plot) of the residuals. It is important to check the assumptions of your model are met before using it. plot(rstandard(model1) ~ fitted(model1)) abline(h=0, lty=3, lwd=1.5) Figure 5.3: Standardised residuals versus fitted values plot from fitting a simple linear regression model to transformed variables. # or # plot(model1, which =1) qqnorm(rstandard(model1)) qqline(rstandard(model1)) Figure 5.4: Normal probability (Q-Q) plot from fitting a simple linear regression model to transformed variables. Figure 5.3 displays the residual plots after fitting a simple linear regression model to the transformed variables. The points are fairly evenly scattered above and below the zero line, which suggests it is reasonable to assume that the random errors have mean zero. The vertical variation of the points seems to be small for small fitted values. However, there are also less points in this case. It would be preferable if more data are available. In the normal probability plot (Figure 5.4), we see that points do not exactly lie on diagonal line. This indicates that the normality assumption may not necessarily be satisfied. The independence of the random errors seems to be reasonable since each point refers to a different country. QUESTIONS: Using the following summary statistics, calculate the least squares estimates of \\(\\beta_0\\) and \\(\\beta_1\\). Check your answers with the column of Estimate in the summary table. \\[\\bar{x} = 7.18,\\ \\bar{y} = 70.51,\\ S_{xx}=72.42,\\ S_{yy}=2825.24,\\ S_{xy}=-397.56\\] Hint \\[\\hat{\\beta_0} = \\bar{y} - \\hat{\\beta_1}\\bar{x},\\ \\hat{\\beta_1} = \\frac{S_{xy}}{S_{xx}}\\] Write down the equation of the fitted model. Based on it, comment on the least squares estimate of \\(\\beta_1\\) and predict the value of female life expectancy when the number of people per physician equals 4000. Solution The regression equation is \\[\\text{FLE} = 109.95 - 5.49 \\cdot \\log(\\text{PPP})\\] \\(\\hat{\\beta_1}\\) can be interpreted as: if \\(\\log(\\text{PPP})\\) increases by 1 unit, the expected female life expectancy decreases by 5.49. When predicting the value of the response for a new observation, we need to back transform the variable. For example, if the number of people per physician is 4000, the best prediction of female life expectancy is \\(109.95 - 5.49 \\cdot \\log(4000) \\approx 64.42\\). Use the summary statistics in (a) to complete the analysis of variance table below, i.e. finding the degrees of freedom, the sum of squares, the mean squared error, and the \\(F\\)-statistic. Check your answer with the anova output. Component DF Sum Sq (SS) Mean Sq (MS) F Model Residual Total What hypotheses are being examined by the \\(F\\)-statistic in the ANOVA table? By looking at its \\(p\\)-value, what can you conclude about the fitted model? Solution The null and alternative hypotheses are: H\\(_0\\): \\(\\beta_1=0\\) versus H\\(_1\\): \\(\\beta_1 \\neq 0\\). Note that \\(\\beta_0\\) is not included in the hypothesis. Since the \\(p\\)-value (from Pr(&gt;F)) is less than 0.05, we reject the null hypothesis and conclude that \\(\\log\\)(PPP) is useful in predicting FLE. Compute and interpret the coefficient of determination, \\(R^2\\). Check your answer with the term Multiple R-squared in the summary table. Hint \\[R^2 = 1-\\frac{SSE}{SST}\\] Solution \\[\\begin{equation*} \\begin{split} R^2 &amp;= 1-\\frac{SSE}{SST}\\\\ &amp;=1-\\frac{642.91}{2182.34+642.91}\\\\ &amp;=0.772 \\end{split} \\end{equation*}\\] The \\(R^2\\) tells us the model provides a relatively adequate fit to the data with 77% of variability in FLE can be explained by this model. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
