# Example 1: Power transformation on $Y$

The `stopping.csv` file contains 63 observations of cars in the process of breaking. Our question of interest is: Can we determine if there is a relationship between the speed of cars and the distance taken to stop?

**Data:** `stopping`

**Columns:**

|                       **C1:** `Distance` - stopping distance measured in feet

|                       **C2:** `Speed` - Speed of the car as when it breaks in miles per hour

Read the data using
```{r}
stopping <- read.csv("stopping.csv")
```



## Exploratory analysis and model fitting

The scatterplot of distance ($Y$) against the speed ($x$) in Figure \@ref(fig:stopping) (left) shows that the variables do not appear to be linearly related. The temptation in this case would be to apply a quadratic transformation for $x$ give that the data are curved and concave. The centre plot in Figure \@ref(fig:stopping) shows this transformation. It is linear, but the variance has a fanning effect. For this reason, we consider transforming $Y$ using the square-root transformation ($\sqrt{Y}$), which should have a similar effect to our first attempt. The right plot in the figure now shows a linear relationship with constant variance between predictor and response.


```{r stopping, echo=FALSE, fig.cap="Left: scatterplot of *Distance* versus *Speed*. Centre: scatterplot of *Distance* versus *square root of Speed*. Right: scatterplot of *square root of Distance* versus *Speed*.", fig.align='center'}
par(mfrow=c(1,3)) # this splits your screen to obtain 1 row of figure and 2 columns
par(mar=c(4,4,2,1)) # sets margins

plot(Distance~Speed, data=stopping,
     xlab="Speed (in MPH)", ylab="Distance (in feet)", 
     pch = 16, main = "Original Scale")
plot(Distance~I(Speed^2), data=stopping,
     xlab="Squared speed (in MPH) ", ylab="Distance (in feet)", 
     pch = 16, main = "Transform Speed")
plot(sqrt(Distance)~Speed, data=stopping,
     xlab="Speed (in MPH)", ylab="Square root of distance (in feet)", 
     pch = 16, main = "Transform Distance")
```

With a linear relationship, we can more appropriately use our simple linear regression model between $\sqrt{\text{Distance}}$ (as new $Y$) and speed as
\[Y_i = \beta_0 + \beta_1 x_i + \epsilon_i,\quad \epsilon_i \sim N(0,\sigma^2), \quad i = 1,\ldots, 63.\] 

```{r, eval = F}
model.sqrt <- lm(I(Distance^0.5) ~ Speed, data = stopping)
# or 
# model.sqrt <- lm(sqrt(Distance) ~ Speed, data = stopping)
```


## Assumption checking and interpretation
Figure \@ref(fig:stopping2) gives the residual plots after fitting a simple linear regression model to the original  variables. 

```{r eval=FALSE}
model.og <- lm(Distance ~ Speed, data = stopping)
plot(rstandard(model.og) ~ fitted(model.og), 
     pch = 16)
qqnorm(rstandard(model.og))
qqline(rstandard(model.og))
```

```{r echo=FALSE, fig.align='center', out.width="70%"}
model.og <- lm(Distance ~ Speed, data = stopping)
plot(rstandard(model.og) ~ fitted(model.og), 
     pch = 16)
```

```{r stopping2, echo=FALSE, fig.cap="Residual plots from fitting a simple linear regression model to original variables. Top: Standardised residuals versus fitted values. Bottom: Normal probability (Q-Q) plot.", fig.align='center', out.width="70%"}
qqnorm(rstandard(model.og))
qqline(rstandard(model.og))
```

The plots show the problems of curvature, non-constant variance and non-normality, indicating that the wrong type of model was used.

Figure \@ref(fig:stopping3) gives the residual plots after fitting a simple linear regression model to the transformed variables. 

```{r eval=FALSE}
model.sqrt <- lm(sqrt(Distance) ~ Speed, data = stopping)
plot(rstandard(model.sqrt) ~ fitted(model.sqrt), 
     pch = 16)
qqnorm(rstandard(model.sqrt))
qqline(rstandard(model.sqrt))
```

```{r echo=FALSE, fig.align='center', out.width="70%"}
model.sqrt <- lm(sqrt(Distance) ~ Speed, data = stopping)
plot(rstandard(model.sqrt) ~ fitted(model.sqrt), 
     pch = 16)
```

```{r stopping3, echo=FALSE, fig.cap="Residual plots from fitting a simple linear regression model to transformed variables. Top: Standardised residuals versus fitted values. Bottom: Normal probability (Q-Q) plot.", fig.align='center', out.width="70%"}
qqnorm(rstandard(model.sqrt))
qqline(rstandard(model.sqrt))
```

The curvature disappears and the variance is almost constant across the range of fitted values. The normality assumption, however, remains to be invalid. This is not ideal but, on the positive side, the estimates of parameters will not be affected and hence we can still use the model to describe the relationship between variables and make predictions.  


```{r}
summary(model.sqrt)
```

The interpretation of the parameters is always done in the scale of the model. For example, we have $\beta_1 = 0.2526$, so we say that `sqrt(Distance)` changes $0.2526$ feet for every $(mile/hour)$. If you are making predictions, you can **back-transform** the results to the original scale. You can do this by performing the inverse operation that you used for the transformation. If you used $sqrt(Y)$ for the transformation, then the back-transformation requires you to square the data, $\hat{Y}^2$, to put it back in the original scale. 


**Tasks**

(a) Write down the equation of the fitted model. 
`r hide("Solution")`
The regression equation is
\[\sqrt{\text{Distance}} = 0.918 + 0.253 \cdot \text{Speed} \]
`r unhide()`

(b) Based on the regression equation in (a), comment on the relationship between speed and square root of distance. In addition, pick a speed value yourself and predict the distance for this speed. 

`r hide("Solution")`
The estimated parameter of $0.253$ suggests the square root of distance is positively linearly related to speed. As the speed increases by 1 MPH, the expected square root of distance increases by 0.253 feet. 

When predicting the value of the response, we back transform the variable as $\text{Distance} = (0.918 + 0.253 \cdot \text{Speed})^2$. For example, if the speed is 20 MPH, the predicted distance is $(0.918+0.252\cdot 20)^2 \approx 35.64$ feet.

Note that our model is built only for speed ranging from 4 to 40. It would be unwise to make predictions outside this range in the absence of other information. 
`r unhide()`
