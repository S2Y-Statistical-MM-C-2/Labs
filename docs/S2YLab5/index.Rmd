---
title: "Lab 5 - Ordinary Least Squares"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
biblio-style: apalike
link-citations: yes
rmd_files: ["index.Rmd", "Example1.Rmd", "Exercise1.Rmd", "Exercise2.Rmd","Exercise3.Rmd"]

---

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(webexercises)
library(tinytex)
library(dplyr)
library(knitr)
library(ggplot2)
library(tidyverse)
library(PASWR2)
library(MASS)
# RUBBER<- Rubber
# FERTILIZE <- FERTILIZE

```


```{r include=FALSE, echo=FALSE}
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

# Welcome to Lab 5!

Intended Learning Outcomes:

1. Introduce and understand the matrix notation of linear models.
2. Obtain model parameters using ordinary least squares (OLS) and using matrix notation in `R`. 
<!-- 3. Complete a one-sample hypothesis test of the mean in R by yourself. -->

## Introduction to Ordinary Least Squares (OLS)
The primary tool used to model associations among variables is regression (linear or otherwise).
Regression analysis is used to model the relationship between a single variable $Y$ , called
the **response** or **dependent variable**, and one or more **explanatory variables**, also called
**predictor(s)**, **covariates**, or **independent variable(s)**, $x_1, x_2,...,x_p$.

As a first step, the response variable will be continuous, but the predictor variables can be either continuous, discrete, or
categorical. You can predict non-numeric (continuous) variables using regression models, but they will be called **generalised** regression models. Here, we stick to the classical regression models. Parameter estimation for linear models can be carried out via OLS or maximum likelihood (MLE). In this lab, we will look at OLS.  

The word “regression” is due to Sir Francis Galton, who in 1885 demonstrated that
offspring do not tend toward the size of the parents; rather, offspring size tends toward the
mean of the population. That is, there is a “regression toward mediocrity.”

What else is Francis famous for? **"Whenever you can, count"**, **"Nature vs nurture"** and **"London is the epicenter of female beauty, Aberdeen is the opposite..."**. Also, eugenics. Fun guy. 

<center>
![](Images/sir_francis.png){width=300px}
<center>

## The simplest regression model: Simple Linear Regression (one covariate)##

The simple linear regression model is defined as 

\begin{equation}
Y_i = \beta_0 + \beta_1x_i + \epsilon_i,
(\#eq:SLR)
\end{equation}

where 

* $Y_i$ is the value of the response variable for the $i$th observation.
* $\beta_0$ and $\beta_1$ are regression parameters.
  + $\beta_0$ is known as the intercept.
  + $\beta_1$ is the regression coefficient of $x$ (the effect of $x$ on $Y$).
* $\epsilon_i$ is a random error term that is assumed to have a $N(o,\sigma^2)$ distribution. 

The "modelling" bit refers to estimating model parameters, $\beta_0$ and $\beta_1$ for the $n$ measurements. It looks like

$$
Y_1 = \beta_0 + \beta_1x_1 + \epsilon_1\\
Y_2 = \beta_0 + \beta_1x_2 + \epsilon_2\\
\vdots\\
Y_n = \beta_0 + \beta_1x_n + \epsilon_n.\\
$$
The model can be written in matrix notation as 
$$
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{\epsilon},
$$
where

$$\mathbf{Y} = \left[\begin{matrix} Y_1\\Y2\\ \vdots \\Y_n \end{matrix}\right] \text{ as a } (n \times 1) \text{ matrix, } \mathbf{X} = \left[\begin{matrix} 1 & x_1\\1 & x2 \\ \vdots&\vdots \\ 1 & x_n \end{matrix}\right] \text{ as } (n \times 2) \text{, } \boldsymbol{\beta} =\left[\begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right] \text{ as } (2 \times 1) \text{.} $$
Note that $\epsilon \sim N(\mathbf{0}, \sigma^2\mathbf{I})$ where $\sigma^2 \mathbf{I}$ is the variance-covariance matrix of the vector of errors. Given that $\mathbf{I}$ is the identity matrix, $\sigma^2$ is only along the diagonals, indicating that $\epsilon_i$ are independent from each other. 

## The next-simplest regression model: Multiple Linear Regression (multiple predictors)##

Multiple linear regression is an extension of simple linear regression, where we have more than one coefficient $x$. $Y$ is still the dependent variable, $\beta_0$ the intercept, but $\beta_1$ is generalised to $\beta_j$ which is now the coefficeint of $x_ij$. The model is written as 

$$
Y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \cdots + \beta_{(p-1)}x_{i(p-1)} \epsilon_i, \text{ for } i = 1, 2,...,n.
$$
The matrix notation is

The model can be written in matrix notation as 
$$
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{\epsilon},
$$

where

$$\mathbf{Y} = \left[\begin{matrix} Y_1\\Y2\\ \vdots \\Y_n \end{matrix}\right] \text{ as a } (n \times 1) \text{ matrix, } \mathbf{X} = \left[\begin{matrix} 1 & x_{11} & \cdots& x_{1(p-1)}\\1 & x_{21} & \cdots& x_{2(p-1)} \\ \vdots&\vdots&&\vdots \\ 1 & x_{n1} & \cdots& x_{n(p-1)} \end{matrix}\right] \text{ as } (n \times p) \text{, } \boldsymbol{\beta} =\left[\begin{matrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_n \end{matrix} \right] \text{ as } (p \times 1), \text{ and } \mathbf{\epsilon} = \left[\begin{matrix} \epsilon_0 \\ \epsilon_1 \\ \vdots \\ \epsilon_n \end{matrix} \right] \text{ as } (n \times 1) \text{.} $$

Note some things

* $\mathbf{X}$ contains the values of particular independent variables (predictors). 
  + $\mathbf{X}$ is assumed to be known constants.
* $\mathbf{Y}$ and $\mathbf{\epsilon}$ are random vectors whose elements are random variables. 
* $\boldsymbol{\beta}$ is a vector of unknown coefficients (what we have to estimate!). 

## Ordinary Least Squares ##

Parameter estimation can be carried out using OLS or MLE. The ordinary least squares method of estimating parameters minimizes the sum of the
squared deviations of the $Y_i$s from their expected values such that

$$
\epsilon_i = Y_i - E(Y_i) \text{ same as saying } = Y_i - (\beta_0 + \beta_1 x_i). 
$$
The estimates $\hat{\beta}_0$ of $\beta_0$ and $\hat{\beta_1}$ of $\beta_1$ seek to minimise the quantity $\mathcal{Q}$, which represents the sum of the squared residuals (differences) as 

$$
\mathcal{Q} = \sum_{i=1}^{n}\epsilon_i^2 = \sum_{i=1}^n\left((Y_i - (\beta_0 + \beta_{1}x_i)\right)^2,
$$
which, in matrix notation, is the same as saying
$$
\mathcal{Q} = \epsilon'\epsilon = (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}). 
$$
If we want parameter estimates that minimise $\mathcal{Q}$, we take the derivative with respect to our parameter

$$
\begin{aligned}
\frac{\delta \mathcal{Q}}{\delta \beta_0} & =2 \sum_{i=1}^n\left(Y_i-\beta_0-\beta_1 x_i\right)(-1) \\
& =-2 \sum_{i=1}^n\left(Y_i-\beta_0-\beta_1 x_i\right) . \\
\frac{\delta \mathcal{Q}}{\delta \beta_1} & =2 \sum_{i=1}^n\left(Y_i-\beta_0-\beta_1 x_i\right)\left(-x_i\right) \\
& =-2 \sum_{i=1}^n\left(Y_i-\beta_0-\beta_1 x_i\right)\left(x_i\right), 
\end{aligned}
$$
 set it to zero, and solve for our parameter $\hat{\beta}_j$. The solutions for $\hat{\beta}_0$ and $\hat{\beta}_1$ are
 
 $$
 \begin{aligned}
 \hat{\beta}_0 & = \overline{Y} - \hat{\beta}_1\overline{x} \\
 \hat{\beta}_1 & = \frac{\sum_{i=1}^n(x_i - \overline{x})(Y_i - \overline{Y})}{\sum_{i=1}^{n}(x_i - \overline{x})^2} = \frac{S_{XY}}{S_{XX}}.
 \end{aligned}
 $$
 
 
In matrix notation, the estimation we can generalise the estimation of $\boldsymbol{\beta}$ as 

\begin{equation}
\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}. 
(\#eq:LSE)
\end{equation}

<!-- and estimate of the error variance, which is given as -->
<!-- \begin{equation} -->
<!-- \hat{\sigma}^2=\frac{\text{RSS}}{n-p} =\frac{\mathbf{Y}'\mathbf{Y}-\mathbf{Y}'\mathbf{X}\boldsymbol{\hat{\beta}}}{n-p} -->
<!-- (\#eq:sigma) -->
<!-- \end{equation} -->

Deterministically speaking, these equations will give you the values for $\hat{\beta}_0$ and $\hat{\beta}_1$ (or $\hat{\boldsymbol{\beta}}$) that minimise $\mathcal{Q}$. If you would like the proof, let me know!


## Likely Questions
The exam for the course will contain questions on regression models. Below are example questions from a past exams. As you go through the lab, think of how each each part can be used to address the exam-style questions. 
 
<!-- \begin{enumerate} -->
1. For the Crimes example discussed in the lecture, consider the following model:

|             **Data**: $(y_i, x_i) \quad i=1,\dots,50$

|             **Model**: $\mathbb{E}(Y_i) = \beta_0+\beta_1 x_i, \quad \text{Var}(Y_i) = \sigma^2$
\
where $x_i$ denotes the \% high school dropouts in state $i$ and $y_i$ denotes the crime rate in state $i$.
<!-- \begin{enumerate} -->

|      a. Using the sum of squares function, $\mathcal{Q}(\beta_0, \beta_1) = \sum_{i=1}^{50}(Y_i-\beta_0-\beta_1 x_i)^2$, derive the least squares estimators for $\beta_0$ and $\beta_1$.
|      b. Use the summary statistics below to calculate the least squares estimates for $\beta_0$ and $\beta_1$.
<!-- \end{enumerate} -->

 \[ \sum_{i=1}^{50}Y_i =254258, \quad\, \sum_{i=1}^{50}x_i =512.6, \quad\,   \sum_{i=1}^{50}x_i^2 = 5538.8,\quad\,  \sum_{i=1}^{50}y_i^2 = 1365790732,\quad\, \sum_{i=1}^{50} x_iy_i= 2686567.5\]

<!-- \vsp -->

<br> 

2.  Consider the following model:\

|             **Data**: $(y_i, x_i) \quad i=1,\dots,n$

|             **Model**: $\mathbb{E}(Y_i) = \beta_0+\beta_1 x_i+\beta_2 x_i^2,\quad \text{Var}(Y_i) = \sigma^2$

\
This model has been proposed for the potato example used in the lectures.\

|      a. Write this model in vector-matrix form, $\mathbb{E}(\mathbf{Y}$) = $\mathbf{X}\boldsymbol{\beta}$,  clearly identifying the elements of $\mathbf{Y}$, $\mathbf{X}$ and $\boldsymbol{\beta}$.

|      b. Identify formulae (in terms of the $Y_i$'s and $x_i$'s) for the elements of $\mathbf{X}'\mathbf{X}$ and $\mathbf{X}'\mathbf{y}$.
<!-- \end{enumerate} -->




## Credit where credit is due

The labs in S2 incorporate and adapt materials from:

Ugarte, M. D., Militino, A. F., & Arnholt, A. T. (2008). [Probability and Statistics with R](https://ebookcentral.proquest.com/lib/gla/detail.action?docID=5338596). CRC press.

