[["index.html", "1 Welcome to S2Y Lab 7 1.1 Introduction 1.2 A brief reminder from Lab 6: The summary() function 1.3 Exam-style questions", " S2Y Lab 7 Examining model fit – ANOVA and \\(R^2\\) 1 Welcome to S2Y Lab 7 Intended Learning Outcomes: obtain the elements of an analysis of variance (ANOVA) table; use the ANOVA table to compute and interpret the \\(F\\)-statistic and its hypothesis test; assess the goodness of fit of linear models based on \\(R^2\\) and \\(R^2_a\\). 1.1 Introduction 1.1.1 The ANOVA table In the lectures we studied analysis of variance (ANOVA) table which can be used to investigate the variability explained by the model. It is based on partitioning the sums of squares (\\(SS\\)) and the degrees of freedom (df) associated with the response variable \\(Y\\). The total deviation, \\(Y_i - \\overline{Y}\\), can be decomposed as The deviation from the fitted value \\(\\hat{Y}_i\\) to the mean \\(\\overline{Y}\\) and The deviation fo the observation \\(Y_i\\) to the regression line \\(\\hat{Y}_i\\). \\[\\underbrace{Y_i - \\bar{Y}}_{\\text{Total Deviation}} = \\underbrace{\\hat{Y_i} - \\bar{Y}}_{\\substack{\\text{Deviation of Fitted}\\\\ \\text{Regression Value}\\\\ \\text{around the Mean}}} + \\underbrace{Y_i - \\hat{Y_i}}_{\\substack{\\text{Deviation around}\\\\ \\text{the Fitted Regression Line}}}\\] When we take the sum over all observations and square it, we obtain the following equation, which decomposes the total variability in the response into two parts:: \\[\\underbrace{\\sum_{i=1}^n(Y_i - \\bar{Y})^2}_{SST} = \\underbrace{\\sum_{i=1}^n(\\hat{Y_i} - \\bar{Y})^2}_{SSR} + \\underbrace{\\sum_{i=1}^n(Y_i - \\hat{Y_i})^2}_{SSE}\\] We have \\(SST\\): Total sum of squares \\(SSR\\): Regression sum of squares \\(SSE\\): Error (residual) sum of squares. In a simple linear regression (only one predictor), degrees of freedom can be partitioned as: \\(SST\\) has \\(n-1\\) degrees of freedom, \\(SSE\\) has \\(n-2\\), and \\(SSR\\) has \\(1\\). If we divide the \\(SS\\) by its degrees of freedom, we obtain the mean square as \\[\\frac{SSR}{1} = MSR \\text{ and } \\frac{SSE}{n-2} = MSE\\] When \\(\\beta_1 = 0\\), meaning the predictor does not have an effect in our regression, the \\(MSR\\) and \\(MSE\\) will be similar in magnitude. If \\(\\beta_1\\neq0\\), the center of the \\(MSR\\) will be larger than that of \\(MSE\\). We can use this understanding to test the effect of the predictor in the model using a hypothesis test. In particular, we could test \\(H_0: \\beta_1 = 0\\) versus \\(H_1: \\beta_1 \\neq 0\\) using the test statistic \\[F_\\text{obs} = \\frac{MSR}{MSE}\\] When the null hypothesis is true, i.e. \\(H_0: \\beta_1 = 0\\), then \\[F_\\text{obs} = \\frac{MSR}{MSE} \\sim F_{1,n-2}.\\] As a wide rule of thumb, you can say that values of \\(F_{obs}\\) close to \\(1\\) generally support the null hypothesis while larger values will support the alternative. However, you will always need to compare it with a critical value found using a statistical table or look at the \\(p\\)-value produced in R. To generate an ANOVA table on linear objects, use anova(lm.object). The full ANOVA table for a single predictor looks like this: Figure 1.1: ANOVA table for a simple linear regression model More generally, for a multiple linear regression with \\(p-1\\) predictors, the ANOVA table looks like this: Figure 1.2: ANOVA table for a multiple linear regression model We can perform \\(F\\)-test to test whether all parameters in the model of interest (i.e. all parameters excluding the intercept) are zero: \\[H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_{p-1} = 0 \\quad \\text{versus} \\quad H_1: \\text{at least one } \\beta_i \\neq 0 \\text{ for } i =1, 2, \\ldots, p-1.\\] The test statistic is again \\(F_\\text{obs} = \\frac{MSR}{MSE}\\) and under the assumption that \\(H_0\\) is true, \\[F_\\text{obs} = \\frac{MSR}{MSE} \\sim F_{p-1;n-p}.\\] 1.1.2 \\(R^2\\) ANOVA table also allows us to easily calculate the coefficient of determination, \\(R^2\\). \\(R^2\\) is a useful tool to assess the goodness-of-fit of our linear regression models. It provides a measure for the proportion of variability in the \\(Y_i\\)s that can be explained by our model. The formula of \\(R^2\\) is given by: \\[R^2 = 1-\\frac{SSE}{SST} = \\frac{SSR}{SST}. \\] Think about it this way: if \\(SSE\\) is the sum of squares of the error, it can be interpreted as the amount of variability in \\(Y\\) that is not explained by the model. \\(SST\\) represents all the variability in the data. If our model did not explain any variation, it would be equivalent to a horizontal line and \\(SSE=SST\\). Therefore, the ratio of \\(\\frac{SSE}{SST}\\) represents the proportion of variability not explained by the model and \\(1-\\frac{SSE}{SST}\\) is the variability explained. In today’s lab we will first revisit ordinary least squares estimates (OLS) and then delve into the coefficient of determination (\\(R^2\\)) and the analysis of variance (ANOVA) table to deepen our understanding of the R output. 1.2 A brief reminder from Lab 6: The summary() function Let's take a look at the summary() function again. Think of the HSwrestlers data set where we try to predict the hydrostatic fat (HWFAT) measurement of a wrestler from abomnimal fat (ABS) and tricep fat (TRICEPS) measurements. library(PASWR) data(HSwrestler) Model1 &lt;- lm(HWFAT ~ ABS + TRICEPS, data = HSwrestler) The summary function gives the following output: summary(Model1) ## ## Call: ## lm(formula = HWFAT ~ ABS + TRICEPS, data = HSwrestler) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.5558 -2.2550 -0.5245 2.3365 9.4957 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.05904 0.65219 3.157 0.0023 ** ## ABS 0.33708 0.06415 5.255 1.34e-06 *** ## TRICEPS 0.50430 0.09920 5.084 2.64e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.061 on 75 degrees of freedom ## Multiple R-squared: 0.8832, Adjusted R-squared: 0.8801 ## F-statistic: 283.6 on 2 and 75 DF, p-value: &lt; 2.2e-16 The above R output displays a few elements, which were briefly explained below. Call: This shows the formula that was used in the regression model. Residuals: This lists the five-number summary of the residuals from the regression model. Coefficients: This shows a summary of estimated coefficients of the regression model. Within this section the column headers are: Estimate: The estimated parameter. These can be used to write down the fitted regression model. Std. Error: This is the estimated standard error of the parameter estimate. t value: This is the \\(t\\)-statistic for the parameter, calculated as Estimate / Std. Error. Pr(&gt;|t|): This is the \\(p\\)-value that corresponds to the \\(t\\)-statistic, i.e. \\(2\\cdot \\mathbb{P}(X&gt;|t|)\\) for \\(X \\sim t(n-p)\\), where \\(t\\) is the t value computed above, \\(n\\) is the sample size, and \\(p\\) is the number of parameters. Significance codes: These codes in asterisks are appended to the \\(p\\)-values in regression analysis results. They provide a quick indication of the level of significance of the predictors in the model. The most commonly used significance levels and their corresponding codes are: significance code p-value *** &lt; 0.001 ** 0.001 - 0.01 * 0.01 - 0.05 . 0.05 - 0.1 ≥ 0.1 Residual standard error: This is the square root of mean squared error (MSE), where \\(MSE=\\frac{\\sum_{i=1}^n \\hat{\\epsilon}_i^2}{n-p}\\) is calculated as the sum of squared residuals divided by the degrees of freedom in the model. Multiple R-squared: The proportion of variability in the data that is explained in the model. It is calculated as \\(R^2 = 1-\\frac{SSE}{SST} = \\frac{SSR}{SST}\\). Higher values of \\(R^2\\) indicate a better model fit. Adjusted R-squared: Adjust \\(R^2\\) for increased number of predictors. \\(R^2_a = 1-\\frac{\\frac{SSE}{n-p}}{\\frac{SST}{n-1}}\\). F-statistic: This is the \\(F\\)-statistic from the ANOVA table (\\(F_\\text{obs}\\)), along with the regression degrees of freedom and error degrees of freedom, followed by the \\(p\\)-value corresponding to the \\(F\\)-test. 1.3 Exam-style questions This section is designed to help you make connections between the lab content and the exam questions. In the exam, since you will not have access to R and the data, the questions will be phrased as the example questions below. Try to think about them as you work through the examples and exercises in this lab. Question 1: Given the following summary statistics \\[\\sum_{i=1}^{20} \\left(Y_i - \\hat{Y}_i\\right)^2 = 6.94983 \\quad\\quad \\sum_{i=1}^{20} \\left(Y_i - \\bar{Y}\\right)^2 = 13.09869,\\] complete the analysis of variance table below. Question 2: The following linear model was fitted to some data: \\[\\mathbb{E}(Y_i) = \\beta_0 + \\beta_1 x_i, \\quad i = 1,\\ldots, 50.\\] The output and some summary statistics from the data are given below: \\[S_{xx} = 7.0408 \\quad\\quad S_{yy} = 6.0882 \\quad\\quad S_{xy} = 4.8616 \\quad\\quad \\bar{x} = 3.4280 \\quad\\quad \\bar{y} = 5.0060 \\]      a. Use the summary statistics to obtain the estimated parameters of \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\).          Check your answers with the column of Estimate in R.      b. Use the summary statistics to complete the analysis of variance table below,          i.e. finding the degrees of freedom, the regression sum of squares, the error sum of squares,          the mean squared error, and the \\(F\\)-statistic. Check your answer with the R output.      c. What hypotheses are being examined by the \\(F\\)-statistic in the ANOVA table?          Given that its \\(p\\)-value \\(&lt; 0.05\\), what does this tell us about the fitted model?      d. Compute and interpret the coefficient of determination, \\(R^2\\).      e. Comment on the strength of linear relationship between \\(x\\) and \\(Y\\). "],["eg-1.html", "2 Example 1: GRADES dataset 2.1 Constructing an ANOVA table 2.2 Testing for the significance of the regression model 2.3 Computing \\(R^2\\)", " 2 Example 1: GRADES dataset Recall the GRADES dataset we explored in Labs 4 and 6. Previously we built a simple linear regression model between GPA and SAT scores and verified the usefulness of SAT in predicting GPA using a \\(t\\)-test. Today, we will look at the ANOVA table produced in R, test the significance of the regression model using an \\(F\\)-test, and finally examine the \\(R^2\\) to understand how well the model performs. To open the data, type: library(PASWR2) data(GRADES) 2.1 Constructing an ANOVA table Let's construct an ANOVA table for the simple linear regression using the data in GRADES. To start, we first fit the model for gpa and sat: model.lm &lt;- lm(gpa ~ sat, data = GRADES) Then use the anova() command to obtain the ANOVA table: anova(model.lm) The ANOVA table output is shown below: ## Analysis of Variance Table ## ## Response: gpa ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## sat 1 40.397 40.397 253.18 &lt; 2.2e-16 *** ## Residuals 198 31.592 0.160 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Check if you understand everything produced in the above table. If not, ask for help! 2.2 Testing for the significance of the regression model The \\(F\\)-test is used to assess if the model of interest is useful or not. In this example, since there is only one predictor, testing the significance of the model is equivalent to testing the significance of the predictor. Therefore, we would expect the conclusion from the \\(F\\)-test to be same with that from the \\(t\\)-test in Lab 6. Let's check. TASK: Test if the regression model is significant at the \\(\\alpha=0.05\\) significance level. Step 1: Hypotheses The null and alternative hypotheses for testing the significance of the regression model are: \\[H_0: \\beta_1 = 0\\quad \\text{ versus }\\quad H_1: \\beta_1 \\neq 0\\] (Unsurprisingly, the hypotheses are identical to those in Example 1 of Lab 6, as there is only one predictor.) Step 2: Test statistic Looking at the ANOVA table above, what is the value of test statistic for this problem? \\(F_\\text{obs}=\\) Solution \\(F_\\text{obs}\\) corresponds to F value in the above ANOVA table. It is calculated as: \\[F_{obs} = \\frac{MSR}{MSE} = \\frac{40.3965}{0.1596} = 253.1824\\] Step 3: Rejection region calculation What is the critical value in this case? ANSWER \\(=\\) Solution Because \\(F_\\text{obs} \\sim F_{1,198}\\) and this is a one-tailed test, the rejection region is \\(F_\\text{obs} &gt; F_{0.95;1,198} = 3.888853\\) (value found by using the R command below). qf(0.95,1,198) ## [1] 3.888853 Step 4: Statistical conclusion From the rejection region, we rejectdo not reject \\(H_0\\) because \\(F_{obs}\\) is lies indoes not lie in the rejection region. From the \\(p\\)-value (Pr(&gt;|F|)) in the above ANOVA table), we rejectdo not reject \\(H_0\\) because the value for Pr(&gt;|F|)) is greater thanless than 0.05. Step 5: English conclusion There is sufficient evidenceinsufficient evidence that suggests the current regression model is significance, thus there exists a linear relationship exists between first-semester gpa and sat scores. 2.3 Computing \\(R^2\\) The \\(R^2\\) metric is a measurement of the proportion of variability captured by the model. It is a helpful tool to assess how well our model fits the data (goodness-of-fit). Compute \\(R^2\\) based on anova(model.lm) (it should be the same as the value of Multiple R-squared returned in summary(model.lm)). \\(R^2 =\\) Hint: How to calculate \\(R^2\\) The formula for \\(R^2\\) is \\(R^2 = 1-\\frac{SSE}{SST}\\). Solution: \\(R^2\\) \\(R^2 = 1-\\frac{SSE}{SST} = \\frac{31.592}{31.592+40.397} = 0.5612\\) "],["ex2.html", "3 Exercise 1: HSwrestler dataset 3.1 Constructing an ANOVA table and testing for the significance of the regression model. 3.2 Computing \\(R^2\\)", " 3 Exercise 1: HSwrestler dataset In Lab 4, we looked at the HSwrestler from the PASWR package, which measures the body fat of 78 high school wrestlers using three separate techniques, namely hydrostatic weighing, skin fold measurements and the Tanita body fat scale. We built a multiple linear regression model to understand the relationship between the hydrostatic fat (HWFAT; response variable) and abdominal fat (ABS; predictor variable 1) and tricep fat (TRICEPS; predictor variable 2). Our question of interest today is to test if the model is useful and comment on how well the model performs. Read in the data using: library(PASWR) data(HSwrestler) 3.1 Constructing an ANOVA table and testing for the significance of the regression model. Task: Perform a hypothesis test to assess whether the multiple linear regression model is significant. To complete the task, we will need to: Use R to construct an ANOVA table for a model where hydrostatic fat level (HWFAT) is the response variable and abdominal fat (ABS) and tricep fat (TRICEPS) are the predictor variables; Manually construct an ANOVA table which combine abdominal fat (ABS) and tricep fat (TRICEPS) are a single term of Regression as in Table 1.2; Perform the \\(F\\)-test with \\(\\alpha=0.05\\) significance level. Remember to follow the steps in Example 1. Step 1: Hypotheses Suppose the model is of the following form: \\[Y_i = \\beta_0 + \\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i.\\] The null and alternative hypotheses for testing the significance of the regression model are: \\[H_0: \\beta_1 = \\beta_2 = 0\\quad \\text{ versus }\\quad H_1: \\text{at least one }\\beta_i \\neq 0 \\text{ for } i =1,2\\] Step 2: Test statistic Use the function anova() to obtain the initial ANOVA table, where each predictor is listed as a separate row. Then, create the ANOVA table as in Table 1.2. Remember, you will need to combine certain rows to create the Regression component. Once completed the calculation, enter your answer below for each component of the ANOVA table: \\(n =\\) , \\(p =\\) \\(SSR =\\) , \\(SSE =\\) , \\(SST =\\) \\(MSR =\\) , \\(MSE =\\) Hint model.lm &lt;- lm(HWFAT ~ ABS + TRICEPS, data = HSwrestler) anova(model.lm) ## Analysis of Variance Table ## ## Response: HWFAT ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## ABS 1 5072.8 5072.8 541.365 &lt; 2.2e-16 *** ## TRICEPS 1 242.2 242.2 25.844 2.639e-06 *** ## Residuals 75 702.8 9.4 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Solution model.lm &lt;- lm(HWFAT ~ ABS + TRICEPS, data = HSwrestler) anova.tab &lt;- anova(model.lm) n &lt;- nrow(HSwrestler) p &lt;- 3 # Intercept + parameters associated with the two predictors SSR &lt;- sum(anova.tab$`Sum Sq`[1:2]) SSE &lt;- sum(anova.tab$`Sum Sq`[3]) SST &lt;- SSR + SSE MSR &lt;- sum(anova.tab$`Sum Sq`[1:2])/(p-1) MSE &lt;- sum(anova.tab$`Sum Sq`[3])/(n-p) # Produce SSR ## [1] 5315.008 SSE ## [1] 702.7837 SST ## [1] 6017.792 MSR ## [1] 2657.504 MSE ## [1] 9.370449 Now calculate the value of the test statistic \\(F_\\text{obs}\\). \\(F_\\text{obs} =\\) Hint The formula of \\(F_{obs}\\) is given by \\(F_{obs} = \\frac{MSR}{MSE}\\). Solution \\(F_\\text{obs} = \\frac{MSR}{MSE} = \\frac{2657.504}{9.370449} = 283.6043\\) Step 3: Rejection region calculation Now calculate the critical value \\(F_{1-\\alpha;p-1,n-p}\\) to define the rejection region. Remember that \\(\\alpha = 0.05\\). \\(F_{1-\\alpha;p-1,n-p} =\\) Hint Use the function qf() to obtain the quantile of the \\(F\\)-distribution. Solution qf(0.95, 2, 75) ## [1] 3.118642 Step 4: Statistical conclusion Using the rejection region approach, we rejectdo not reject \\(H_0\\) because \\(F_\\text{obs}\\) is lies indoes not lie in the rejection region. Step 5: English conclusion There is sufficient evidenceinsufficient evidence that suggests that at least one predictor (ABS and TRICEPS) has a linear relationship with the response variable (HWFAT). 3.2 Computing \\(R^2\\) Compute \\(R^2\\) based on anova(model.lm). \\(R^2 =\\) Hint The formula for \\(R^2\\) is \\(R^2 = 1-\\frac{SSE}{SST}\\) Solution \\(R^2 = 1-\\frac{SSE}{SST} = 1-\\frac{702.8}{6017.792} = 0.883213\\) Again, from the summary table output, the value of the coefficient of determination, \\(R^2\\), is %. This gives us the proportion of variation in HWFAT that is explained by the linear regression model with TRICEPS AND ABS as predictors. Hence % of the variation in the hydrostatic fat is explained by taking into account abdominal fat and tricep fat using a multiple linear regression model. Hence the model gives a badmoderategood fit to the data. The adjusted coefficient of determination, \\(R^2_a\\), is also useful in examining model fit and can be obtained from the summary table output. In this case, \\(R^2_a\\) is %, which is higherlower than the \\(R^2\\) value. This is reasonable since \\(R^2_a\\) includes a penalty when including more predictors in the model. Which of the two statistics, \\(R^2\\) and \\(R^2_a\\), is more appropriate to assess the model's goodness of fit? \\(R^2_a\\), because it takes into account the number of prdictor variables are in a model. \\(R^2\\), because it is always higher. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
