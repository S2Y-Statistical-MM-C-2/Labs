[["index.html", "1 Welcome to S2Y Lab 7 1.1 Introduction 1.2 Exam-style questions", " S2Y Lab 7 Examining model fit – ANOVA and \\(R^2\\) 1 Welcome to S2Y Lab 7 Intended Learning Outcomes: obtain the elements of an analysis of variance (ANOVA) table; use the ANOVA table to compute and interpret the \\(F\\)-statistic and its hypothesis test; assess the goodness of fit of linear models based on \\(R^2\\) and \\(R^2_a\\). 1.1 Introduction 1.1.1 The ANOVA table In the lectures we studied analysis of variance (ANOVA) table which can be used to investigate the variability explained by the model. It is based on partitioning the sums of squares (\\(SS\\)) and the degrees of freedom (df) associated with the response variable \\(Y\\). The total deviation, \\(Y_i - \\overline{Y}\\), can be decomposed as The deviation from the fitted value \\(\\hat{Y}_i\\) to the mean \\(\\overline{Y}\\) and The deviation fo the observation \\(Y_i\\) to the regression line \\(\\hat{Y}_i\\). \\[\\underbrace{Y_i - \\bar{Y}}_{\\text{Total Deviation}} = \\underbrace{\\hat{Y_i} - \\bar{Y}}_{\\substack{\\text{Deviation of Fitted}\\\\ \\text{Regression Value}\\\\ \\text{around the Mean}}} + \\underbrace{Y_i - \\hat{Y_i}}_{\\substack{\\text{Deviation around}\\\\ \\text{the Fitted Regression Line}}}\\] When we take the sum over all observations and square it, we obtain the following equation, which decomposes the total variability in the response into two parts:: \\[\\underbrace{\\sum_{i=1}^n(Y_i - \\bar{Y})^2}_{SST} = \\underbrace{\\sum_{i=1}^n(\\hat{Y_i} - \\bar{Y})^2}_{SSR} + \\underbrace{\\sum_{i=1}^n(Y_i - \\hat{Y_i})^2}_{SSE}\\] We have \\(SST\\): Total sum of squares \\(SSR\\): Regression sum of squares \\(SSE\\): Error (residual) sum of squares. In a simple linear regression (only one predictor), degrees of freedom can be partitioned as: \\(SST\\) has \\(n-1\\) degrees of freedom, \\(SSE\\) has \\(n-2\\), and \\(SSR\\) has \\(1\\). If we divide the \\(SS\\) by its degrees of freedom, we obtain the mean square as \\[\\frac{SSR}{1} = MSR \\text{ and } \\frac{SSE}{n-2} = MSE\\] When \\(\\beta_1 = 0\\), meaning the predictor does not have an effect in our regression, the \\(MSR\\) and \\(MSE\\) will be similar in magnitude. If \\(\\beta_1\\neq0\\), the center of the \\(MSR\\) will be larger than that of \\(MSE\\). We can use this understanding to test the effect of the predictor in the model using a hypothesis test. In particular, we could test \\(H_0: \\beta_1 = 0\\) versus \\(H_1: \\beta_1 \\neq 0\\) using the test statistic \\[F_\\text{obs} = \\frac{MSR}{MSE}\\] When the null hypothesis is true, i.e. \\(H_0: \\beta_1 = 0\\), then \\[F_\\text{obs} = \\frac{MSR}{MSE} \\sim F_{1,n-2}.\\] As a wide rule of thumb, you can say that values of \\(F_{obs}\\) close to \\(1\\) generally support the null hypothesis while larger values will support the alternative. However, you will always need to compare it with a critical value found using a statistical table or look at the \\(p\\)-value produced in R. To generate an ANOVA table on linear objects, use anova(lm.object). The full ANOVA table for a single predictor looks like this: More generally, for a multiple linear regression with \\(p-1\\) predictors, the ANOVA table looks like this: We can perform \\(F\\)-test to test whether all parameters in the model of interest (i.e. all parameters excluding the intercept) are zero: \\[H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_{p-1} = 0 \\quad \\text{versus} \\quad H_1: \\text{at least one } \\beta_i \\neq 0 \\text{ for } i =1, 2, \\ldots, p-1.\\] The test statistic is again \\(F_\\text{obs} = \\frac{MSR}{MSE}\\) and under the assumption that \\(H_0\\) is true, \\[F_\\text{obs} = \\frac{MSR}{MSE} \\sim F_{p-1;n-p}.\\] 1.1.2 \\(R^2\\) ANOVA table also allows us to easily calculate the coefficient of determination, \\(R^2\\). \\(R^2\\) is a useful tool to assess the goodness-of-fit of our linear regression models. It provides a measure for the proportion of variability in the \\(Y_i\\)s that can be explained by our model. The formula of \\(R^2\\) is given by: \\[R^2 = 1-\\frac{SSE}{SST} = \\frac{SSR}{SST}. \\] Think about it this way: if \\(SSE\\) is the sum of squares of the error, it can be interpreted as the amount of variability in \\(Y\\) that is not explained by the model. \\(SST\\) represents all the variability in the data. If our model did not explain any variation, it would be equivalent to a horizontal line and \\(SSE=SST\\). Therefore, the ratio of \\(\\frac{SSE}{SST}\\) represents the proportion of variability not explained by the model and \\(1-\\frac{SSE}{SST}\\) is the variability explained. In today’s lab we will first revisit ordinary least squares estimates (OLS) and then delve into the coefficient of determination (\\(R^2\\)) and the analysis of variance (ANOVA) table to deepen our understanding of the R output. 1.2 Exam-style questions This section is designed to help you make connections between the lab content and the exam questions. In the exam, since you will not have access to R and the data, the questions will be phrased as the example questions below. Try to think about them as you work through the examples and exercises in this lab. Question 1: Given the following summary statistics \\[\\sum_{i=1}^{20} \\left(Y_i - \\hat{Y}_i\\right)^2 = 6.94983 \\quad\\quad \\sum_{i=1}^{20} \\left(Y_i - \\bar{Y}\\right)^2 = 13.09869,\\] complete the analysis of variance table below. Question 2: The following linear model was fitted to some data: \\[\\mathbb{E}(Y_i) = \\beta_0 + \\beta_1 x_i, \\quad i = 1,\\ldots, 50.\\] The output and some summary statistics from the data are given below: \\[S_{xx} = 7.0408 \\quad\\quad S_{yy} = 6.0882 \\quad\\quad S_{xy} = 4.8616 \\quad\\quad \\bar{x} = 3.4280 \\quad\\quad \\bar{y} = 5.0060 \\]      a. Use the summary statistics to obtain the estimated parameters of \\(\\hat{\\beta_0}\\) and \\(\\hat{\\beta_1}\\).          Check your answers with the column of Estimate in R.      b. Use the summary statistics to complete the analysis of variance table below,          i.e. finding the degrees of freedom, the regression sum of squares, the error sum of squares,          the mean squared error, and the \\(F\\)-statistic. Check your answer with the R output.      c. What hypotheses are being examined by the \\(F\\)-statistic in the ANOVA table?          Given that its \\(p\\)-value \\(&lt; 0.05\\), what does this tell us about the fitted model?      d. Compute and interpret the coefficient of determination, \\(R^2\\).      e. Comment on the strength of linear relationship between \\(x\\) and \\(Y\\). "],["eg-1.html", "2 Example 1: GRADES dataset 2.1 Constructing an ANOVA table 2.2 Testing for the significance of the regression model 2.3 Computing \\(R^2\\)", " 2 Example 1: GRADES dataset Recall the GRADES dataset we explored in Labs 4 and 6. Previously we built a simple linear regression model between GPA and SAT scores and verified the usefulness of SAT in predicting GPA using a \\(t\\)-test. Today, we will look at the ANOVA table produced in R, test the significance of the regression model using an \\(F\\)-test, and finally examine the \\(R^2\\) to understand how well the model performs. To open the data, type: library(PASWR2) data(GRADES) 2.1 Constructing an ANOVA table Let's construct an ANOVA table for the simple linear regression using the data in GRADES. To start, we first fit the model for gpa and sat: model.lm &lt;- lm(gpa ~ sat, data = GRADES) Then use the anova() command to obtain the ANOVA table: anova(model.lm) The ANOVA table output is shown below: ## Analysis of Variance Table ## ## Response: gpa ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## sat 1 40.397 40.397 253.18 &lt; 2.2e-16 *** ## Residuals 198 31.592 0.160 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Check if you understand everything produced in the above table. If not, ask for help! 2.2 Testing for the significance of the regression model The \\(F\\)-test is used to assess if the model of interest is useful or not. In this example, since there is only one predictor, testing the significance of the model is equivalent to testing the significance of the predictor. Therefore, we would expect the conclusion from the \\(F\\)-test to be same with that from the \\(t\\)-test in Lab 6. Let's check. TASK: Test if the regression model is significant at the \\(\\alpha=0.05\\) significance level. Step 1: Hypotheses The null and alternative hypotheses for testing the significance of the regression model are: \\[H_0: \\beta_1 = 0\\quad \\text{ versus }\\quad H_1: \\beta_1 \\neq 0\\] (Unsurprisingly, the hypotheses are identical to those in Example 1 of Lab 6, as there is only one predictor.) Step 2: Test statistic Looking at the ANOVA table above, what is the value of test statistic for this problem? \\(F_\\text{obs}=\\) Solution \\(F_\\text{obs}\\) corresponds to F value in the above ANOVA table. It is calculated as: \\[F_{obs} = \\frac{MSR}{MSE} = \\frac{40.3965}{0.1596} = 253.1824\\] Step 3: Rejection region calculation What is the critical value in this case? ANSWER \\(=\\) Hint Because \\(F_\\text{obs} \\sim F_{1,198}\\) and this is a one-tailed test, the rejection region is \\(F_\\text{obs} &gt; F_{0.95;1,198} = 3.888853\\) (value found by using the R command below). qf(0.95,1,198) ## [1] 3.888853 Step 4: Statistical conclusion From the rejection region, we rejectdo not reject \\(H_0\\) because \\(F_{obs}\\) is lies indoes not lie in the rejection region. From the \\(p\\)-value (Pr(&gt;|F|)) in the above ANOVA table), we rejectdo not reject \\(H_0\\) because the value for Pr(&gt;|F|)) is greater thanless than 0.05. Step 5: English conclusion There is sufficient evidenceinsufficient evidence that suggests the current regression model is significance, thus there exists a linear relationship exists between first-semester gpa and sat scores. 2.3 Computing \\(R^2\\) Compute \\(R^2\\) based on anova(model.lm) (it should be the same as the value returned in summary(model.lm)) \\(R^2 =\\) Hint: How to calculate \\(R^2\\) The formula for \\(R^2\\) is \\(R^2 = \\frac{SST}{SST + SSE}\\) Solution: \\(R^2\\) \\(R^2 = \\frac{SST}{SST + SSE} = \\frac{40.397}{40.397+31.592} = 0.5612\\) "],["ex2.html", "3 Exercise 1: HSWRESTLER dataset 3.1 Constructing an ANOVA table 3.2 Testing for the significance of the regression model 3.3 Computing \\(R^2\\)", " 3 Exercise 1: HSWRESTLER dataset In Lab 4, we looked at the HSWRESTLER from the PASWR package, which measures the body fat of 78 high school wrestlers using three separate techniques, namely hydrostatic weighing, skin fold measurements and the Tanita body fat scale. dataset we explored in Lab 4 and 6. The question of interest is to investigate how hydrostatic fat (HWFAT) is related to abdominal fat (ABS) and tricep fat (TRICEPS). Read in the data using: library(PASWR) data(HSWRESTLER) 3.1 Constructing an ANOVA table Construct an ANOVA table using the data in HSWRESTLER. Then, test if a linear relationship exists between the hydrostatic fat level (hwfat) and tricep fat (triceps) using the information in the ANOVA table at the \\(\\alpha = 0.05\\) level. Start by fitting the model and computing the ANOVA table. Hint: ANOVA table Use the function anova() to obtain the ANOVA table. Solution: ANOVA table code and output model.lm &lt;- lm(hwfat ~ triceps, data = HSWRESTLER) anova(model.lm) ## Analysis of Variance Table ## ## Response: hwfat ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## triceps 1 5056.3 5056.3 399.65 &lt; 2.2e-16 *** ## Residuals 76 961.5 12.7 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.2 Testing for the significance of the regression model Test if a linear relationship exists between hwfat and triceps using the information in the ANOVA table at the \\(\\alpha = 0.05\\) level. Step 1: Calculate the rejection region, i.e., \\(F_{obs}\\). \\(F_{obs} =\\) Hint: How to calculate \\(F_{obs}\\) Because \\(F_{obs} \\sim F_{1,76}\\) and this is a one-tailed test, the rejection region is \\(F_{obs} &gt; F_{0.95;1,76} = 3.9691\\) (from the statistical tables). The value of the standardised test statistic is \\(F_{obs} = \\frac{MSR}{MSE}\\). This is also found in the ANOVA table output (note that the values will be slightly different due to the rounding). Solution: \\(F_{obs}\\) \\(F_{obs} = \\frac{MSR}{MSE} = \\frac{5056.3}{12.7} = 398.1339\\) Step 2: Use the p-value (Pr(&gt;|F|))) in the ANOVA table to determine whether to reject \\(H_0\\) or not. 1. From the rejection region, we rejectdo not reject \\(H_0\\) because \\(F_{obs}\\) is less thangreater than 3.9691. 2. From the p-value, we rejectdo not reject \\(H_0\\) because the value for Pr(&gt;|F|)) is greater thanless than 0.05. Step 3: Interpret your results. There is strong evidenceinsufficient evidence that suggests a linear relationship exists between the variables hwfat and triceps. 3.3 Computing \\(R^2\\) Compute \\(R^2\\) based on anova(model.lm) (it should be the same as the value returned in summary(model.lm)) \\(R^2 =\\) Hint: How to calculate \\(R^2\\) The formula for \\(R^2\\) is \\(R^2 = \\frac{SST}{SST + SSE}\\) Solution: \\(R^2\\) \\(R^2 = \\frac{SST}{SST + SSE} = \\frac{5056.3}{5056.3+961.5} = 0.8402\\) Again, from the summary table output, the value of the coefficient of determination, \\(R^2\\), is %. This gives us the percentage of variation in hwfat that is explained by the linear regression model with triceps as a predictor. Hence % of the variation in the price of a diamond ring is explained by taking into account the weight of the diamond using out simple linear regression model. Hence the model gives a relatively goodan excellenta bad fit to the data. The adjusted coefficient of determination, \\(R^2_a\\), is also useful in examining model fit and can be obtained from the summary table output. In this case, \\(R^2_a\\) is %, which is higherlower than the \\(R^2\\) value. Which of the two statistics, \\(R^2\\) and \\(R^2_a\\), is more appropriate to assess the model goodness of fit? \\(R^2_a\\), because it takes into account how many variables are in a model to determine the most appropriate variables to include. \\(R^2\\), because it is always higher. A multiple linear regression model has provided a goodmoderatebad model for predicting hydrostatic fat from abdominal fat and tricep fat, as nonesomemost of the variability in the response has been explained. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
