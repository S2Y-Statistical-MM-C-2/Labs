---
title: "Lab 9 - Transformations and Assumptions"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
biblio-style: apalike
link-citations: yes
rmd_files: ["index.Rmd", "Example1.Rmd","Example2.Rmd", "Exercise1.Rmd", "Exercise2.Rmd"]

---
```{r include=FALSE, cache=FALSE}
library(webexercises)
```

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(webexercises)
library(tinytex)
library(dplyr)
library(knitr)
library(ggplot2)
library(tidyverse)
library(PASWR2)
library(MASS)
library(VGAM)
# RUBBER<- Rubber
# FERTILIZE <- FERTILIZE

```


```{r include=FALSE, echo=FALSE}
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

# Welcome to Lab 9!

Intended Learning Outcomes:

* Produce residual plots and assess the assumptions of a linear model. 
* Fit linear models with transformed variables. 

## Introduction to variable transformations
The need to transform variables may arise during the exploratory analysis when you realize that your predictor(s) do not have a linear relationship with the response, or during assumption checking when the residuals do not meet assumptions. In either case, a transformation of the response variable, the predictors, or both response and predictors may be necessary.

Once the model has been fitted using a transformation, the results will be given in the transformed scale. It is still possible to report the findings in the original scale, but this requires a back-transformation. How do I back-transform my data, you ask? Simply apply the inverse operation (inverse of the transformation) to your predictions.  

In this lab, we will explore some simple transformations and how to tell if you need them. All examples are a simple linear regression where $Y = \beta_0 + \beta_1 x + \epsilon$, where $Y$ is the response and $x$ is the predictor. 

## Root 
A root transformation is one where you take a root of the variable. For example, you may take the square or cubic root. Recognizing the need for the transformation is important. Here is a small simulation to give you a taste for a predictor in need of a square root transformation looks like. In this example, $\beta_0 = 1$ and $\beta_1 = 2$. 
```{r, include = FALSE, eval = TRUE, warnings= FALSE, message = FALSE, echo = FALSE}
set.seed(123) # Ignore this line

# Simulate data from a simple linear model
n <- 100
beta_0 <- 1
beta_1 <- 2
x.trans <- runif(n, min = 0, max =10)
Y <- beta_0 + beta_1*x.trans + rnorm(n, 0, sd = 1)
x <- x.trans^2
```

```{r root, fig.cap="Left: Scatterplot of Y vs x in the original scale. Right: Scatterplot after the square-root transformation of $x$", fig.align='center'}
par(mfrow = c(1,2))
plot(Y~x, pch = 16, 
     main = "Original Scale")
plot(Y~I(x^0.5), pch = 16, 
    main = "After Square-root Transformation")

```
As you can see in the plots, the transformation may be required if the predictor displays a convex curved relationship with the response. If you think this is you, you can apply the transformation in the model using:
```{r, eval = FALSE}
# Model without transformation
model <- lm(y ~ x)
# Model with transformation
model.root <- lm(y ~ I(x^0.5))
```

## Reciprocal
The reciprocal transformation of $x$ is the same as saying $\frac{1}{x}$ or $x^{-1}$. We performed another simulation to show you what data in need of the reciprocal transformation look like. Here, $\beta_0 = 1$ and $\beta1 = 1.5$. 
```{r, include = FALSE, eval = TRUE, warnings= FALSE, message = FALSE, echo = FALSE}
set.seed(123) # Ignore this line

# Simulate data from a simple linear model
beta_0 <- 1
beta_1 <- 1.5
x.trans <- runif(n, min = 0.2, max =2)
Y <- beta_0 + beta_1*x.trans + rnorm(n, 0, sd = 1)
x <- 1/x.trans
```

```{r reciprocal, fig.cap="Left: Scatterplot of Y vs x in the original scale. Right: Scatterplot after the reciprocal transformation of $x$", fig.align='center'}
par(mfrow = c(1,2))
plot(Y~x, pch = 16, 
     main = "Original Scale")
plot(Y~I(x^-1), pch = 16, 
    main = "After Reciprocal Transformation")

```
 The figure shows a concave curve that slopes downward. The response is largest when $x \leq 1$ because $\frac{1}{x} > 1$ when $x < 1$. Using the same logic, we understand that large values of $x$ result in smaller values of $Y$.  
 
 You can fit it in a model using:
```{r, eval = FALSE}
# Model without transformation
model <- lm(y ~ x)
# Model with transformation
model.root <- lm(y ~ I(x^-1))
```



## Quadratic
The quadratic transformation of $x$ is the same as saying $x^2$. It is the inverse of the square-root transformation. It it a subset of the power transformations. In more general terms, it is $x^a$, where $a$ is anything. The reciprocal can also be thought of in this same category. 

But what does it look like? The figures below are a simulated example using $\beta_0=1$ and $\beta_1=2$. 
```{r, include = FALSE, eval = TRUE, warnings= FALSE, message = FALSE, echo = FALSE}
set.seed(123) # Ignore this line

# Simulate data from a simple linear model
n <- 500
beta_0 <- 1
beta_1 <- 2
x.trans <- runif(n, min = 0, max =2)
Y <- beta_0 + beta_1*x.trans + rnorm(n, 0, sd = 0.25)
x <- sqrt(x.trans)
```

```{r quadratic, fig.cap="Left: Scatterplot of Y vs x in the original scale. Right: Scatterplot after the quadratic transformation of $x$", fig.align='center'}
par(mfrow = c(1,2))
plot(Y~x, pch = 16, 
     main = "Original Scale")
plot(Y~I(x^2), pch = 16, 
    main = "After Quadratic Transformation")

```

As you can see from the plots, the predictor has a concave relationship with the response previous to the quadratic transformation. You can use it in a model by using the code:

```{r, eval = FALSE}
# Model without transformation
model <- lm(y ~ x)
# Model with transformation
model.root <- lm(y ~ I(x^2))
```

## Logarithmic
Last but not least, the logarithmic transformation is equivalent to saying $\ln(x)$, which in `R` is the same as `log(x)`. Here is another simulation that shows the original and transformed predictor when the transformation should be logarithmic. 
```{r, include = FALSE, eval = TRUE, warnings= FALSE, message = FALSE, echo = FALSE}
set.seed(123) # Ignore this line

# Simulate data from a simple linear model
n <- 100
beta_0 <- 1
beta_1 <- 2
x.trans <- runif(n, min = 0, max =5)
Y <- beta_0 + beta_1*x.trans + rnorm(n, 0, sd = 1.5)
x <- exp(x.trans)
```

```{r log1, fig.cap="Left: Scatterplot of Y vs x in the original scale. Right: Scatterplot after the log transformation of $x$", fig.align='center'}
par(mfrow = c(1,2))
plot(Y~x, pch = 16, 
     main = "Original Scale")
plot(Y~log(x), pch = 16, 
    main = "After Log Transformation")

```

Fitting the transformation in `R` is straight forward as in the previous transformations. You can use:
```{r, eval = FALSE}
# Model without transformation
model <- lm(y ~ x)
# Model with transformation
model.root <- lm(y ~ log(x))
```


Another useful feature of the logarithmic transformation is that it can help when applied to the response variable too so that both predictor and response are log-transformed. In the original scale, the data can look like this:

```{r, include = FALSE, eval = TRUE, warnings= FALSE, message = FALSE, echo = FALSE}
set.seed(123) # Ignore this line

# Simulate data from a simple linear model
n <- 100
beta_0 <- 1
beta_1 <- 2
x.trans <- runif(n, min = 0, max =2)
Y.trans <- beta_0 + beta_1*x.trans + rnorm(n, 0, sd = 0.5)
x <- exp(x.trans)
Y <- exp(Y.trans)
```

```{r log2, fig.cap="Left: Scatterplot of Y vs x in the original scale. Right: Scatterplot after the log transformation of $x$ and $Y$", fig.align='center'}
par(mfrow = c(1,2))
plot(Y~x, pch = 16, 
     main = "Original Scale")
plot(log(Y)~log(x), pch = 16, 
    main = "After Log Transformation")

```
The main thing to note in these plots is the increasing variance the further away from the origin. Sort of like a "fanning" effect. The transformation is easy to fit using:
```{r, eval = FALSE}
# Model with transformation
model.root <- lm(log(y) ~ log(x))
```


You will not need to know this for the exam, but as a special feature, models that use logarithmic transformations have approximate interpretations without the need for back-transformation. When $x$ has been transformed with a natural log transformation, the change in the $\ln(x)$ is roughly equal to the change in $x$ provided the changes in $x$ are small. Consider the figure below, which graphically illustrates how changing the $x$ values 3 and 6 by 10% corresponds to an approximate increase in $\ln(x)$ of about 10%.

<center>
![Small change in $x$ gives a similar small change in $\ln(x)$](Images/fig_logs.png){width=400px, height=300px}
</center>










<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Example 1: Power transformation on $Y$

The `stopping.csv` file contains 63 observations of cars in the process of breaking. Our question of interest is: Can we determine if there is a relationship between the speed of cars and the distance taken to stop?

**Data:** `stopping`

**Columns:**

|                       **C1:** `Distance` - stopping distance measured in feet

|                       **C2:** `Speed` - Speed of the car as when it breaks in miles per hour

Read the data using
```{r}
stopping <- read.csv("stopping.csv")
```



## Exploratory analysis and model fitting

The scatterplot of distance ($Y$) against the speed ($x$) in Figure \@ref(fig:stopping) (left) shows that the variables do not appear to be linearly related. The temptation in this case would be to apply a quadratic transformation for $x$ give that the data are curved and concave. The centre plot in Figure \@ref(fig:stopping) shows this transformation. It is linear, but the variance has a fanning effect. For this reason, we consider transforming $Y$ using the square-root transformation ( $\sqrt{Y}$), which should have a similar effect to our first attempt. The right plot in the figure now shows a linear relationship with constant variance between predictor and response.
<!-- Transforming the response variable $Y$ may be considered when the assumption of constant variances is violated, particularly when the variance of $Y$ increases with its mean. It may also be considered when the residuals are not normally distributed due to the distribution of $Y$ is highly skewed or does not approximate a normal distribution. As an added bonus, transforming $Y$ may also help mitigate the problem of a curved relationship.  -->

```{r stopping, echo=FALSE, fig.cap="Left: scatterplot of *Distance* versus *Speed*. Right: scatterplot of *square root of Distance* versus *Speed*.", fig.align='center'}
par(mfrow=c(1,2)) # this splits your screen to obtain 1 row of figure and 2 columns
par(mar=c(4,4,2,1)) # sets margins

plot(Distance~Speed, data=stopping,
     xlab="Speed (in MPH)", ylab="Distance (in feet)", 
     pch = 16, main = "Original Scale")
plot(Distance~I(Speed^2), data=stopping,
     xlab="Squared speed (in MPH) ", ylab="Distance (in feet)", 
     pch = 16, main = "Transform Speed")
plot(sqrt(Distance)~Speed, data=stopping,
     xlab="Speed (in MPH)", ylab="Square root of distance (in feet)", 
     pch = 16,"Transform Distance")
```

With a linear relationship, we can more appropriately use our simple linear regression model between $\sqrt{\text{Distance}}$ (as new $Y$) and speed as
\[Y_i = \beta_0 + \beta_1 x_i + \epsilon_i,\quad \epsilon_i \sim N(0,\sigma^2), \quad i = 1,\ldots, 63.\] 

```{r, eval = F}
model.sqrt <- lm(I(Distance^0.5) ~ Speed, data = stopping)
# or 
# model.sqrt <- lm(sqrt(Distance) ~ Speed, data = stopping)
```


## Assumption checking and Interpretation
Figure \@ref(fig:stopping2) gives the residual plots after fitting a simple linear regression model to the original  variables. 

```{r echo=FALSE, fig.align='center', out.width="70%"}
model.og <- lm(Distance ~ Speed, data = stopping)
plot(rstandard(model.og) ~ fitted(model.og), 
     pch = 16)
```

```{r stopping2, echo=FALSE, fig.cap="Residual plots from fitting a simple linear regression model to original variables. Top: Standardised residuals versus fitted values. Bottom: Normal probability (Q-Q) plot.", fig.align='center', out.width="70%"}
qqnorm(rstandard(model.og))
qqline(rstandard(model.og))
```

The plots show the problems of curvature, non-constant variance and non-normality, indicating that the wrong type of model was used.

Figure \@ref(fig:stopping3) gives the residual plots after fitting a simple linear regression model to the transformed variables. 

```{r echo=FALSE, fig.align='center', out.width="70%"}
model.sqrt <- lm(sqrt(Distance) ~ Speed, data = stopping)
plot(rstandard(model.sqrt) ~ fitted(model.sqrt), 
     pch = 16)
```

```{r stopping3, echo=FALSE, fig.cap="Residual plots from fitting a simple linear regression model to transformed variables. Top: Standardised residuals versus fitted values. Bottom: Normal probability (Q-Q) plot.", fig.align='center', out.width="70%"}
qqnorm(rstandard(model.sqrt))
qqline(rstandard(model.sqrt))
```

The curvature disappears and the variance is almost constant across the range of fitted values. The normality assumption, however, remains to be invalid. This is not ideal but, on the positive side, the estimates of parameters will not be affected and hence we can still use the model to describe the relationship between variables and make predictions.  


```{r}
summary(model.sqrt)
```

The interpretation of the parameters is always done in the scale of the model. For example, we have $\beta_1 = 0.2526$, so we say that `sqrt(Distance)` changes $0.2526$ feet for every $(mile/hour)$. If you are making predictions, you can **back-transform** the results to the original scale. You can do this by performing the inverse operation that you used for the transformation. If you used $sqrt(Y)$ for the transformation, then the back-transformation requires you to square the data, $\hat{Y}^2$, to put it back in the original scale. 


**Tasks**

(a) Write down the equation of the fitted model. 
`r hide("Solution")`
The regression equation is
\[\sqrt{\text{Distance}} = 0.918 + 0.253 \cdot \text{Speed} \]
`r unhide()`

(b) Based on the regression equation in (a), comment on the relationship between speed and square root of distance. In addition, pick a speed value yourself and predict the distance for this speed. 

`r hide("Solution")`
The estimated parameter of $0.253$ suggests the square root of distance is positively linearly related to speed. As the speed increases by 1 MPH, the expected square root of distance increases by 0.253 feet. 

When predicting the value of the response, we back transform the variable as $\text{Distance} = (0.918 + 0.253 \cdot \text{Speed})^2$. For example, if the speed is 20 MPH, the predicted distance is $(0.918+0.252\cdot 20)^2 \approx 35.64$ feet.

Note that our model is built only for speed ranging from 4 to 40. It would be unwise to make predictions outside this range in the absence of other information. 
`r unhide()`

<!--chapter:end:Example1.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Example 2: Animal brains and double transformations

Variables are often transformed to ﬁx constant variance or normality assumptions; however, transformations can complicate the interpretation of the model. In this example, we explore transforming both the response and the predictor(s). To illustrate what we mean, we will use the data frame `Animals` from the `MASS` package. The data consists of 28 animals, including dinosaurs, and their respective body weight in kilograms and brain weight in grams. Our question of interest is: is a bigger brain required to govern a bigger body?

**Data: `Animals`**

**Columns:**

|                       **C1:** `body` - body weight given in kilograms

|                       **C2:** `brain` - weight of brains given in grams

Read the data using:
```{r}
library(MASS)
data(Animals)
```

**Exploratory analysis and model fitting **
As always, it is important to first plot the data to understand the relationship between response and predictor in the original scale. Before you start, make sure to remove the dinosaurs from the data (observations 26 to 28 after sorting), because they are outliers (which we know from previous labs!).  
```{r, include = TRUE}
# Sort by body size
SA <- Animals[order(Animals$body), ]
# Remove dinos
NoDINO <- SA[-c(26:28),]
# Fit the model
plot(NoDINO, pch = 16, xlab = "Body", ylab = "Brain")
```

From the figure above, we can see the response and predictor have a difficult relationship that is curved (convex) and increases in variability. Transforming both variables may be useful in this case. From the introduction, we know logarithmic transformations are often a good choice. 

You can fit this model in the same way that you have fitted the other transformations. 

```{r, include = TRUE}
# Sort by body size
SA <- Animals[order(Animals$body), ]
# Remove dinos
NoDINO <- SA[-c(26:28),]
# Fit the model
simple.model <- lm(log(brain) ~ log(body), data = NoDINO)
coef(simple.model)
```


**Interpretation**
The predicted log(brain weight) of the jaguar, whose body weight is listed as $100 \text{kg}$ with the ﬁtted model, is $2.1504+0.7523\times\text{ln}(100)=5.6147$. 
This must be back transformed to get the units of original brain measurement (grams). The brain weight predicted by the model is $\exp(5.6147) = 274.4312 \text{g}$. \ 


When both the response and the predictors have been transformed with a natural logarithm, one can use the percentage interpretation of $\beta_1$ and be very close to the actual change given by the model for small changes in the $x$-variable. If the body weight of an animal increases by $1%$, the approximate increase in brain weight
is $(0.01\times0.7523 = 0.007523 = 0.7523\%)$ since $\hat{\beta}_1= 0.7523$. If the jaguar were to increase its weight by $10\%$, the expected increase in brain weight would be approximately $7.5\%$ for a new weight of $1.075 \times 274.4312 = 295.0136 \text{g}$. The actual brain weight change predicted by the model for a body weight of $110 \text{kg}$ is $294.83 \text{g}$ and
the change in brain weight as predicted from the model is $7.4331\%$ (see Table 1). Note that for this model, $\hat{\beta}_1= 0.7523 \approx \% \Delta Y /\% \Delta x = 0.0743/0.1 = 0.7433$. 
In fact,

<center>
![Table 1. Actual change in jaquar brain weight. ](Images/tab1.png){width=300px}
</center>

<!-- Just FYI, more generally, for models of the form $\text{ln}(Y) = \beta_0 + \beta_1x$, for each unit of increase in $x$, $Y$ increases roughly by $\beta_1 \times 100\%$. -->


<!--chapter:end:Example2.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Exercise 1: Identify the appropriate transformation

The following simulated data contain different non-linear relationships between response and predictor
variables. 

**Data:** `SIMDATAXT` and `SIMDATAST`

**Columns:** 

|                       **C1:** `x1` - predictor 1

|                       **C2:** `x2` - predictor 2

|                       **C3:** `x3` - predictor 3

Read in the data using:
```{r, include = T, message=FALSE, warning = FALSE}
library(PASWR2)
data("SIMDATAXT")
data("SIMDATAST")
```

**QUESTIONS**: 

(a) Using the data `SIMDATAXT`, apply appropriate transformations to $x_1$, $x_2$, and $x_3$ to linearise the relationships between the response and predictors one at a time. 

It might be best if you break it down and address one predictor at the time. 

**Transform $x_1$**
You can begin by examining the relationship between $y$ and $x_1$ at the original scale and in a simple linear model. 
`r hide ("Hint")`
```{r}
# Plot relationship
plot(y ~ x1, data = SIMDATAXT)
# Fit and plot model
model1 <- lm(y ~ x1, data = SIMDATAXT)
plot(model1,which=c(1))
```
The relationship between $y$ and $x_1$ is curved and convex. 
`r unhide ()`

Apply the transformation you think is necessary and fit a model.\ 
From the figures, the appropriate transformation of $x_1$ is the `r mcq(c(answer="square root", "quadratic", "logarithmic", "reciprocal", "none"))` . 

`r hide ("Solution")`
```{r, eval = FALSE, include = TRUE}
# Before transformation
plot(y ~ x1, data = SIMDATAXT)
model1 <- lm(y ~ x1, data = SIMDATAXT)
plot(model1,which=c(1)
# Figures suggest a square root transformation
plot(y ~ sqrt(x1), data = SIMDATAXT)
model2 <- lm(y ~ sqrt(x1), data = SIMDATAXT)
plot(model2,which=c(1)) # Much better!
```
`r unhide()`

<!-- From all the figuresof $x_1$ against $Y$ to analize the relationship between predictor and response. Are transformations necessary? -->


**Transform $x_2$**
The second predictor can be evaluated in the same way as the first. You can begin by examining the relationship between $y$ and $x_2$ and the relationship between the two in a simple linear model using scatterplots. 

`r hide ("Hint")`

```{r}
# Plot relationship
plot(y ~ x2, data = SIMDATAXT)
# Fit and plot model
model3 <- lm(y ~ x2, data = SIMDATAXT)
plot(model3,which=c(1))
```
The relationship seems both curved and concave. 
`r unhide ()`

Apply the transformation you think is necessary and fit a model. \
From the figures, the appropriate transformation of $x_2$ is  `r mcq(c(answer="quadratic", "square root", "logarithmic","reciprocal", "none"))`. 

`r hide ("Solution")`
```{r, eval = FALSE, include = TRUE}
# Before transformation
plot(y ~ x2, data = SIMDATAXT)
model3 <- lm(y ~ x2, data = SIMDATAXT)
plot(model3,which=c(1))
# Figures suggest a square root transformation
plot(y ~ I(x2^2), data = SIMDATAXT)
model4 <- lm(y ~ I(x2^2), data = SIMDATAXT)
plot(model4,which=c(1)) 
```
`r unhide()`



**Transform $x_3$**
Plot $y$ and $x_3$ and the residuals of a simple linear model to assess the relationship between the two. 

`r hide ("Hint")`

```{r}
# Plot relationship
plot(y ~ x3, data = SIMDATAXT)
# Fit and plot model
model5 <- lm(y ~ x3, data = SIMDATAXT)
plot(model5,which=c(1))
```
The relationship seems curved and concave. 
`r unhide ()`

Apply the transformation you think is necessary and fit a model. \
The figures suggest a `r mcq(c(answer="reciprocal", "square root", "logarithmic","quadratic", "none"))` transformation for $x_3$. 


`r hide ("Solution")`
```{r, eval = FALSE, include = TRUE}
# Before transformation
plot(y ~ x3, data = SIMDATAXT)
model5 <- lm(y ~ x3, data = SIMDATAXT)
plot(model5)
# Figures suggest a square root transformation
plot(y ~ I(x3^(-1)), data = SIMDATAXT)
model6 <- lm(y ~ I(x2^(-1)), data = SIMDATAXT)
plot(model6) 
```
`r unhide()`



(b) Using the data `SIMDATAST`, figure out the appropriate transformation for $x_1$ and/or $y$. 
Follow the usual steps by producing a scatterplot of $y$ and $x_1$ and the residuals of the simple linear model at original scale. Consider the possible transformations. 

The figures suggest using a `r mcq(c(answer="logarithmic", "square root", "reciprocal","quadratic", "none"))` transformation for `r mcq(c(answer="Y","x1","both","none"))`. 

`r hide ("Solution")`
```{r, eval = FALSE, include = TRUE}
# Before transformation
plot(y ~ x1, data = SIMDATAST)
model1b <- lm(y ~ x1, data = SIMDATAST)
plot(model1b)
# Figures suggest transforming the response
plot(log(y) ~ x1, data = SIMDATAST)
model2b <- lm(log(y) ~ x1, data = SIMDATAST)
plot(model2b) 
```
`r unhide()`

(c) Using the data `SIMDATAST`, figure out the appropriate transformation for $x_2$ and/or $y$.
Follow the steps outlined in (b). 

The figures suggest using a `r mcq(c(answer="reciprocal", "square root", "logarithmic","quadratic","none"))` transformation for `r mcq(c(answer="Y","x2","both","none"))`. 

`r hide ("Solution")`
```{r, eval = FALSE, include = TRUE}
# Before transformation
plot(y ~ x2, data = SIMDATAST)
model1c <- lm(y ~ x2, data = SIMDATAST)
plot(model1c)
# Figures suggest transforming the response
plot(log(y) ~ x2, data = SIMDATAST)
model2c <- lm(log(y) ~ x2, data = SIMDATAST)
plot(model2c) 
```
`r unhide()`

<!--chapter:end:Exercise1.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Exercise 2: Log-transformation on $x$

Rossman (1994) collected information on life expectancy in various countries of the world and the densities of people per television set and of people per physician in those countries. Our focus is to identify how female life expectancy ($Y$, abbreviated to FLE) is related to the number of people per physician ($x$, abbreviated to PPP).

**Data:** `LifeExp.csv`

**Columns:**

|                       **C1:** `PPP` - umber of people per physician

|                       **C2:** `FLE` - female life expectancy

Read the data using
```{r}
life <- read.csv("LifeExp.csv")
```


Firstly, produce a scatterplot of FLE against PPP.
```{r fig.cap="Scatterplot of female life expectancy versus the number of people per physician.", fig.align='center', out.width="70%"}
plot(FLE ~ PPP, data = life)
```

<!-- We can say that the relationship between female life expectancy and the number of people per physician? Do we need to consider transformations? A transformation of the predictor variable(s) may be useful if there is a non-linear monotonic (i.e. entirely non-increasing or entirely non-decreasing) trend in the data, and non-linearity is the only problem. -->
<!-- In other words, assumptions on the error term (i.e. independence, zero-mean, constant variance, normality) should be met. -->

As the relationship appears to be non-linear, we apply log-transformation to the predictor variable. Transforming the values of $x$ might be the first thing to try if there is a non-linear monotonic (i.e. entirely non-increasing or entirely non-decreasing) trend in the data, and non-linearity is the only problem. In other words, model assumptions (i.e. independence, zero-mean, constant variance, normality) should be met. The scatterplot of female life expectancy against $\log$(PPP) is produced using:

```{r logX, fig.cap="Scatterplot of female life expectancy versus log(number of people per physician).", fig.align='center', out.width="70%"}
plot(FLE ~ log(PPP), data = life, xlab = "log(PPP)")
```

Figure \@ref(fig:logX) shows a linear trend after transforming $x$. Therefore, we will model the relationship between FLE and $\log$(PPP) using a simple linear regression model. The model is given as:
\[Y_i = \beta_0 + \beta_1 \log(x_i) + \epsilon_i,\quad \epsilon_i \sim N(0,\sigma^2), \quad i = 1,\ldots, 37.\]

## Statistical analysis

A simple linear regression model can be fitted by using the `lm` command:

```{r}
model1 <- lm(FLE ~ log(PPP), data = life)
```

The summary table and ANOVA table of this model can be produced by using `summary()` and `anova()`:
```{r}
summary(model1)
anova(model1)
```

```{r echo=FALSE, results='hide'}
x <- log(life$PPP)
y <- life$FLE
n_digits <- 2
x_mean <- round(mean(x),n_digits)
y_mean <- round(mean(y),n_digits)
S_xy <- round(sum((x-mean(x))*(y-mean(y))),n_digits)
S_xx <- round(sum((x-mean(x))^2),n_digits)
S_yy <- round(sum((y-mean(y))^2),n_digits)
print(c(x_mean, y_mean, S_xx, S_yy, S_xy))
```

## Assumption checking

As seen in previous examples and exercises, model assumptions can be assessed graphically by producing a plot of the residuals versus the fitted values and a normal probability plot (Q-Q plot) of the residuals. It is important to check the assumptions of your model are met before using it. 

```{r ResVsFitted, fig.cap="Standardised residuals versus fitted values plot from fitting a simple linear regression model to transformed variables.", fig.align='center'}
plot(rstandard(model1) ~ fitted(model1))
abline(h=0, lty=3, lwd=1.5)
# or
# plot(model1, which =1)
```

```{r QQ-plot, fig.cap="Normal probability (Q-Q) plot from fitting a simple linear regression model to transformed variables.", fig.align='center'}
qqnorm(rstandard(model1))
qqline(rstandard(model1))
```

Figure \@ref(fig:ResVsFitted) displays the residual plots after fitting a simple linear regression model to the transformed variables. The points are fairly evenly scattered above and below the zero line, which suggests it is reasonable to assume that the random errors have mean zero. The vertical variation of the points seems to be small for small fitted values. However, there are also less points in this case. It would be preferable if more data are available. In the normal probability plot (Figure \@ref(fig:QQ-plot)), we see that points do not exactly lie on diagonal line. This indicates that the normality assumption may not necessarily be satisfied. The independence of the random errors seems to be reasonable since each point refers to a different country.

**QUESTIONS**:

(a) Using the following summary statistics, calculate the least squares estimates of $\beta_0$ and $\beta_1$. Check your answers with the column of `Estimate` in the summary table.

\[\bar{x} = 7.18,\ \bar{y} = 70.51,\ S_{xx}=72.42,\ S_{yy}=2825.24,\ S_{xy}=-397.56\]

<!-- (*Enter your answer by rounding to two decimal places*) -->

<!-- $\hat{\alpha}$ = `r fitb(109.93)`  -->

<!-- $\hat{\beta}$ = `r fitb(c(-5.489644,-5.48964,-5.4896,-5.49,-5.5))` -->

`r hide("Hint")`
\[\hat{\beta_0} = \bar{y} - \hat{\beta_1}\bar{x},\ 
  \hat{\beta_1}  = \frac{S_{xy}}{S_{xx}}\]
`r unhide()`
```{r echo=FALSE, results='hide'}
beta1_hat <- S_xy/S_xx#;beta1_hat
beta0_hat <- y_mean - beta1_hat*x_mean#; beta0_hat
```

(b) Write down the equation of the fitted model. Based on it, comment on the least squares estimate of $\beta_1$ and predict the value of female life expectancy when the number of people per physician equals 4000. 

`r hide("Solution")`
The regression equation is
\[\text{FLE} = 109.95 - 5.49 \cdot \log(\text{PPP})\]

$\hat{\beta_1}$ can be interpreted as: if $\log(\text{PPP})$ increases by 1 unit, the expected female life expectancy decreases by 5.49. 

When predicting the value of the response for a new observation, we need to back transform the variable. For example, if the number of people per physician is 4000, the best prediction of female life expectancy is $109.95 - 5.49 \cdot \log(4000) \approx 64.42$.

`r unhide()`

(c) Use the summary statistics in (a) to complete the analysis of variance table below, i.e. finding the degrees of freedom, the sum of squares, the mean squared error, and the $F$-statistic. Check your answer with the `anova` output. 

Component    DF       Sum Sq (SS)    Mean Sq (MS)   F
-----------  -------  -------------  -------------  --------
Model              
Residual       
Total         

(d) What hypotheses are being examined by the $F$-statistic in the ANOVA table? By looking at its $p$-value, what can you conclude about the fitted model?

`r hide("Solution")`
The null and alternative hypotheses are:
H$_0$: $\beta_1=0$ versus H$_1$: $\beta_1 \neq 0$.
Note that $\beta_0$ is not included in the hypothesis.

Since the $p$-value (from `Pr(>F)`) is less than 0.05, we reject the null hypothesis and conclude that $\log$(PPP) is useful in predicting FLE. 
`r unhide()`

(e) Compute and interpret the coefficient of determination, $R^2$. Check your answer with the term `Multiple R-squared` in the summary table.

`r hide("Hint")`
\[R^2 = 1-\frac{SSE}{SST}\]
`r unhide()`

`r hide("Solution")`
\begin{equation*}
\begin{split}
R^2 &= 1-\frac{SSE}{SST}\\
&=1-\frac{642.91}{2182.34+642.91}\\
&=0.772
\end{split}
\end{equation*}
The $R^2$ tells us the model provides a relatively adequate fit to the data with 77% of variability in FLE can be explained by this model.
`r unhide()`


<!--chapter:end:Exercise2.Rmd-->

