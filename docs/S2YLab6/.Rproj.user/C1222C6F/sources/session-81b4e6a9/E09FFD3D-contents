---
title: "Lab 5 - Ordinary least squares"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
biblio-style: apalike
link-citations: yes
rmd_files: ["index.Rmd", "Example1.Rmd", "Exercise1.Rmd", "Exercise2.Rmd","Exercise3.Rmd"]

---
```{r include=FALSE, cache=FALSE}
library(webexercises)
```

```{r, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(webexercises)
library(tinytex)
library(dplyr)
library(knitr)
library(ggplot2)
library(tidyverse)
library(PASWR2)
library(MASS)
```


```{r include=FALSE, echo=FALSE}
knitr::write_bib(c(
  .packages(), 'bookdown', 'knitr', 'rmarkdown'
), 'packages.bib')
```

# Welcome to Lab 5!

Intended Learning Outcomes:

* Introduce and understand the matrix notation of linear models.
* Obtain model parameters using ordinary least squares (OLS) and using matrix notation in `R`. 

## Introduction to Ordinary Least Squares (OLS)
The primary tool used to model associations among variables is regression (linear or otherwise).
Regression analysis is used to model the relationship between a single variable $Y$, called
the **response** or **dependent variable**, and one or more **explanatory variables**, also called
**predictor(s)**, **covariates**, or **independent variable(s)**, $x_1, x_2,...,x_p$.

As a first step, the response variable will be continuous, but the predictor variables can be either continuous, discrete, or
categorical. You can predict non-numeric (continuous) variables using regression models, but they will be called **generalised** regression models. Here, we stick to the classical regression models. Parameter estimation for linear models can be carried out via OLS or maximum likelihood estimation. In this lab, we will look at OLS.  

The word “regression” is due to Sir Francis Galton, who in 1885 demonstrated that
offspring do not tend toward the size of the parents; rather, offspring size tends toward the
mean of the population. That is, there is a “regression toward mediocrity.”

What else is Francis famous for? **"Whenever you can, count"**, **"Nature vs nurture"** and **"London is the epicenter of female beauty, Aberdeen is the opposite..."**. Also, eugenics. Fun guy. 

<center>
![](Images/sir_francis.png){width=300px}
<center>

## The simplest regression model: simple linear regression (one covariate)##

The simple linear regression model is defined as 

\begin{equation}
Y_i = \beta_0 + \beta_1x_i + \epsilon_i,
(\#eq:SLR)
\end{equation}

where 

* $Y_i$ is the value of the response variable for the $i$th observation.
* $\beta_0$ and $\beta_1$ are regression parameters.
  + $\beta_0$ is known as the intercept.
  + $\beta_1$ is the regression coefficient of $x$ (the effect of $x$ on $Y$).
* $\epsilon_i$ is a random error term that is assumed to have a $N(0,\sigma^2)$ distribution. 

The "modelling" bit refers to estimating model parameters, $\beta_0$ and $\beta_1$ for the $n$ measurements. It looks like

$$
Y_1 = \beta_0 + \beta_1x_1 + \epsilon_1\\
Y_2 = \beta_0 + \beta_1x_2 + \epsilon_2\\
\vdots\\
Y_n = \beta_0 + \beta_1x_n + \epsilon_n.\\
$$
The model can be written in matrix notation as 
$$
\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \mathbf{\epsilon},
$$
where

$$\mathbf{Y} = \left[\begin{matrix} Y_1\\Y2\\ \vdots \\Y_n \end{matrix}\right] \text{ as a } (n \times 1) \text{ matrix, } \mathbf{X} = \left[\begin{matrix} 1 & x_1\\1 & x2 \\ \vdots&\vdots \\ 1 & x_n \end{matrix}\right] \text{ as } (n \times 2) \text{, } \boldsymbol{\beta} =\left[\begin{matrix} \beta_0 \\ \beta_1 \end{matrix} \right] \text{ as } (2 \times 1) \text{.} $$
Note that $\epsilon \sim N(\mathbf{0}, \sigma^2\mathbf{I})$ where $\sigma^2 \mathbf{I}$ is the variance-covariance matrix of the vector of errors. Given that $\mathbf{I}$ is the identity matrix, $\sigma^2$ is only along the diagonals, indicating that $\epsilon_i$ are independent from each other. 

## The next-simplest regression model: multiple linear regression (multiple predictors)##

Multiple linear regression is an extension of simple linear regression, where we have more than one coefficient $x$. $Y$ is still the dependent variable, $\beta_0$ the intercept, but $\beta_1$ is generalised to $\beta_j$ which is now the coefficeint of $x_{ij}$. The model is written as 

$$
Y_i = \beta_0 + \beta_1x_{i1} + \beta_2x_{i2} + \cdots + \beta_{p-1}x_{i(p-1)} + \epsilon_i, \text{ for } i = 1, 2,...,n.
$$
The model can be written in matrix notation as 
$$
\mathbf{Y} = \mathbf{X} \boldsymbol{\beta} + \mathbf{\epsilon},
$$

where

$$\mathbf{Y} = \left[\begin{matrix} Y_1\\Y2\\ \vdots \\Y_n \end{matrix}\right] \text{ as a } (n \times 1) \text{ matrix, } \mathbf{X} = \left[\begin{matrix} 1 & x_{11} & \cdots& x_{1(p-1)}\\1 & x_{21} & \cdots& x_{2(p-1)} \\ \vdots&\vdots&&\vdots \\ 1 & x_{n1} & \cdots& x_{n(p-1)} \end{matrix}\right] \text{ as } (n \times p) \text{, } \boldsymbol{\beta} =\left[\begin{matrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_n \end{matrix} \right] \text{ as } (p \times 1), \text{ and } \mathbf{\epsilon} = \left[\begin{matrix} \epsilon_0 \\ \epsilon_1 \\ \vdots \\ \epsilon_n \end{matrix} \right] \text{ as } (n \times 1) \text{.} $$

Note some things

* $\mathbf{X}$ is a matrix of known constants, containing the values of particular predictors. 
* $\mathbf{Y}$ and $\mathbf{\epsilon}$ are random vectors whose elements are random variables. 
* $\boldsymbol{\beta}$ is a vector of unknown coefficients (what we have to estimate!). 

## Ordinary least squares ##

The ordinary least squares method of estimating parameters minimises the sum of the squared deviations of the $Y_i$s from their expected values such that

$$\begin{split}
\epsilon_i &= Y_i - E(Y_i)\\
&= Y_i - (\beta_0 + \beta_1 x_i)\quad \text{in the case of simple linear regression}. 
\end{split}$$

The estimates $\hat{\beta}_0$ of $\beta_0$ and $\hat{\beta_1}$ of $\beta_1$ seek to minimise the quantity $\mathcal{Q}$, which represents the sum of the squared residuals (differences) as 

$$
\mathcal{Q} = \sum_{i=1}^{n}\epsilon_i^2 = \sum_{i=1}^n\left((Y_i - (\beta_0 + \beta_{1}x_i)\right)^2,
$$
which, in matrix notation, is the same as saying
$$
\mathcal{Q} = \epsilon'\epsilon = (\mathbf{Y} - \mathbf{X}\boldsymbol{\beta})'(\mathbf{Y} - \mathbf{X}\boldsymbol{\beta}). 
$$
If we want parameter estimates that minimise $\mathcal{Q}$, we take the derivative with respect to our parameters

$$
\begin{aligned}
\frac{\delta \mathcal{Q}}{\delta \beta_0} & =2 \sum_{i=1}^n\left(Y_i-\beta_0-\beta_1 x_i\right)(-1) \\
& =-2 \sum_{i=1}^n\left(Y_i-\beta_0-\beta_1 x_i\right) . \\
\frac{\delta \mathcal{Q}}{\delta \beta_1} & =2 \sum_{i=1}^n\left(Y_i-\beta_0-\beta_1 x_i\right)\left(-x_i\right) \\
& =-2 \sum_{i=1}^n\left(Y_i-\beta_0-\beta_1 x_i\right)\left(x_i\right), 
\end{aligned}
$$
 set it to zero, and solve for $\hat{\beta}_j$. The solutions for $\hat{\beta}_0$ and $\hat{\beta}_1$ are
 
 $$
 \begin{aligned}
 \hat{\beta}_0 & = \overline{Y} - \hat{\beta}_1\overline{x} \\
 \hat{\beta}_1 & = \frac{\sum_{i=1}^n(x_i - \overline{x})(Y_i - \overline{Y})}{\sum_{i=1}^{n}(x_i - \overline{x})^2} = \frac{S_{XY}}{S_{XX}}.
 \end{aligned}
 $$
 
 
In matrix notation, we can generalise the estimation of $\boldsymbol{\beta}$ as 

\begin{equation}
\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y}. 
(\#eq:LSE)
\end{equation}


## Exam-style questions
Below are example questions from past exam papers. As you go through the lab, think of how each part can be used to address the exam-style questions. 
 
1. For the Crimes example discussed in the lecture, consider the following model:

|             **Data**: $(y_i, x_i) \quad i=1,\dots,50$

|             **Model**: $\mathbb{E}(Y_i) = \beta_0+\beta_1 x_i, \quad \text{Var}(Y_i) = \sigma^2$
\
where $x_i$ denotes the \% high school dropouts in state $i$ and $y_i$ denotes the crime rate in state $i$.

|      a. Using the sum of squares function, $\mathcal{Q}(\beta_0, \beta_1) = \sum_{i=1}^{50}(Y_i-\beta_0-\beta_1 x_i)^2$, derive the least squares estimators for $\beta_0$ and $\beta_1$.
|      b. Use the summary statistics below to calculate the least squares estimates for $\beta_0$ and $\beta_1$.

 \[ \sum_{i=1}^{50}Y_i =254258, \quad\, \sum_{i=1}^{50}x_i =512.6, \quad\,   \sum_{i=1}^{50}x_i^2 = 5538.8,\quad\,  \sum_{i=1}^{50}y_i^2 = 1365790732,\quad\, \sum_{i=1}^{50} x_iy_i= 2686567.5\]


<br> 

2.  Consider the following model:

|             **Data**: $(y_i, x_i) \quad i=1,\dots,n$

|             **Model**: $\mathbb{E}(Y_i) = \beta_0+\beta_1 x_i+\beta_2 x_i^2,\quad \text{Var}(Y_i) = \sigma^2$

\
This model has been proposed for the potato example used in the lectures.\

|      a. Write this model in matrix notation, $\mathbb{E}(\mathbf{Y}$) = $\mathbf{X}\boldsymbol{\beta}$,  clearly identifying the elements of $\mathbf{Y}$, $\mathbf{X}$ and $\boldsymbol{\beta}$.

|      b. Identify formulae (in terms of the $Y_i$'s and $x_i$'s) for the elements of $\mathbf{X}'\mathbf{X}$ and $\mathbf{X}'\mathbf{y}$.

<!--chapter:end:index.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```

# Example 1: Crime

We briefly looked at this data in the lectures. Here 50 US states were investigated in terms of their crime rates (per 100,000 people), which includes crimes such as murder, assault, and car theft. Some demographic information about each state was also recorded, such as the number of police and prisoners per 100,000 people, the percentage of population living in poverty, and the percentage of high school dropouts (i.e. 16-19 year olds who were not in school and did not finish the 12$^\text{th}$ grade). The question of interest is whether we can predict US crime rates from the high school dropout rates and other predictors? 

The data are available from the csv file `crime.csv` and contain six columns, described as follows:

|C1     |C1.T        | US state
|:-----|:------|:-----------|
|**C2** |**Crime**   | **Crime rate per 100,000** 
|**C3** |**Police**  | **Number of police per 100,000**
|**C4** |**Prison**  | **Number of prisoners per 100,000**
|**C5** |**Poverty** | **Percentage of population living in poverty**
|**C6** |**Dropout** | **Percentage of high school dropouts**

<br>

## Simple linear regression  

**TASK 1**

(a) Use `plot` or `pairs` to visualise the data and determine which predictors may be useful in predicting `Crime`.

(b) Build a simple linear regression model with `Dropout` as the predictor and interpret estimated coefficients.


According to the model, when Dropout is equal to 0, the crime rate would be roughly `r fitb(c(2196.5435,2196.544,2196.54,2196.5,2197))`.
For every 1% increase in the % of high school dropouts, the expected crime rate (per 100,000) would `r mcq(c(answer="increase", "decrease"))` by `r fitb(c(281.7613,281.761,281.76,281.8,282))`.

`r hide("Hint")`
Use `lm` to build a linear regression model and `summary()` to find the coefficients. 
`r unhide()`

(c) From the lectures we know that the least squares estimates for the model parameters are

$$\hat{\boldsymbol \beta} = \begin{bmatrix} \hat{\beta_0} \\[0.5em] \hat{\beta_1} \end{bmatrix} = \begin{bmatrix} \overline{Y} - \hat{\beta_1} \overline{x} \\[0.5em] \frac{S_{xy}}{S_{xx}} \end{bmatrix}$$
Use this formula to compute the least squares estimates for the model parameters, $\overline{\beta_0}$ and $\overline{\beta_1}$.

<center>
$\hat{\beta}_0$ = `r fitb(c(2196.5435,2196.544,2196.54,2196.5,2197))`, 
$\hat{\beta}_1$ = `r fitb(c(281.7613,281.761,281.76,281.8,282))`
</center>

`r hide("Hint")`

```{r, eval=TRUE, echo=TRUE}
x<-crime$Dropout
y<-crime$Crime
x_mean <- mean(x)
y_mean <- mean(y)
S_xx <- sum((x-x_mean)^2)  
S_xy <- sum((x-x_mean)*(y-y_mean))
```

`r unhide()`

### Least squares estimates of model parameters in matrix notation

To use the formula for least squares estimation in matrix notation given by \@ref(eq:LSE), we first need to find the design matrix, $\mathbf{X}$. This can be done using the following `R` command:
```{r}
X <- model.matrix(~Dropout, data = crime)
```

This gives us the design matrix, $\mathbf{X}$, for the simple linear regression model in \@ref(eq:SLR), where we have $n = 50$ rows corresponding to each of the 50 US states, and $p = 2$ columns corresponding to the model parameters $\beta_0$ and $\beta_1$. More generally this is written as
$$\mathbf{X} =\begin{bmatrix} 1 & x_1 \\1 & x_2 \\ \vdots & \vdots \\ 1 & x_n\end{bmatrix} $$
The first column of $\mathbf{X}$ contains 1's as that is the column that multiplies the first component of the parameter vector $\boldsymbol\beta = \begin{bmatrix} \beta_0 & \beta_1 \end{bmatrix}'$, and as can be seen from the model, the intercept term, $\beta_0$, is constant across all $i$ observations. The slope parameter, $\beta_1$, is also constant, however, the differences in crime rates between states comes from changes in the percentage of high school dropouts, given by $x_i$, which multiplies $\beta_1$. The random errors, $\epsilon_i$, also differ per state.

Next we need the following commands to calculate each component in the matrix formula of least squares estimation:
```{r eval=FALSE, echo=TRUE}
t       # gets the transpose of a vector or matrix
%*%     # multiplies matrices together
solve   # computes the inverse of a matrix
```

Let's compute $\mathbf{X}'\mathbf{X}$ using the following `R` code:
```{r}
XtX <- t(X) %*% X
```

This gives

$$\mathbf{X}'\mathbf{X} = \begin{bmatrix} 50.0 & 512.6 \\ 512.6 & 5538.8 \end{bmatrix}$$

Recall, if 

$$\mathbf{A} = \begin{bmatrix} a & b \\ c & d \end{bmatrix} \quad\quad \text{then} \quad\quad \mathbf{A}^{-1} = \frac{1}{\text{det}\left(\mathbf{A}\right)}\begin{bmatrix} d & -b \\ -c & a \end{bmatrix} = \frac{1}{ad - bc}\begin{bmatrix} d & -b \\ -c & a \end{bmatrix}.$$

Given $\mathbf{X}'\mathbf{X}$ above, compute its inverse $\left(\mathbf{X}'\mathbf{X}\right)^{-1}$:
<br><br>

$$(\mathbf{X}'\mathbf{X})^{-1} = \hspace{9em}$$
<br>

To find $\hat{\boldsymbol\beta}$ we also need to find $\mathbf{X}'\mathbf{Y}$. Using the above commands, or by hand, compute $\mathbf{X}'\mathbf{Y}$:
<br><br>

$$\mathbf{X}'\mathbf{Y} = \begin{bmatrix} & & & & \\\\ \\  \end{bmatrix}$$
<br>

Multiplying $\left(\mathbf{X}'\mathbf{X}\right)^{-1}$ and $\mathbf{X}'\mathbf{Y}$ then gives us the least squares estimates of the model parameters. By hand, compute the parameter estimates, $\hat{\boldsymbol\beta}$, such that

<br>

$$\hat{\boldsymbol{\beta}} = (\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{Y} = $$

<br>

(d) Use the `R` commands given above to compute $\left(\mathbf{X}'\mathbf{X}\right)^{-1}$ and $\mathbf{X}'\mathbf{Y}$, and hence $\hat{\boldsymbol\beta}$, and compare the output with your handwritten results.

`r hide("Solution")`

```{r, echo=TRUE, eval=TRUE}
XtX_inv <- solve(XtX)
Y <- crime$Crime
XtY <- t(X) %*% Y
beta.hat <- solve(XtX) %*% XtY
```

`r unhide()`

<br>

We can also obtain the vector of random errors, $\boldsymbol \epsilon$, by taking the difference between the observed values, $\mathbf{Y}$, and the fitted values, $\hat{\mathbf{Y}} = \mathbf{X}\hat{\boldsymbol\beta}$, using the command:

```{r eval=FALSE}
residuals <- Y - X %*% param.ests
```


## Multiple linear regression

(e) Use one or more predictors alongside `Dropout` to build a multiple linear regression model for explaining `Crime`.

`r hide("Hint")`

Use the graph found using the `pairs()` command in (a) to select predictors that appear suitable for describing Crime.

Recall that a multiple linear regression model can be constructed using 

<center> `model <- lm(Crime ~ Dropout + Predictor1 + Predictor2 + ..., data = crime)` </center>

`r unhide()`

(f) Calculate the least squares estimates of parameters in the new multiple linear regression model using the formula in matrix notation.

`r hide("Solution")`

The same steps can be followed as in (d), but the design matrix `X` has to be updated accordingly. Say we want to add `Police` and `Prison` variables to our model. We would then use the following code.

```{r, echo=TRUE, eval=TRUE}
X <- model.matrix(~ Dropout + Police + Prison, data = crime)
Y <- crime$Crime

XtX <- t(X) %*% X
XtY <- t(X) %*% Y

beta.hat <- solve(XtX) %*% XtY
beta.hat
```
`r unhide()`


<!--chapter:end:Example1.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Exercise 1: SLR with no intercept

#### Introducing the data: Hubble's Constant {-}

**Source:** Hubble, E. (1929) A Relationship Between Distance and Radial Velocity among Extra-Galactic Nebulae, Proceedings of the National Academy of Science, 168.

**Context:** In 1929 Edwin Hubble investigated the relationship between distance and radial velocity of extragalactic nebulae (celestial objects). It was hoped that some knowledge of this relationship might give clues as to the way the universe was formed and what may happen later. His findings revolutionised astronomy and are the source of much research today on the ‘Big Bang’. Given here is the data that Hubble used for 24 nebulae. It is of interest to determine the effect of distance on velocity.

**Data:** `hubble.csv`

**Columns:** 

|                       **C1:** `Distance` - (in Megaparsecs) from Earth

|                       **C2:** `Velocity` - recession (in km/sec)

Read in the data using:
```{r}
hubble <- read.csv("hubble.csv")
```

<br>

## Exploratory analysis

**TASK 2**

(a) Produce a scatterplot of the data with `Velocity` on the y-axis.

(b) Describe the shape of the relationship.

The relationship appears to be `r mcq(c("non-linear", answer="linear"))` and it `r mcq(c(answer="seems plausible", "does not seem plausible"))` that the line passes through the origin.

<br>

## Fitting a simple linear regression model

(c) Fit a simple linear regression model with the response variable `Velocity` and the explanatory variable
`Distance` and write down the equation of the fitted line.

The equation of the fitted line is given by:

<center> `Velocity` = `r fitb(c("-40.78365","-40.7837","-40.784","-40.78","-40.8"))` **+** `r fitb(c("454.15844","454.1584","454.158","454.16","454.2"))` `Distance` </center>

This is the line of best fit, describing the effect of distance on velocity.

(d) Produce a plot of the data including the fitted line.

`r hide("Hint")`
A fitted line can be added to a plot using the command `abline`.
`r unhide()`

#### Now with OLS {-}
(e) Fit a simple linear regression model with the response variable `Velocity` and the explanatory variable `Distance` and provide parameter estimates using the matrix formula of least squares estimation. 

`r hide("Hint")`
$\hat{\boldsymbol{\beta}}=(\mathbf{X}'\mathbf{X})^{-1} \mathbf{X}'\mathbf{Y}$
`r unhide()`

`r hide("Hint")`
`X <- model.matrix(~ Distance, data = hubble)`
`r unhide()`


## Fitting a linear model with no intercept

From the scatterplot in (a), it is plausible from the data that the regression line should be forced to go through the origin. A model with no intercept could be fitted using: 

```{r, eval=FALSE}
lm(Velocity ~ -1 + Distance, data = hubble)
```

The inclusion of `-1` is what removes the intercept term from the model fit.

Note down the equation of the fitted line that is given:

<center> `Velocity` = `r fitb(c("423.9373","423.937","423.94","423.9"))` `Distance` </center>

If we save the model with and without intercept using
```{r}
model.Int <- lm(Velocity ~ Distance, data = hubble)
model.NoInt <- lm(Velocity ~ -1 + Distance, data = hubble)
```
a plot of the data can be re-produced as before with the fitted lines from both linear models added using the \texttt{abline} command:

```{r hubble-plot, fig.align='center', out.width="70%", fig.cap="Scatterplot of `Velocity` versus `Distance` with two fitted models - simple linear regression model with and without the intercept."}
plot(Velocity ~ Distance, data = hubble)
abline(model.Int, col = "red")
abline(model.NoInt, col = "blue", lty = 2)
legend("topleft", legend = c("with intercept", "w/o intercept"), lty = c(1,2), col = c("red","blue"), bty = "n")
```

The `lty` argument in `abline` changes the type/style of the line plotted, which is handy when printing in black and white. 

A `legend` is also added to distinguish between the lines plotted.


#### Now with OLS {-}
(f) Fit a simple linear regression model with the response variable `Velocity` and the explanatory variable `Distance` but with no intercept. Again use the matrix formula of least squares estimation. 

`r hide("Hint")`
`X <- model.matrix(~ -1 + Distance, data = hubble)`
`r unhide()`


**DISCUSSION**: Comparing the two fitted lines, is the model that passes through the origin plausible?



<!--chapter:end:Exercise1.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Exercise 2: Quadratic regression

#### Introducing the data: Publishing history {-}

**Source:** Tweedie, F. J., Bank, D. A. and McIntyre, B. (1998) Modelling Publishing History, 1475–1640:
Change points in the STC.

**Context:** The Short Title Catalogue (STC) is a list of all the books that were published in Scotland, England and Ireland between 1475 and 1640. We are interested in finding out if there are any changes in the number of books published between 1500 and 1640.

**Data:** `books.csv`

**Columns:**

|               **C1:** `Year` - 1500 – 1640

|               **C2:** `Number of Books` - Total number of books published

Read in the data using: 

```{r}
books <- read.csv("books.csv")
```

<br>

## Exploratory analysis 

**TASK 3**

(a) Produce a scatterplot of the data with **Number of books** on the y-axis and **Year** on the x-axis.

(b) Describe the shape of the relationship. Does it seem linear?

## Fitting a quadratic regression model

Use the commands below to fit a quadratic regression and output the model summary: 
```{r, eval=FALSE}
model <- lm(Number.of.Books ~ Year + I(Year^2), data = books)
summary(model)
```

Note: `year^2` needs to be 'protected' by `I`, the identity function.

(c) Note down the equation of the fitted quadratic line and plot the fitted line.

<center> **Total number of books =**  `r fitb(c("59.9779612","59.977961","59.97796","59.9780","59.978","59.98","60.0","60"))` **+** `r fitb(c("-0.491710","-0.49171","-0.4917","-0.492","-0.49","-0.5"))` **Year +** `r fitb(c("0.033940","0.03394","0.0339","0.034","0.03"))` **Year^2**</center>

`r hide("Hint")`
The `R` command `lines()` can be used to plot the points obtained by using `fitted()` on the model.
`r unhide()`


<!--chapter:end:Exercise2.Rmd-->

```{r include=FALSE, cache=FALSE}
library(webexercises)
```
# Exercise 3: The taste of cheese

**Context:** This model might be considered for an experiment involving the chemical constituents of cheese and its taste. The dataset contains the concentrations of acetic acid, hydrogen sulphide ($H_2S$) and lactic acid, as well as a subjective taste score. It is of interest to investigate the effects of the different acids on the taste score.

**Model:** $\mathbb{E}(Y_i) = \alpha + \beta x_i$, $Var(Y_i) = \sigma^2$

**Data:** `cheese.csv`

**Columns:**

|           **C1:** `Case` - Number of sample

|           **C2:** `Taste` - Taste score

|           **C3:** `Acetic.Acid` - Acetic acid concentration

|           **C4:** `H2S` - $H_2S$ concentration

|           **C5:** `Lactic.Acid` - Lactic acid concentration


Read in the data using

```{r, eval=FALSE}
cheese <- read.csv("cheese.csv")
```

## Exploratory analysis

**TASK 4**

(a) Produce scatterplots of `Taste` ($y$-axis) against `Lactic.Acid` ($x$-axis), and `Taste` ($y$-axis) against `H2S` ($x$-axis).

(b) Now plot `Taste` against log(`H2S`), and against log(`Lactic.Acid`). The command in `R` to perform a natural logarithmic transform is, for example, `log(H2S)`.

(c) Which of the 4 variables (`H2S`, log(`H2S`), `Lactic.Acid`, log(`Lactic.Acid`)) seems best for describing a linear relationship with `Taste`?

`r mcq(c("H2S", answer="log(H2S)", "Lactic.Acid", "log(Lactic.Acid)"))`

`r hide("Hint")`
Which of the four plots shows a straight/close-to-straight line similar to the line $y=x$?
`r unhide()`

## Fitting a model with a transformation

(d) Fit a linear regression (using the `lm` command) with `Taste` as the response variable and the explanatory variable you selected from part (c). Make a note of the fitted model. 

(e) Produce a plot with a line from your fitted model in (d) using the `abline` command.

(f) How well do you think the model and the data agree?

```{r echo = FALSE}
opts_p <- c(
  answer = "There is a weak positive relationship", 
  "There is a strong positive relationship", 
  "There is a moderate positive relationship"
)
```

`r longmcq(opts_p)`

<!--chapter:end:Exercise3.Rmd-->

