[["index.html", "Lab 6 - Hypothesis testing and interval estimation 1 Welcome to Lab 6 1.1 The summary() function for regression models 1.2 Hypothesis testing for linear relationships 1.3 Confidence intervals for the model parameters", " Lab 6 - Hypothesis testing and interval estimation 1 Welcome to Lab 6 Intended Learning Outcomes: use R to conduct hypothesis tests for parameters in a linear model; use various summary statistics and R output to compute confidence and prediction intervals; use the built-in R function to compute confidence and prediction intervals; interpret hypothesis tests, confidence intervals and prediction intervals. 1.1 The summary() function for regression models Before discussing hypothesis testing and confidence intervals for regression models, let's first revise the output of summary() function. Recall the model we created in Lab 4, where we try to predict the hydrostatic fat measurement of a wrestler from abomnimal fat and tricep fat measurements. library(PASWR) HSWRESTLER &lt;- HSwrestler Model1 &lt;- lm(HWFAT ~ ABS + TRICEPS, data = HSWRESTLER) We apply the summary function, which gives the following output: summary(Model1) ## ## Call: ## lm(formula = HWFAT ~ ABS + TRICEPS, data = HSWRESTLER) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.5558 -2.2550 -0.5245 2.3365 9.4957 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.05904 0.65219 3.157 0.0023 ** ## ABS 0.33708 0.06415 5.255 1.34e-06 *** ## TRICEPS 0.50430 0.09920 5.084 2.64e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.061 on 75 degrees of freedom ## Multiple R-squared: 0.8832, Adjusted R-squared: 0.8801 ## F-statistic: 283.6 on 2 and 75 DF, p-value: &lt; 2.2e-16 The above R output displays a few elements, which were briefly explained below. Call: This shows the formula that was used in the regression model. Residuals: This lists the five-number summary of the residuals from the regression model. Coefficients: This shows a summary of estimated coefficients of the regression model. Within this section the column headers are: Estimate: The estimated parameter. These can be used to write down the fitted regression model. Std. Error: This is the estimated standard error of the parameter estimate. t value: This is the \\(t\\)-statistic for the parameter, calculated as Estimate / Std. Error. Pr(&gt;|t|): This is the \\(p\\)-value that corresponds to the \\(t\\)-statistic, i.e. \\(2\\cdot \\mathbb{P}(X&gt;|t|)\\) for \\(X \\sim t(n-p)\\), where \\(t\\) is the t value computed above, \\(n\\) is the sample size, and \\(p\\) is the number of parameters. Significance codes: These codes in asterisks are appended to the \\(p\\)-values in regression analysis results. They provide a quick indication of the level of significance of the predictors in the model. The most commonly used significance levels and their corresponding codes are: significance code p-value *** &lt; 0.001 ** 0.001 - 0.01 * 0.01 - 0.05 . 0.05 - 0.1 â‰¥ 0.1 Residual standard error: This is the square root of mean squared error (MSE), where \\(MSE=\\frac{\\sum_{i=1}^n \\epsilon_i^2}{n-p}\\) is calculated as the sum of squared residuals divided by the degrees of freedom in the model. The remaining terms, Multiple R-squared, Adjusted R-squared and F-statistic, will be explained in subsequent labs. Here we can look at the estimates of the model coefficients \\(\\hat{\\beta}_k\\), which are both positive, indicating both Abs and Tri have a positive relationship with Hydrostatic fat. We can also state that both Abdominal Fat and Tricep Fat measurements are significant in predicting the Hydrostatic fat measurement to a significance level of \\(\\approx 0\\) by looking at the \\(p\\)-values associated with the \\(t\\)-tests. 1.2 Hypothesis testing for linear relationships Recall a linear regression model can be written in the matrix form as \\[\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol \\epsilon.\\] Assuming \\(\\boldsymbol{\\epsilon} \\sim N (\\mathbf{0},\\sigma^2 \\bf{I})\\), we have shown that \\(\\mathbf{Y} \\sim N (\\mathbf{X} \\boldsymbol{\\beta},\\sigma^2 \\bf{I})\\) and \\(\\hat{\\boldsymbol{\\beta}} \\sim N (\\boldsymbol{\\beta},\\sigma^2 \\mathbf{(X&#39;X)}^{-1})\\). 1.2.1 Finding the estimate of error variance We know that \\(\\hat{\\sigma}^2 = s^2=MSE=\\frac{SSE}{n-p}=\\frac{\\sum^2_{i=1} \\hat{\\epsilon}^2_i}{n-p}\\) and \\[\\mathbf{s}^2_{{\\hat{\\boldsymbol{\\beta}}}} = \\hat{\\sigma}^2 \\mathbf{(X&#39;X)}^{-1} = \\begin{bmatrix} s^2_{{\\hat{\\beta}_0}} &amp; s_{\\hat{\\beta}_0,\\hat{\\beta}_1} &amp;...&amp; s_{\\hat{\\beta}_0,\\hat{\\beta}_{p-1}}\\\\ s_{\\hat{\\beta}_1,\\hat{\\beta}_0} &amp; s^2_{\\hat{\\beta}_1} &amp;...&amp; s_{\\hat{\\beta}_1,\\hat{\\beta}_{p-1}}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ s_{\\hat{\\beta}_{p-1},\\hat{\\beta}_0} &amp; s_{\\hat{\\beta}_{p-1},\\hat{\\beta}_1} &amp;...&amp; s^2_{\\hat{\\beta}_{p-1}} \\end{bmatrix} \\] In R the matrices of \\(\\mathbf{(X&#39;X)}^{-1}\\) and \\(\\mathbf{s}^2_{{\\hat{\\boldsymbol{\\beta}}}}\\) can be found using summary(lm.object)$cov.unscaled and vcov(lm.object) respectively, where lm.object is the linear model object built by using lm(). 1.2.2 Steps of the hypothesis test Step 1: Hypotheses If we want to test a model coefficient against a hypothesised value. the null and alternative hypotheses are: \\[H_0: \\beta_k = \\beta_{k_0} \\quad \\text{vs} \\quad H_1: \\beta_k \\neq \\beta_{k_0}\\] Step 2: Test Statistic The test statistic is \\(\\hat{\\beta_k}\\). Assuming the error terms are distributed normally, \\[\\hat{\\beta_k} \\sim N(\\beta_k, \\sigma^2_{\\hat{\\beta_k}}).\\] The standardised test statistic under the assumption that \\(H_0\\) is true and its distribution are: \\[\\frac{\\text{unbiased estimator} - \\text{hypothesized value}}{\\text{standard error of estimator}} = \\frac{\\hat{\\beta}_k-\\beta_{k_0}}{s_{\\hat{\\beta}_k}} \\sim t_{n-p}\\] Step 3: Rejection region calculations The standardised test statistic is \\(\\frac{\\hat{\\beta_k}-\\beta_{k_0}}{s_{\\hat{\\beta}_k}} \\sim t_{n-p}\\), and so \\(t_\\text{obs} = \\frac{\\hat{\\beta_k}-\\beta_{k_0}}{s_{\\hat{\\beta}_k}}\\). This can be compared to the critical value, \\(t_{1-\\alpha,n-p}\\), which can be found from the statistical table or using R. In R the test can be run using the function lm(). For example: model &lt;- lm() summary(model)$coef This will give the observed \\(t\\)-value and the corresponding \\(p\\)-value which can then be compared to the significance level \\(\\alpha\\). Step 4: Statistical conclusion From the rejection region, reject \\(H_0\\) if \\(|t_\\text{obs}\\) is greater than the critical value. From the \\(p\\)-value, reject \\(H_0\\) if the \\(p\\)-value is less than \\(\\alpha\\). Step 5: English conclusion Depending on whether \\(H_0\\) is rejected or not, conclude whether there is sufficient or insufficient evidence evidence to suggest a linear relationship between the response variable and the predictor variable. 1.3 Confidence intervals for the model parameters We may also want to build confidence intervals (CI) for our model parameters. The CI for the parameter \\(\\beta_k\\) with confidence level \\(1-\\alpha\\) is: \\[\\left( \\hat{\\beta}_k - t_{1-\\alpha/2\\text{ };\\text{ }n-p} \\cdot s_{\\hat{\\beta}_k},\\ \\hat{\\beta}_k + t_{1-\\alpha/2\\text{ };\\text{ }n-p} \\cdot s_{\\hat{\\beta}_k}\\right),\\] where the degrees of freedom is \\(n-p\\) because \\(\\sigma^2\\) is estimated using \\(MSE=\\frac{SSE}{n-p}\\). "],["example-1---modelling-college-grades.html", "2 Example 1 - Modelling College Grades 2.1 Hypothsis Testing for Regression Models 2.2 Confidence intervals for model parameters", " 2 Example 1 - Modelling College Grades In Lab 4, we looked at the Grades dataset from the PASWR package, which records the first-semester college GPA and SAT scores for 200 freshmen. The question of interest is to check whether there is a linear relationship between GPA and SAT scores. Last time, we viewed the data in scatterplots, built a linear model, and calculated correlation to learn more about the relationship between SAT and GPA scores. Today, we will formally assess if the SAT is a useful variable in predicting GPA. To open the dataset, type: library(PASWR) GRADES &lt;- Grades TASK: Build the simple linear regression model for how GPA scores change with SAT scores and print the model summary. Hint Remember the function lm(y~x, data=). It will be helpful to save this output using &lt;- for future analysis. 2.1 Hypothsis Testing for Regression Models We want to test whether there is a linear relationship between GPA and SAT scores at the \\(\\alpha = 0.10\\) significance level. While this is automatically done in R, we will practice conducting the hypothesis test using the five-step procedure, during which you will become more familiar with the R output. Step 1 - Hypotheses As we are interested in the predictive power of SAT, we will perform a hypothesis test on the slope of the model. Suppose the model has the form of \\(Y_i=\\beta_0+\\beta_1 x_i +\\epsilon_i\\). \\[H_0 : \\beta_1 = 0 \\quad \\text{versus} \\quad H_1 : \\beta_1 \\neq 0\\] Learn More A regression equation with slope \\(\\beta_1 = 0\\) would indicate no influence of \\(x\\) on \\(y\\). Thus, to assess whether \\(x\\) is a useful predictor, we test if the slope is different from 0. Step 2 - Test statistic Looking at the model summary below, we find that the value of test statistic is \\(\\hat{\\beta}_1 = 0.0031\\). ## ## Call: ## lm(formula = gpa ~ sat, data = GRADES) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.04954 -0.25960 -0.00655 0.26044 1.09328 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.1920638 0.2224502 -5.359 2.32e-07 *** ## sat 0.0030943 0.0001945 15.912 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3994 on 198 degrees of freedom ## Multiple R-squared: 0.5612, Adjusted R-squared: 0.5589 ## F-statistic: 253.2 on 1 and 198 DF, p-value: &lt; 2.2e-16 Assuming the assumptions of a linear model are satisfied then: \\[\\hat{\\beta}_1 \\sim N(\\beta_1, \\sigma^2_{\\hat{\\beta}_1})\\] The standardised test statistic under the assumption that \\(H_0\\) is true and its distribution are: \\[\\frac{\\hat{\\beta}_1-\\beta_1}{s_{\\hat{\\beta}_1}} \\sim t_{200-2}\\] Step 3 - Rejection region calculations Finding the rejection region Because the standardised test statistic is distributed \\(t_{198}\\) and \\(H_1\\) is a two-sided hypothesis, the rejection region is the \\(|t_\\text{obs}| &gt; t_{1-\\alpha;n-p} = t_{0.95;198}\\). The value of standardised test statistic is given in the R output as 15.912 (t value associated with the predictor sat). Let's verify this is correct: \\[t_\\text{obs}=\\frac{\\hat{\\beta}_1-\\beta_1}{s_{\\hat{\\beta}_1}} = \\frac{0.0031-0}{0.0001945}=15.9117\\] \\(s_{\\hat{\\beta}_1}\\) can be found by either looking at Std. Error associated with sat, or taking the square root of the (2,2) entry of the variance-covariance matrix of \\(\\mathbf{s}^2_{\\boldsymbol \\beta}\\): s2_beta &lt;- vcov(Model) #the variance-covariance matrix s2_beta ## (Intercept) sat ## (Intercept) 4.948408e-02 -4.290866e-05 ## sat -4.290866e-05 3.781665e-08 sqrt(s2_beta[2,2]) ## [1] 0.000194465 The critical value (the \\(t\\)-value that corresponds to the significance level of 0.01), \\(t_{0.95;198}\\), can be found using: qt(0.95,198) ## [1] 1.652586 Step 4 - Statistical Conclusion QUESTION: Based on the value of standardised test statistic and critical value, do we reject the null hypothesis? Solution We reject \\(H_0\\) because the value of standardised test statistic is greater than the critical value and hence is in the rejection region, i.e. \\(t_{obs} = 15.9117 \\geq 1.6526\\). Alternatively, we can compute the \\(p\\)-value associated with this \\(t\\)-test, which is equal to \\(2 \\times \\mathbb{P}(t_{0.95;198} \\geq 15.9117)\\). For the area to the right of the observed value of standardised test statistic, use lower.tail=FALSE. pvalue &lt;- 2*pt(15.9117, 198, lower.tail = FALSE) pvalue ## [1] 2.923121e-37 This gives the \\(p\\)-value nearly 0. Step 5 - English conclusion QUESTION: What does the previoius statistical conclusion tell you about the relationship between GPA and SAT scores? There is insufficient evidence to suggest a linear relationship between sat and gpa. There is sufficient evidence to suggest a non-linear relationship between sat and gpa. There is sufficient evidence to suggest a linear relationship between sat and gpa. We cannot conclude if there is sufficient evidence to suggest a linear relationship between sat and gpa. 2.2 Confidence intervals for model parameters We will practice constructing 90% confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\). These 90% CIs will take the format of \\(\\left( \\hat{\\beta}_k - t_{0.95;198} \\cdot s_{\\hat{\\beta}_k} \\text{ },\\text{ } \\hat{\\beta}_k + t_{0.95;198} \\cdot s_{\\hat{\\beta}_k}\\right)\\) The parameter estimates \\(\\hat{\\beta}_k\\) can be found in the model summary and their standard errors \\(s_{\\hat{\\beta}_k}\\) can be found from the variance-covariance matrix or from the model summary as well. b0 &lt;- coef(summary(Model))[1, 1] s.b0 &lt;- coef(summary(Model))[1, 2] #or s.bo &lt;- sqrt(vcov(Model)[1,1]) b1 &lt;- coef(summary(Model))[2, 1] s.b1 &lt;- coef(summary(Model))[2, 2] #or s.bo &lt;- sqrt(vcov(Model)[2.2]) ct &lt;- qt(1 - 0.1/2, 198) # alpha = 0.10 CI.B0 &lt;- b0 + c(-1, 1) * ct * s.b0 CI.B1 &lt;- b1 + c(-1, 1) * ct * s.b1 CI.B0 ## [1] -1.5596818 -0.8244458 CI.B1 ## [1] 0.00277290 0.00341564 R also has a built-in function confint to compute the confidence interval: confint(Model, level = 0.9) ## 5 % 95 % ## (Intercept) -1.5596818 -0.82444581 ## sat 0.0027729 0.00341564 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
