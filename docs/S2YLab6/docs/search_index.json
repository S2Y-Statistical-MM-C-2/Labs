[["index.html", "1 Welcome to S2Y Lab 6 1.1 The summary() function for regression models 1.2 Hypothesis testing for linear relationships 1.3 Confidence intervals for the model parameters 1.4 Confidence interval for the population mean response and prediction interval for a future response", " S2Y Lab 6 Hypothesis testing and interval estimation 1 Welcome to S2Y Lab 6 Intended Learning Outcomes: use R to conduct hypothesis tests for parameters in a linear model; use various summary statistics and R output to compute confidence and prediction intervals; use the built-in R function to compute confidence and prediction intervals; interpret hypothesis tests, confidence intervals and prediction intervals. 1.1 The summary() function for regression models Before discussing hypothesis testing and confidence intervals for regression models, let's first revise the output of summary() function. Recall the model we created in Lab 4, where we try to predict the hydrostatic fat measurement of a wrestler from abomnimal fat and tricep fat measurements. library(PASWR) HSWRESTLER &lt;- HSwrestler Model1 &lt;- lm(HWFAT ~ ABS + TRICEPS, data = HSWRESTLER) We apply the summary function, which gives the following output: summary(Model1) ## ## Call: ## lm(formula = HWFAT ~ ABS + TRICEPS, data = HSWRESTLER) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.5558 -2.2550 -0.5245 2.3365 9.4957 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.05904 0.65219 3.157 0.0023 ** ## ABS 0.33708 0.06415 5.255 1.34e-06 *** ## TRICEPS 0.50430 0.09920 5.084 2.64e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.061 on 75 degrees of freedom ## Multiple R-squared: 0.8832, Adjusted R-squared: 0.8801 ## F-statistic: 283.6 on 2 and 75 DF, p-value: &lt; 2.2e-16 The above R output displays a few elements, which were briefly explained below. Call: This shows the formula that was used in the regression model. Residuals: This lists the five-number summary of the residuals from the regression model. Coefficients: This shows a summary of estimated coefficients of the regression model. Within this section the column headers are: Estimate: The estimated parameter. These can be used to write down the fitted regression model. Std. Error: This is the estimated standard error of the parameter estimate. t value: This is the \\(t\\)-statistic for the parameter, calculated as Estimate / Std. Error. Pr(&gt;|t|): This is the \\(p\\)-value that corresponds to the \\(t\\)-statistic, i.e. \\(2\\cdot \\mathbb{P}(X&gt;|t|)\\) for \\(X \\sim t(n-p)\\), where \\(t\\) is the t value computed above, \\(n\\) is the sample size, and \\(p\\) is the number of parameters. Significance codes: These codes in asterisks are appended to the \\(p\\)-values in regression analysis results. They provide a quick indication of the level of significance of the predictors in the model. The most commonly used significance levels and their corresponding codes are: significance code p-value *** &lt; 0.001 ** 0.001 - 0.01 * 0.01 - 0.05 . 0.05 - 0.1 â‰¥ 0.1 Residual standard error: This is the square root of mean squared error (MSE), where \\(MSE=\\frac{\\sum_{i=1}^n \\epsilon_i^2}{n-p}\\) is calculated as the sum of squared residuals divided by the degrees of freedom in the model. The remaining terms, Multiple R-squared, Adjusted R-squared and F-statistic, will be explained in subsequent labs. Here we can look at the estimates of the model coefficients \\(\\hat{\\beta}_k\\), which are both positive, indicating both Abs and Tri have a positive relationship with Hydrostatic fat. We can also state that both Abdominal Fat and Tricep Fat measurements are significant in predicting the Hydrostatic fat measurement to a significance level of \\(\\approx 0\\) by looking at the \\(p\\)-values associated with the \\(t\\)-tests. 1.2 Hypothesis testing for linear relationships Recall a linear regression model can be written in the matrix form as \\[\\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol \\epsilon.\\] Assuming \\(\\boldsymbol{\\epsilon} \\sim N (\\mathbf{0},\\sigma^2 \\bf{I})\\), we have shown that \\(\\mathbf{Y} \\sim N (\\mathbf{X} \\boldsymbol{\\beta},\\sigma^2 \\bf{I})\\) and \\(\\hat{\\boldsymbol{\\beta}} \\sim N (\\boldsymbol{\\beta},\\sigma^2 \\mathbf{(X&#39;X)}^{-1})\\). 1.2.1 Finding the estimate of error variance We know that \\(\\hat{\\sigma}^2 = s^2=MSE=\\frac{SSE}{n-p}=\\frac{\\sum^2_{i=1} \\hat{\\epsilon}^2_i}{n-p}\\) and \\[\\mathbf{s}^2_{{\\hat{\\boldsymbol{\\beta}}}} = \\hat{\\sigma}^2 \\mathbf{(X&#39;X)}^{-1} = \\begin{bmatrix} s^2_{{\\hat{\\beta}_0}} &amp; s_{\\hat{\\beta}_0,\\hat{\\beta}_1} &amp;...&amp; s_{\\hat{\\beta}_0,\\hat{\\beta}_{p-1}}\\\\ s_{\\hat{\\beta}_1,\\hat{\\beta}_0} &amp; s^2_{\\hat{\\beta}_1} &amp;...&amp; s_{\\hat{\\beta}_1,\\hat{\\beta}_{p-1}}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ s_{\\hat{\\beta}_{p-1},\\hat{\\beta}_0} &amp; s_{\\hat{\\beta}_{p-1},\\hat{\\beta}_1} &amp;...&amp; s^2_{\\hat{\\beta}_{p-1}} \\end{bmatrix} \\] In R the matrices of \\(\\mathbf{(X&#39;X)}^{-1}\\) and \\(\\mathbf{s}^2_{{\\hat{\\boldsymbol{\\beta}}}}\\) can be found using summary(lm.object)$cov.unscaled and vcov(lm.object) respectively, where lm.object is the linear model object built by using lm(). 1.2.2 Steps of the hypothesis test Step 1: Hypotheses If we want to test a model coefficient against a hypothesised value. the null and alternative hypotheses are: \\[H_0: \\beta_k = \\beta_{k_0} \\quad \\text{vs} \\quad H_1: \\beta_k \\neq \\beta_{k_0}\\] Step 2: Test Statistic The test statistic is \\(\\hat{\\beta_k}\\). Assuming the error terms are distributed normally, \\[\\hat{\\beta_k} \\sim N(\\beta_k, \\sigma^2_{\\hat{\\beta_k}}).\\] The standardised test statistic under the assumption that \\(H_0\\) is true and its distribution are: \\[\\frac{\\text{unbiased estimator} - \\text{hypothesized value}}{\\text{standard error of estimator}} = \\frac{\\hat{\\beta}_k-\\beta_{k_0}}{s_{\\hat{\\beta}_k}} \\sim t_{n-p}\\] Step 3: Rejection region calculations The standardised test statistic is \\(\\frac{\\hat{\\beta_k}-\\beta_{k_0}}{s_{\\hat{\\beta}_k}} \\sim t_{n-p}\\), and so \\(t_\\text{obs} = \\frac{\\hat{\\beta_k}-\\beta_{k_0}}{s_{\\hat{\\beta}_k}}\\). This can be compared to the critical value, \\(t_{1-\\alpha,n-p}\\), which can be found from the statistical table or using R. In R the test can be run using the function lm(). For example: model &lt;- lm() summary(model)$coef This will give the observed \\(t\\)-value and the corresponding \\(p\\)-value which can then be compared to the significance level \\(\\alpha\\). Step 4: Statistical conclusion From the rejection region, reject \\(H_0\\) if \\(|t_\\text{obs}\\) is greater than the critical value. From the \\(p\\)-value, reject \\(H_0\\) if the \\(p\\)-value is less than \\(\\alpha\\). Step 5: English conclusion Depending on whether \\(H_0\\) is rejected or not, conclude whether there is sufficient or insufficient evidence evidence to suggest a linear relationship between the response variable and the predictor variable. 1.3 Confidence intervals for the model parameters We may also want to build confidence intervals (CI) for our model parameters. The CI for the parameter \\(\\beta_k\\) with confidence level \\(1-\\alpha\\) is: \\[\\begin{equation} \\left( \\hat{\\beta}_k - t_{1-\\alpha/2;n-p} \\cdot s_{\\hat{\\beta}_k},\\ \\hat{\\beta}_k + t_{1-\\alpha/2;n-p} \\cdot s_{\\hat{\\beta}_k}\\right), \\tag{1.1} \\end{equation}\\] where the degrees of freedom is \\(n-p\\) because \\(\\sigma^2\\) is estimated using \\(MSE=\\frac{SSE}{n-p}\\). 1.4 Confidence interval for the population mean response and prediction interval for a future response Once we have fitted the model of interest, it can be used to make predictions on unseen data by plugging in the particular values of the predictor variables: \\[\\hat{Y}_h = \\mathbf{X}_h \\hat{\\boldsymbol \\beta},\\] where \\(\\mathbf{X}_h = [1, x_{h,1}, x_{h,2}, \\ldots, x_{h,p-1}]\\) is the vector of predictor variables for the new observation. In addition to the point estimate, it is also important to compute the confidence interval for the mean response for this new \\(\\mathbf{X}_h\\) and the prediction interval on a single, new observation. The confidence interval for the mean response is given by \\[\\begin{equation} \\left( \\hat{Y}_h - t_{1-\\alpha/2;n-p} \\cdot s_{\\hat{Y}_k},\\ \\hat{Y}_h + t_{1-\\alpha/2;n-p} \\cdot s_{\\hat{Y}_k}\\right). (\\#eq:CI_mean) \\end{equation}\\] \\[\\small \\left( \\hat{Y}_h - t_{1-\\alpha/2;n-p} \\cdot \\sqrt{MSE\\cdot \\mathbf{X}_h (\\mathbf{X}_h&#39; \\mathbf{X}_h)^{-1}\\mathbf{X}_h&#39;)},\\ \\hat{Y}_h + t_{1-\\alpha/2;n-p} \\cdot \\sqrt{MSE\\cdot \\mathbf{X}_h (\\mathbf{X}_h&#39; \\mathbf{X}_h)^{-1}\\mathbf{X}_h&#39;)}\\right)\\] The prediction interval for a single new observation is given by \\[\\begin{equation} \\left( \\hat{Y}_h - t_{1-\\alpha/2;n-p} \\cdot s_{\\hat{Y}_k},\\ \\hat{Y}_h + t_{1-\\alpha/2;n-p} \\cdot s_{\\hat{Y}_k}\\right). \\tag{1.2} \\end{equation}\\] \\[\\small \\left( \\hat{Y}_h - t_{1-\\alpha/2;n-p} \\cdot \\sqrt{MSE\\cdot (1+\\mathbf{X}_h (\\mathbf{X}_h&#39; \\mathbf{X}_h)^{-1}\\mathbf{X}_h&#39;))},\\ \\hat{Y}_h + t_{1-\\alpha/2;n-p} \\cdot \\sqrt{MSE\\cdot (1+\\mathbf{X}_h (\\mathbf{X}_h&#39; \\mathbf{X}_h)^{-1}\\mathbf{X}_h&#39;))}\\right)\\] "],["example-1-modelling-college-grades.html", "2 Example 1: Modelling College Grades 2.1 Hypothsis Testing for Regression Models 2.2 Confidence intervals for model parameters", " 2 Example 1: Modelling College Grades In Lab 4, we looked at the Grades dataset from the PASWR package, which records the first-semester college GPA and SAT scores for 200 freshmen. The question of interest is to check whether there is a linear relationship between GPA and SAT scores. Last time, we viewed the data in scatterplots, built a linear model, and calculated correlation to learn more about the relationship between SAT and GPA scores. Today, we will formally assess if the SAT is a useful variable in predicting GPA. To open the dataset, type: library(PASWR) GRADES &lt;- Grades TASK: Build the simple linear regression model for how GPA scores change with SAT scores and print the model summary. Hint Remember the function lm(y~x, data=). It will be helpful to save this output using &lt;- for future analysis. 2.1 Hypothsis Testing for Regression Models We want to test whether there is a linear relationship between GPA and SAT scores at the \\(\\alpha = 0.10\\) significance level. While this is automatically done in R, we will practice conducting the hypothesis test using the five-step procedure, during which you will become more familiar with the R output. Step 1 - Hypotheses As we are interested in the predictive power of SAT, we will perform a hypothesis test on the slope of the model. Suppose the model has the form of \\(Y_i=\\beta_0+\\beta_1 x_i +\\epsilon_i\\). \\[H_0 : \\beta_1 = 0 \\quad \\text{versus} \\quad H_1 : \\beta_1 \\neq 0\\] Learn More A regression equation with slope \\(\\beta_1 = 0\\) would indicate no influence of \\(x\\) on \\(y\\). Thus, to assess whether \\(x\\) is a useful predictor, we test if the slope is different from 0. Step 2 - Test statistic Looking at the model summary below, we find that the value of test statistic is \\(\\hat{\\beta}_1 = 0.0031\\). ## ## Call: ## lm(formula = gpa ~ sat, data = GRADES) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.04954 -0.25960 -0.00655 0.26044 1.09328 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.1920638 0.2224502 -5.359 2.32e-07 *** ## sat 0.0030943 0.0001945 15.912 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3994 on 198 degrees of freedom ## Multiple R-squared: 0.5612, Adjusted R-squared: 0.5589 ## F-statistic: 253.2 on 1 and 198 DF, p-value: &lt; 2.2e-16 Assuming the assumptions of a linear model are satisfied then: \\[\\hat{\\beta}_1 \\sim N(\\beta_1, \\sigma^2_{\\hat{\\beta}_1})\\] The standardised test statistic under the assumption that \\(H_0\\) is true and its distribution are: \\[\\frac{\\hat{\\beta}_1-\\beta_1}{s_{\\hat{\\beta}_1}} \\sim t_{200-2}\\] Step 3 - Rejection region calculations Finding the rejection region Because the standardised test statistic is distributed \\(t_{198}\\) and \\(H_1\\) is a two-sided hypothesis, the rejection region is the \\(|t_\\text{obs}| &gt; t_{1-\\alpha;n-p} = t_{0.95;198}\\). The value of standardised test statistic is given in the R output as 15.912 (t value associated with the predictor sat). Let's verify this is correct: \\[t_\\text{obs}=\\frac{\\hat{\\beta}_1-\\beta_1}{s_{\\hat{\\beta}_1}} = \\frac{0.0031-0}{0.0001945}=15.9117\\] \\(s_{\\hat{\\beta}_1}\\) can be found by either looking at Std. Error associated with sat, or taking the square root of the (2,2) entry of the variance-covariance matrix of \\(\\mathbf{s}^2_{\\boldsymbol \\beta}\\): s2_beta &lt;- vcov(Model2) #the variance-covariance matrix s2_beta ## (Intercept) sat ## (Intercept) 4.948408e-02 -4.290866e-05 ## sat -4.290866e-05 3.781665e-08 sqrt(s2_beta[2,2]) ## [1] 0.000194465 The critical value (the \\(t\\)-value that corresponds to the significance level of 0.01), \\(t_{0.95;198}\\), can be found using: qt(0.95,198) ## [1] 1.652586 Step 4 - Statistical Conclusion QUESTION: Based on the value of standardised test statistic and critical value, do we reject the null hypothesis? Solution We reject \\(H_0\\) because the value of standardised test statistic is greater than the critical value and hence is in the rejection region, i.e. \\(t_{obs} = 15.9117 \\geq 1.6526\\). Alternatively, we can compute the \\(p\\)-value associated with this \\(t\\)-test, which is equal to \\(2 \\times \\mathbb{P}(t_{0.95;198} \\geq 15.9117)\\). For the area to the right of the observed value of standardised test statistic, use lower.tail=FALSE. pvalue &lt;- 2*pt(15.9117, 198, lower.tail = FALSE) pvalue ## [1] 2.923121e-37 This gives the \\(p\\)-value nearly 0. Step 5 - English conclusion QUESTION: What does the previoius statistical conclusion tell you about the relationship between GPA and SAT scores? There is insufficient evidence to suggest a linear relationship between sat and gpa. There is sufficient evidence to suggest a non-linear relationship between sat and gpa. There is sufficient evidence to suggest a linear relationship between sat and gpa. We cannot conclude if there is sufficient evidence to suggest a linear relationship between sat and gpa. 2.2 Confidence intervals for model parameters We will practice constructing 90% confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\). These 90% CIs will take the format of \\(\\left( \\hat{\\beta}_k - t_{0.95;198} \\cdot s_{\\hat{\\beta}_k} \\text{ },\\text{ } \\hat{\\beta}_k + t_{0.95;198} \\cdot s_{\\hat{\\beta}_k}\\right)\\) The parameter estimates \\(\\hat{\\beta}_k\\) can be found in the model summary and their estimated standard errors \\(s_{\\hat{\\beta}_k}\\) can be found from the variance-covariance matrix or from the model summary as well. b0 &lt;- coef(summary(Model2))[1, 1] s.b0 &lt;- coef(summary(Model2))[1, 2] #or s.bo &lt;- sqrt(vcov(Model2)[1,1]) b1 &lt;- coef(summary(Model2))[2, 1] s.b1 &lt;- coef(summary(Model2))[2, 2] #or s.bo &lt;- sqrt(vcov(Model2)[2.2]) ct &lt;- qt(1 - 0.1/2, 198) # alpha = 0.10 CI.B0 &lt;- b0 + c(-1, 1) * ct * s.b0 CI.B1 &lt;- b1 + c(-1, 1) * ct * s.b1 CI.B0 ## [1] -1.5596818 -0.8244458 CI.B1 ## [1] 0.00277290 0.00341564 R also has a built-in function confint to compute the confidence interval: confint(Model2, level = 0.9) ## 5 % 95 % ## (Intercept) -1.5596818 -0.82444581 ## sat 0.0027729 0.00341564 "],["example-2-cherry-trees.html", "3 Example 2: Cherry trees 3.1 Exploratory analysis 3.2 Statistical analysis", " 3 Example 2: Cherry trees The trees dataset in R give the volume (in cubic feet), diameter (in inches), and height (in feet) for a sample of 31 black cherry trees in the Allegheny National Forest, Pennsylvania. The data were collected in order to find an estimate for the volume of a tree (and therefore its timber yield), given its height and diameter. A starting point for estimating volume using these data is the geometric formula for a cylinder: \\[\\mathrm{volume} = \\pi \\times \\left(\\frac{\\mathrm{diameter}}{2}\\right)^2\\times \\mathrm{height}\\] Since the underlying geometric model involves the multiplication of the two predictors of interest, we apply a log transformation to convert them in an additive term and obtain the following multiple linear regression model: \\[\\begin{equation} \\log(\\mathrm{volume}_i) = \\alpha+\\beta \\log(\\mathrm{diameter}_i)+\\gamma \\log(\\mathrm{height}_i)+\\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2), \\quad i=1,\\ldots,31 \\tag{3.1} \\end{equation}\\] Now the question of interest is how to model the relationship between log(volume) (\\(y\\)) and log(diameter) (\\(x_1\\)) and log(height) (\\(x_2\\)). To open the dataset and apply the log-transformation, type: # library(stats) #package automatically loaded in R trees &lt;- trees trees1 &lt;- log(trees) colnames(trees1) &lt;- c(&quot;logdiam&quot;, &quot;loght&quot;, &quot;logvol&quot;) trees1 &lt;- trees1[,c(3,1,2)] #move the response variable to first column to facilitate the subsequent visualisation 3.1 Exploratory analysis A matrix plot with scatterplots between each pair of variables can be used to gain an initial impression. This can be produced by using the following command in R: pairs(trees1, lower.panel = NULL) Figure 3.1: Scatterplot displaying the relationships between the three variables. The argument lower.panel = NULL only displays the top diagonal of the matrix which ensures the response of log(volume) is on the y-axis of the plots. It may be more appropriate to use upper.panel = NULL depending on the position of the response variable in the dataframe. Figure 3.1 shows the relationships between the three variables. DISCUSSION: What can we say about the relationships between the three variables? 3.2 Statistical analysis TASK: Fit the multiple linear regression model in formula (3.1) and display the summary of the model. Solution Model3 &lt;- lm(logvol ~ logdiam + loght, data = trees1) summary(Model3) ## ## Call: ## lm(formula = logvol ~ logdiam + loght, data = trees1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.168561 -0.048488 0.002431 0.063637 0.129223 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.63162 0.79979 -8.292 5.06e-09 *** ## logdiam 1.98265 0.07501 26.432 &lt; 2e-16 *** ## loght 1.11712 0.20444 5.464 7.81e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.08139 on 28 degrees of freedom ## Multiple R-squared: 0.9777, Adjusted R-squared: 0.9761 ## F-statistic: 613.2 on 2 and 28 DF, p-value: &lt; 2.2e-16 3.2.1 Hypothesis testing on model parameters To decide if the predictors are useful, we perform a hypothesis test for each of the parameters: \\[H_0: \\beta = 0 \\ /\\ \\gamma = 0 \\quad \\text{vs.} \\quad H_1: \\beta \\ne 0 \\ / \\ \\gamma \\ne 0\\] QUESTION: Based on the model summary produced in R, what can you conclude about the usefulness of log(diameter) and log(height) in predicting log(volume)? log(diameter) is a statistically significant predictor of log(volume) log(diameter) is a statistically significant predictor of log(volume) in addition to log(height). log(diameter) is not a statistically significant predictor of log(volume). We cannot conclude if log(diameter) is a statistically significant predictor of log(volume). log(height) is a statistically significant predictor of log(volume). log(height) is a statistically significant predictor of log(volume) in addition to log(diameter). log(height) is not a statistically significant predictor of log(volume) in addition to log(diameter). We cannot conclude if log(height) is a statistically significant predictor of log(volume). Solution Since the \\(p\\)-values for logdiam and loght are both \\(&lt; 0.001\\) (indicated by the significance codes in R '***'), the null hypothesis \\(H_0\\) is rejected and we conclude that log(diameter) is a statistically significant predictor of log(volume) in addition to log(height), and log(height) is a statistically significant predictor of log(volume) in addition to log(diameter). 3.2.2 Confidence interval on model parameters QUESTION: The 95% confidence interval for \\(\\beta\\) is (, ). The 95% confidence interval for \\(\\gamma\\) is (, ). Hint Calculate using the formula (1.1) or use the R function confint(). Solution confidence interval for \\(\\hat{\\beta}\\) From the regression output, we know \\(\\hat{\\beta}\\) (the coefficient, estimate of the model parameter) and the estimated standard error for the parameter, which are 1.98265 and 0.07501 respectively. We could also verify the result of estimated standard error. We know that \\[\\hat{\\sigma} = 0.08139 \\quad\\quad\\quad\\quad\\quad n-p = 31 - 3 = 28,\\] and \\((\\mathbf{X}^\\top\\mathbf{X})^{-1}\\) can be computed by using X &lt;- model.matrix(~ logdiam + loght, data = trees1) XTXI &lt;- solve(t(X) %*% X) # or # XTXI &lt;- summary(Model3)$cov.unscaled which gives \\[(\\mathbf{X}^\\top\\mathbf{X})^{-1} = \\begin{bmatrix} 96.572067 &amp; 3.1392672 &amp; -24.165092\\\\ 3.139267 &amp; 0.8494646 &amp; -1.227489\\\\ -24.165092 &amp; -1.2274894 &amp; 6.309851 \\end{bmatrix}\\] Therefore, the estimated standard error for \\(\\beta\\) is given by 0.08139 * sqrt(0.8494646) ## [1] 0.07501424 To calculate the confidence interval, it remains to find the 0.975th quantile of the \\(t\\)-distribution with 28 degrees of freedom, and we do this in R as follows: qt(p = 0.975, df = 28) ## [1] 2.048407 Therefore, a 95% confidence interval for \\(\\beta\\) can be found by computing: 1.98265 + c(-1,1) * 2.048407 * 0.07501424 ## [1] 1.82899 2.13631 confidence interval for \\(\\gamma\\) In a similar manner, we can find the confidence interval for \\(\\gamma\\) as follows: 1.11712 + c(-1,1) * 2.048407 * 0.20444 #estimate +/- t_value * estimated standard error #The estimated standard error is computed as 0.08139 * sqrt(6.309851) Both intervals can be computed simultaneously in using the following command: confint(Model3) ## 2.5 % 97.5 % ## (Intercept) -8.269912 -4.993322 ## logdiam 1.828998 2.136302 ## loght 0.698353 1.535894 QUESTION: What conclusions can be drawn from these confidence intervals? Solution The coefficient for log(diameter) is highly likely to lie between 1.83 and 2.14. As this interval does not contain 0, we conclude that the predictor log(diameter) makes a statistically significant contribution in addition to the predictor log(height) in explaining the variability in log(volume). Therefore log(diameter) is retained in the model, in addition to log(height). The coefficient for log(height) is highly likely to lie between 0.70 and 1.54. As this interval does not contain 0, we conclude that the predictor log(height) makes a statistically significant contribution in addition to the predictor log(diameter) in explaining the variability in log(volume). Therefore log(height) is retained in the model, in addition to log(diameter). 3.2.3 Confidence interval for the population mean response and prediction interval for a future response Once we have fitted the model of interest it may also be useful to compute further confidence and prediction intervals from the fitted model. For example, 95% confidence interval for the population mean response; and 95% prediction interval for the response of an individual member of the population. QUESTIONS Suppose that we consider a population of cherry trees for which the log(diameter) is 2.4 and the log(height) is 4.3. Provide a 95% confidence interval for the mean log(volume) in this population of cherry trees. Interpret the interval. Suppose, now, that we wish to obtain a 95% prediction interval for an individual cherry tree in the population which has a log(diameter) of 2.4 and a log(height) of 4.3. Interpret the interval. We use R to compute the required intervals. Question 1 - 95% confidence interval for the population mean: predframe &lt;- data.frame(logdiam = 2.4, loght = 4.3) predict(Model3, int = &quot;c&quot;, newdata = predframe) ## fit lwr upr ## 1 2.930373 2.894059 2.966686 The data.frame command creates a table; each column represents one variable and each row contains one set of values from each column. The predict command can produce both a confidence interval and a prediction interval. To obtain a confidence interval we use the argument int = \"c\". Therefore we conclude that in a population of cherry trees for which the log(diameter) is 2.4 and the log(height) is 4.3 it is highly likely that the log(volume) would lie, on average, somewhere between 2.89 and 2.97. Question 2 - 95% prediction interval for a future tree: predict(Model3, int = &quot;p&quot;, newdata = predframe) ## fit lwr upr ## 1 2.930373 2.759752 3.100994 The argument int = \"p\" in the predict command provides a prediction interval based on the specified values in the new dataframe predframe. Therefore if a cherry tree with a log(diameter) of 2.4 and a log(height) of 4.3 were selected randomly from the population of cherry trees, it is highly likely that it would have a log volume of somewhere between 2.76 and 3.10. Comparing with the 95% confidence interval, the 95% prediction interval has a wider range. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
