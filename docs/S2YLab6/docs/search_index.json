[["index.html", "Lab 6 - Hypothesis testing and interval estimation 1 Welcome to Lab 6 1.1 The summary() function for regression models 1.2 Hypothesis Testing for Linear Relationships 1.3 Confidence Intervals for Linear Model Estimators", " Lab 6 - Hypothesis testing and interval estimation 1 Welcome to Lab 6 Intended Learning Outcomes: use R to produce hypothesis tests for parameters in a linear model; use various summary statistics and R output to compute confidence and prediction intervals; interpret hypothesis tests, confidence intervals and prediction intervals. 1.1 The summary() function for regression models Before discussing hypothesis testing and confidence intervals for regression models, let's first revise the output of summary() function. Recall the model we created in Lab 4, where we try to predict the hydrostatic fat measurement of a wrestler from abomnimal fat and tricep fat measurements. library(PASWR) HSWRESTLER &lt;- HSwrestler Model1 &lt;- lm(HWFAT ~ ABS + TRICEPS, data = HSWRESTLER) We apply the summary function, which gives the following output: summary(Model1) ## ## Call: ## lm(formula = HWFAT ~ ABS + TRICEPS, data = HSWRESTLER) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.5558 -2.2550 -0.5245 2.3365 9.4957 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.05904 0.65219 3.157 0.0023 ** ## ABS 0.33708 0.06415 5.255 1.34e-06 *** ## TRICEPS 0.50430 0.09920 5.084 2.64e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.061 on 75 degrees of freedom ## Multiple R-squared: 0.8832, Adjusted R-squared: 0.8801 ## F-statistic: 283.6 on 2 and 75 DF, p-value: &lt; 2.2e-16 As you can see from this output there are a few different elements displayed. A brief description is stated below: Call: This shows the formula that we used in our regression model. Residuals: This lists the five-number summary of the residuals from our regression model. Coefficients: This shows us a summary of estimated coefficients of the regression model. Within this section the column headers are: Estimate: The estimated parameter. These can be used to write down the fitted regression model. Std. Error: This is the estimated standard error of the parameter estimate. t value: This is the \\(t\\)-statistic for the parameter, calculated as Estimate / Std. Error. Pr(&gt;|t|): This is the \\(p\\)-value that corresponds to the \\(t\\)-statistic, i.e. \\(Pr(X&gt;|t|)\\) for \\(X \\sim t(n-p)\\), where \\(t\\) is the t value computed above, \\(n\\) is the sample size, and \\(p\\) is the number of parameters. Significance codes: This is the key for the starts next to the \\(p\\)-value. It gives you a first glance of which estimators are significant at hat standard significance level for ease of conclusion drawing. Residual standard error: This is the square root of residual mean squares, which can be linked to the output from an anova() table (Residuals Mean Sq). Multiple R-squared: This gives coefficient of determination, \\(R^2\\). Adjusted R-squared: This gives the adjusted coefficient of determination, \\(R^2\\) (adj), which adjusts for the number of predictors in the model. F-statistic: The \\(F\\)-statistic is the test statistic for the hypothesis test H\\(_0\\): all \\(p-1\\) parameters \\(= 0\\) versus H\\(_1\\): at least one parameter \\(\\neq 0\\) p-value: \\(p\\)-value corresponding to the \\(F\\)-test, i.e. \\(Pr(X&gt;|F|)\\) for \\(X \\sim F(\\text{DF}_\\text{model},\\text{DF}_\\text{residual})\\). Here we can interpret things such as both Abdominal Fat and Tricep Fat measurements are significant in predicting the Hydrostatic fat measurement to a significance level of \\(\\approx 0\\) from looking at the \\(t\\)-test \\(p\\)-values. Also from the \\(F\\)-test \\(p\\)-value, we can see that we reject \\(H_0\\) and say that at least one parameter is \\(\\neq 0\\) We could also look at the fact that the estimates of the model coefficients \\(\\hat{\\beta}_k\\) are positive so both Abs and Tri have a positive relationship with Hydrostatic fat. The \\(R^2_{adj}\\) is also high meaning the model explains the data well. 1.2 Hypothesis Testing for Linear Relationships So far, we have seen how to show models in Matrix form. Under this new notation, a linear model \\(\\bf{Y} \\sim N (\\bf{X} \\boldsymbol{\\beta},\\sigma^2 \\bf{I})\\) assumes \\(\\bf{\\epsilon} \\sim N (0,\\sigma^2 \\bf{I})\\) and has \\(\\hat{\\boldsymbol{\\beta}} \\sim N (\\boldsymbol{\\beta},\\sigma^2 \\bf{(X&#39;X)}^{-1})\\). 1.2.1 Finding the Varaince We know that \\(\\hat{\\sigma}^2 = s^2=MSE=\\frac{SSE}{n-p}=\\frac{\\sum^2_{i=1} \\hat{\\epsilon}^2_i}{n-p}\\) and \\(\\bf{s}^2_{{\\hat{\\boldsymbol{\\beta}}}} = \\sigma^2 \\bf{(X&#39;X)}^{-1} = \\begin{bmatrix} s^2_{{\\hat{\\beta}_0}} &amp; s_{\\hat{\\beta}_0,\\hat{\\beta}_1} &amp;...&amp; s_{\\hat{\\beta}_0,\\hat{\\beta}_{p-1}}\\\\ s_{\\hat{\\beta}_1,\\hat{\\beta}_0} &amp; s^2_{\\hat{\\beta}_1} &amp;...&amp; s_{\\hat{\\beta}_1,\\hat{\\beta}_{p-1}}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ s_{\\hat{\\beta}_{p-1},\\hat{\\beta}_0} &amp; s_{\\hat{\\beta}_{p-1},\\hat{\\beta}_1} &amp;...&amp; s^2_{\\hat{\\beta}_{p-1}} \\end{bmatrix}\\) In R this matrix can be found using vcov() on a linear model object. 1.2.2 Steps of the Hyphotesis Test If we want to test an model coefficient/estimator against a hypothesized value \\[H_0: \\beta_k = \\beta_{k_0} \\quad \\text{vs} \\quad H_1: \\beta_k \\neq \\beta_{k_0}\\] we can use the Standardised Test Statistic: \\[\\frac{\\text{unbiased estimator - hypothesized value}}{\\text{standard error of estimator}} = \\frac{\\hat{\\beta_k}-\\beta_{k_0}}{s_{\\hat{\\beta}_k}} \\sim t_{n-p}\\] Step 1: Hypotheses - for the estimate you would like to test. \\[H_0: \\beta_k = \\beta_{k_0} \\quad \\text{vs} \\quad H_1: \\beta_k \\neq \\beta_{k_0}\\] Step 2: Test Statistic \\(\\hat{\\beta_k}\\) is the test statistic. The assumptions \\(\\hat{\\beta_k} \\sim N(\\beta_k, \\sigma^2_{\\hat{\\beta_k}})\\) Step 3: Hypothesis Test Calculations The Standardised Test Statistic \\(\\frac{\\hat{\\beta_k}-\\beta_{k_0}}{s_{\\hat{\\beta}_k}} \\sim t_{n-p} \\text{ and so } t_{obs} = \\frac{\\hat{\\beta_k}-\\beta_{k_0}}{s_{\\hat{\\beta}_k}}\\). This can be compared to the Critical Value, \\(t_{1-\\alpha,\\text{ df}}\\), which can be found from the statistical tables (if completing the test by hand). In R the test can be run using the function lm(). For example: model &lt;- lm() summary(model)$coef This will give the observed t-value and the corresponding p-value which can then be compared to \\(\\alpha\\). Step 4: Statistical Conclusion I. From the rejection region... or II. From the p-value... Step 5: English Conclusion There is evidence/not enough evidence to suggest a linear relationship between... 1.3 Confidence Intervals for Linear Model Estimators We may also want to build confidence intervals (CI) for our model estimators. The CI for estimator \\(\\beta_k\\) with confidence level \\(1-\\alpha\\) is: \\[\\left( \\hat{\\beta}_k - t_{1-\\alpha/2\\text{ };\\text{ }n-p} \\cdot s_{\\hat{\\beta}_k} \\text{ },\\text{ } \\hat{\\beta}_k + t_{1-\\alpha/2\\text{ };\\text{ }n-p} \\cdot s_{\\hat{\\beta}_k}\\right)\\] where the degrees of freedom is \\(n-p\\) because \\(\\sigma^2\\) is esimated using \\(MSE=\\frac{SSE}{n-p}\\). The values needed for the CI can again be found using: model &lt;- lm() summary(model)$coef #and/or model.matrix() #and qt(p,df) #where p=1-alpha/2 We will see how these are used in today's lab examples. "],["example-1---modelling-college-grades.html", "2 Example 1 - Modelling College Grades 2.1 Building the Model 2.2 Hypothsis Testing for Regression Models 2.3 Confidence intervals for model terms", " 2 Example 1 - Modelling College Grades In Lab 4, we looked at the Grades dataset from the PASWR package, which records the first-semester college GPA and SAT scores for 200 freshmen. The question of interest is to check whether there is a linear relationship between GPA and SAT scores. To open the dataset, type: library(PASWR) GRADES &lt;- Grades 2.1 Building the Model In Lab 4, we viewed the data in scatterplots, built a linear model, and calculated correlation to learn more about the relationship between SAT and GPA scores. Today, we will look at checking the assumptions for building this regression model. Task 1 Build the simple linear regression model for how GPA scores change with SAT scores again: Hint Remember the function lm(y~x, data=). It will be helpful to save this output using &lt;- for future analysis Solution To find the equation that best describes the relationship between gpa and sat use: Model2&lt;- lm(gpa ~ sat, data = GRADES) summary(Model2) ## ## Call: ## lm(formula = gpa ~ sat, data = GRADES) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.04954 -0.25960 -0.00655 0.26044 1.09328 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.1920638 0.2224502 -5.359 2.32e-07 *** ## sat 0.0030943 0.0001945 15.912 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3994 on 198 degrees of freedom ## Multiple R-squared: 0.5612, Adjusted R-squared: 0.5589 ## F-statistic: 253.2 on 1 and 198 DF, p-value: &lt; 2.2e-16 This fits a simple linear regression model with the response variable gpa and the explanatory variable sat. Question 1 What is the equation of the regression line? Hint: You will need to use the summary() function. (Enter your answers to 3 decimal places.) gpa = + sat Task 2 Plot the data on a scatterplot with the the simple linear regression line added : Hint plot(y ~ x, data = , xlab = &quot; &quot;, ylab = &quot; &quot;, main = &quot; &quot;) abline() Solution plot(gpa ~ sat, data = Grades, xlab = &quot;SAT score&quot;, ylab = &quot;GPA&quot;, main = &quot;Scatterplot of GPA versus SAT scores&quot;, pch=20) abline(Model2) 2.2 Hypothsis Testing for Regression Models We want to test whether there is a statistical linear relationship between GPA and SAT scores at the \\(\\alpha = 0.10\\) significance level rather than drawing these conclusions from graphical interpretations. We should complete the 5-step hypothesis test procedure on the slope of the model. Step 1 - Hypotheses \\[H_0 : \\beta_1 = 0 \\quad \\text{versus} \\quad H_1 : \\beta_1 \\neq 0\\] Learn More A regression equation with slope \\(\\beta_1 = 0\\) would indicate no gradient and hence no relationship as \\(y\\) does not change as \\(x\\) changes. To test for a relationship, we see if the slope is different from 0. Step 2 - Test Statistic The test statistic is \\(\\hat{\\beta}_1 = 0.0031\\) from the Model2 summary above. Assuming the assumptions of Model are satisfied then: \\[\\hat{\\beta_1} \\sim N(\\beta_1, \\sigma^2_{\\hat{\\beta_1}})\\] The standardized test statistic under the assumption that \\(H_0\\) is true, and its approximate distribution are: \\[\\frac{\\hat{\\beta_1}-\\beta_{1}}{s_{\\hat{\\beta}_1}} \\sim t_{200-2}\\] This is what will be used to complete the test. Step 3 - Hypothesis Test Calculations Finding your Rejection Region Because the standardized test statistic is distributed \\(t_{198}\\) and \\(H_1\\) is a two-sided hypothesis, the rejection region is the \\(|t_{obs}| &gt; t_{0.95;198}=1.6526\\) This Critical Value (the t-value that corresponds to our significance level) above can be found using: qt(0.95,198) ## [1] 1.652586 Finding your standardised test statistic and p-value Our standardised test statistic \\(t_{obs}=\\frac{\\hat{\\beta_1}-\\beta_{1}}{s_{\\hat{\\beta}_1}} = \\frac{0.0031-0}{2\\times10^{-4}}=15.9117\\) \\(s^2_{\\hat{\\beta}_1}\\) was the Variance-Covariance Matrix: vcov(Model2) ## (Intercept) sat ## (Intercept) 4.948408e-02 -4.290866e-05 ## sat -4.290866e-05 3.781665e-08 and therefore \\(s_{\\hat{\\beta}_1} = \\sqrt{3.782\\times10^{-8}} = 2\\times10^{-4}\\). This number is also in the Model Summary as 'SAT Standard Error'. We should use r to find the p-value corresponding to the calculated Standardised Test Statistic (STS) \\(2 \\times \\mathrm{P}(t_{0.95;198} \\geq 15.9117)\\). For the area to the right of our STS, we use lower.tail=FALSE pvalue &lt;- pt(15.9117,198, lower = FALSE) pvalue ## [1] 1.46156e-37 This gives the p-value as \\(2 \\times 1.46156\\times10^{-37} \\approx 0\\) . 15.9117 is greater than 1.6526 and our p-value, 0, is less than \\(\\alpha = 0.10\\). Step 4 - Statistical Conclusion To draw our conclusions we need to consider our rejection region. Is our standardised test statistic inside the rejection region? Is our p-value smaller than 0.05? QUESTION: Do we reject our null hypothesis? Solution I. From the rejection region, we reject \\(H_0\\) because the standardised test statistic is greater than the critical value and hence is in the rejection region i.e \\(t_{obs} = 15.9117 \\geq 1.6526\\). OR From the p-value, we fail to reject \\(H_0\\) because the p-value\\(0 &lt; 0.10\\). Whichever method we use, we reject \\(H_0\\). Step 5 - English Conclusion What does our statistical conclusion mean for the data and the purpose of the test? Is there statistical evidence to suggest a linear relationship between GPA and SAT scores? QUESTION: Which of the following is the correct conclusion of our test? There is not enough evidence to suggest a linear relationship between sat and gpa. There is evidence to suggest a non-linear relationship between sat and gpa. There is evidence to suggest a linear relationship between sat and gpa. We cannot conclude if there is sufficient evidence to suggest to suggest a linear relationship between sat and gpa. 2.3 Confidence intervals for model terms We will practice constructing 90% confidence intervals for \\(\\beta_0\\) and \\(\\beta_1\\). These 90% CIs will take the format of \\(\\left( \\hat{\\beta}_k - t_{0.95;198} \\cdot s_{\\hat{\\beta}_k} \\text{ },\\text{ } \\hat{\\beta}_k + t_{0.95;198} \\cdot s_{\\hat{\\beta}_k}\\right)\\) The \\(\\hat{\\beta}_k\\) estimates can be found in the model summary and \\(s_{\\beta_k}\\) from the Variance-Covariance Matrix again or from the Model Summary. summary(Model2) ## ## Call: ## lm(formula = gpa ~ sat, data = GRADES) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.04954 -0.25960 -0.00655 0.26044 1.09328 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.1920638 0.2224502 -5.359 2.32e-07 *** ## sat 0.0030943 0.0001945 15.912 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.3994 on 198 degrees of freedom ## Multiple R-squared: 0.5612, Adjusted R-squared: 0.5589 ## F-statistic: 253.2 on 1 and 198 DF, p-value: &lt; 2.2e-16 b0 &lt;- coef(summary(Model2))[1, 1] s.b0 &lt;- coef(summary(Model2))[1, 2] b1 &lt;- coef(summary(Model2))[2, 1] s.b1 &lt;- coef(summary(Model2))[2, 2] ct &lt;- qt(1 - 0.1/2, 198) # alpha = 0.10 CI.B0 &lt;- b0 + c(-1, 1) * ct * s.b0 CI.B1 &lt;- b1 + c(-1, 1) * ct * s.b1 CI.B0 ## [1] -1.5596818 -0.8244458 CI.B1 ## [1] 0.00277290 0.00341564 Or by using the model command: confint(Model2, level = 0.9) ## 5 % 95 % ## (Intercept) -1.5596818 -0.82444581 ## sat 0.0027729 0.00341564 "],["example-2-cherry-trees.html", "3 Example 2: Cherry trees 3.1 Exploratory analysis 3.2 Statistical analysis", " 3 Example 2: Cherry trees This data set has been looked at in the lectures. The main question of interest is how to model the relationship between log(volume) (\\(y\\)) and log(diameter) (\\(x_1\\)) and log(height) (\\(x_2\\)), using data from 31 cherry trees. To model the relationship we decided to go with the multiple linear regression model \\[\\begin{equation} Y_i = \\alpha + \\beta x_{1i} + \\gamma x_{2i} + \\epsilon_i, \\quad \\epsilon_i \\sim N(0, \\sigma^2), \\quad i=1,\\ldots,31 \\tag{3.1} \\end{equation}\\] 3.1 Exploratory analysis A matrix plot with scatterplots between each pair of variables can be used to gain an initial impression. This can be produced by using the following command in R: trees1 &lt;- read.csv(&quot;trees1.csv&quot;) pairs(trees1, lower.panel = NULL) Figure 3.1: Scatterplot displaying the relationships between the three variables. The argument lower.panel = NULL only displays the top diagonal of the matrix which ensures the response of log(volume) is on the y-axis of the plots. It may be more appropriate to use upper.panel = NULL depending on the position of the response variable in the dataframe. Figure 3.1 shows the relationships between the three variables. DISCUSSION: What can we say about the relationships between the three variables? 3.2 Statistical analysis We now fit the multiple linear regression model in formula (3.1) in R using the following command: Model3 &lt;- lm(logvol ~ logdiam + loght, data = trees1) and produce residual plots to graphically assess the assumptions of the linear model using: par(mfrow=c(1,2)) plot(rstandard(Model3) ~ fitted(Model3)) abline(h=0, lty=3) qqnorm(rstandard(Model3)) qqline(rstandard(Model3)) Figure 3.2: Residuals vs. fitted values (left) and normal Q-Q plot (right) from model (2.1) fitted to trees. DISCUSSION: Based on the residual plots, comment on whether the assumptions of normal linear models seem to hold or not. 3.2.1 Regression output We can now examine the regression output by typing: summary(Model3) ## ## Call: ## lm(formula = logvol ~ logdiam + loght, data = trees1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.168561 -0.048488 0.002431 0.063637 0.129223 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.63162 0.79979 -8.292 5.06e-09 *** ## logdiam 1.98265 0.07501 26.432 &lt; 2e-16 *** ## loght 1.11712 0.20444 5.464 7.81e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.08139 on 28 degrees of freedom ## Multiple R-squared: 0.9777, Adjusted R-squared: 0.9761 ## F-statistic: 613.2 on 2 and 28 DF, p-value: &lt; 2.2e-16 QUESTION: Which of the following comments is most appropriate to describe how well the model fits the data? Based on the R^2, 97.77% of the variation in the log volume of the cherry trees is accounted for by the linear model with log diameter and log height as predictors, and hence the model provides a very good fit to the data. Based on the adjusted R^2, 97.61% of the variation in the log volume of the cherry trees is accounted for by the linear model with log diameter and log height as predictors, and hence the model provides a very good fit to the data. The p-value for both logdiam and loght are less than 0.05, and hence the model provides a very good fit to the data. The p-value for F-test is less than 0.05, and hence the model provides a very good fit to the data. The analysis of variance (ANOVA) table can also be obtained using: anova(Model3) ## Analysis of Variance Table ## ## Response: logvol ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## logdiam 1 7.9254 7.9254 1196.53 &lt; 2.2e-16 *** ## loght 1 0.1978 0.1978 29.86 7.805e-06 *** ## Residuals 28 0.1855 0.0066 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.2.2 Hypothesis testing The hypotheses being tested for each of the parameters here are: \\[H_0: \\beta = 0 \\ /\\ \\gamma = 0 \\quad \\text{vs.} \\quad H_1: \\beta \\ne 0 \\ / \\ \\gamma \\ne 0\\] Since the \\(p\\)-values for logdiam and loght are both \\(&lt; 0.001\\) (indicated by the significance codes in R '***'), and hence \\(&lt; 0.05\\), the null hypothesis \\(H_0\\) is rejected and we conclude that log(diameter) is a statistically significant predictor of log(volume) in addition to log(height), and log(height) is a statistically significant predictor of log(volume) in addition to log(diameter). 3.2.3 Confidence interval for \\(\\mathbf{b}^\\top \\boldsymbol\\beta\\) From lectures, the formula for a 95% confidence interval for a linear combination of the model parameters is \\[ \\mathbf{b}^\\top\\boldsymbol{\\hat{\\beta}}\\pm t\\left(n-p; 0.975\\right)\\cdot e.s.e(\\mathbf{b}^\\top \\hat{\\boldsymbol\\beta})\\] or, more explicitly, \\[ \\mathbf{b}^\\top\\boldsymbol{\\hat{\\beta}}\\pm t\\left(n-p; 0.975\\right)\\sqrt{\\frac{\\text{RSS}}{n-p}\\mathbf{b}^\\top(\\mathbf{X}^\\top\\mathbf{X})^{-1}\\mathbf{b}}\\] A 95% confidence interval for \\(\\beta\\), the coefficient of log(diameter), can be formed by taking the vector \\(\\mathbf{b}\\) to be \\[\\mathbf{b} = \\begin{bmatrix} 0 \\\\ 1 \\\\ 0 \\end{bmatrix}.\\] From the regression output, we know \\(\\hat{\\beta}\\) (the coefficient, estimate of the model parameter) and the estimated standard error for the parameter, which are 1.98265 and 0.07501 respectively. We could also verify the result of estimated standard error. We know that \\[\\text{RSS} = 0.1855 \\quad\\quad\\quad\\quad\\quad n-p = 31 - 3 = 28,\\] and \\((\\mathbf{X}^\\top\\mathbf{X})^{-1}\\) can be computed by using X &lt;- model.matrix(~ logdiam + loght, data = trees1) solve(t(X) %*% X) which gives \\[(\\mathbf{X}^\\top\\mathbf{X})^{-1} = \\begin{bmatrix} 96.572067 &amp; 3.1392672 &amp; -24.165092\\\\ 3.139267 &amp; 0.8494646 &amp; -1.227489\\\\ -24.165092 &amp; -1.2274894 &amp; 6.309851 \\end{bmatrix}\\] Therefore, the estimated standard error for \\(\\beta\\) is given by sqrt(0.1855 / 28 * 0.8494646) ## [1] 0.07501802 To calculate the confidence interval, it remains to find the 0.975th quantile of the \\(t\\)-distribution with 28 degrees of freedom, and we do this in R as follows: qt(p = 0.975, df = 28) ## [1] 2.048407 Calculations Therefore, a 95% confidence interval for \\(\\beta\\) can be found by computing: 1.98265 + 2.048407 * 0.07501 ## [1] 2.136301 1.98265 - 2.048407 * 0.07501 ## [1] 1.828999 Hence a 95% confidence interval for the coefficient of log(diameter) is (1.83, 2.14). As this interval does not contain 0, we conclude that the predictor log(diameter) makes a statistically significant contribution in addition to the predictor log(height) in explaining the variability in log(volume). Therefore log(diameter) is retained in the model, in addition to log(height). The coefficient for log(diameter) is highly likely to lie between 1.83 and 2.14. These intervals can be computed for all estimated parameters simultaneously in using the following command: confint(Model3) ## 2.5 % 97.5 % ## (Intercept) -8.269912 -4.993322 ## logdiam 1.828998 2.136302 ## loght 0.698353 1.535894 QUESTION: Calculate a 95% confidence interval for the coefficient \\(\\gamma\\) using the general formula for a confidence interval; check the result is same as the R output returned from confint(Model3). Interpret this interval. Solution 1.11712 + 2.048407 * 0.20444 1.11712 - 2.048407 * 0.20444 #The estimated standard error is computed as sqrt(0.1855 / 28 * 6.309851) We can check this against the answers in confint() results: 0.698353 1.535894 A 95% confidence interval for the coefficient of log(height) is (0.70, 1.54). As this interval does not contain 0, we conclude that the predictor log(height) makes a statistically significant contribution in addition to the predictor log(diameter) in explaining the variability in log(volume). Therefore log(height) is retained in the model, in addition to log(diameter). The coefficient for log(height) is highly likely to lie between 0.70 and 1.54. 3.2.4 Confidence interval for the population mean response and prediction interval for a future response Once we have fitted the model of interest it may also be useful to compute further confidence and prediction intervals from the fitted model. For example, a 95% confidence interval for the population mean response; and a 95% prediction interval for the response of an individual member of the population. Questions Suppose that we consider a population of cherry trees for which the log(diameter) is 2.4 and the log(height) is 4.3. Provide a 95% confidence interval for the mean log(volume) in this population of cherry trees. Interpret the interval. Suppose, now, that we wish to obtain a 95% prediction interval for an individual cherry tree in the population which has a log(diameter) of 2.4 and a log(height) of 4.3. Interpret the interval. We use R to compute the required intervals. Question 1 - 95% confidence interval for the population mean: predframe &lt;- data.frame(logdiam = 2.4, loght = 4.3) predict(Model3, int = &quot;c&quot;, newdata = predframe) ## fit lwr upr ## 1 2.930373 2.894059 2.966686 The data.frame command creates a table; each column represents one variable and each row contains one set of values from each column. The predict command can produce both a confidence interval and a prediction interval. To obtain a confidence interval we use the argument int = \"c\". Therefore we conclude that in a population of cherry trees for which the log(diameter) is 2.4 and the log(height) is 4.3 it is highly likely that the log(volume) would lie, on average, somewhere between 2.89 and 2.97. Question 2 - 95% prediction interval for a future tree: predict(Model3, int = &quot;p&quot;, newdata = predframe) ## fit lwr upr ## 1 2.930373 2.759752 3.100994 The argument int = \"p\" in the predict command provides a prediction interval based on the specified values in the new dataframe predframe. Therefore if a cherry tree with a log(diameter) of 2.4 and a log(height) of 4.3 were selected randomly from the population of cherry trees, it is highly likely that it would have a log volume of somewhere between 2.76 and 3.10. Comparing with the 95% confidence interval, the 95% prediction interval has a wider range. "],["exercise-1-body-fat.html", "4 Exercise 1: Body fat", " 4 Exercise 1: Body fat The dataset femalebodyfat.csv gives the % body fat, triceps skinfold thickness (taken at the midpoint of the upperarm) and the midarm circumference (the circumference of the non-dominant arm midway between the shoulder and the elbow) for twenty healthy females aged 20 to 34. The % body fat for each person was obtained by a cumbersome and expensive procedure requiring the immersion of the person in water. It would therefore be very helpful if a regression model with tricep skinfold thickness and midarm circumference could provide reliable predictions of the amount of body fat, since the measurements needed for the predictor variables are straightforward to obtain. Read in the data using: bf &lt;- read.csv(&quot;femalebodyfat.csv&quot;) QUESTION Use an appropriate exploratory analysis to explore the relationships between % body fat, triceps skinfold thickness and midarm circumference. Solution pairs(bf, lower.panel = NULL) This shows a moderate positive linear relationship between % body fat and triceps skinfold thickness. There doesn't seem to be much of a relationship between % body fat and midarm circumference but we will continue to explore whether it is useful to the model. Fit a multiple linear regression model to the data in order to predict % body fat from triceps skinfold thickness and midarm circumference. Provide very brief comments on the adequacy of your model. Solution modelfat &lt;- lm(Fat ~ Triceps + Midarm, data=bf) Use R to compute a 95% confidence interval for the coefficient of each predictor in the model. Solution confint(modelfat) ## 2.5 % 97.5 % ## (Intercept) -2.67783060 16.261085426 ## Triceps 0.73003885 1.271130967 ## Midarm -0.08040683 -0.005881575 Comment on your intervals. The confidence interval for the variable Triceps containsdoes not contain zero so we conclude it makesdoes not make a statistically significant contribution in addition to the predictor Midarm in explaining the variability in Fat. Therefore Triceps should be retained inremoved from the model. The coefficient for Triceps is highly likely to lie between and (Enter your answer by rounding to two decimal places). Find a 95% confidence interval for the mean % body fat for a female (aged 20 to 34) whose triceps skinfold thickness is 25mm and midarm circumference is 310mm. Interpret the interval. Hint Define the prediction dataframe for the given predictor values and use the predict() function with that dataframe. Be careful to use the correct parameter to get a confidence interval not a prediction interval. Solution predframe &lt;- data.frame(Triceps = 25, Midarm = 310) predict(modelfat, int = &quot;c&quot;, newdata = predframe) ## fit lwr upr ## 1 18.43155 16.67794 20.18516 Find a 95% prediction interval for the % body fat of a future female with skinfold thickness and midarm circumference values of your choice. Interpret the interval. Solution predframe &lt;- data.frame(Triceps = 20, Midarm = 250) predict(modelfat, int = &quot;p&quot;, newdata = predframe) ## fit lwr upr ## 1 16.01728 10.46253 21.57202 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
