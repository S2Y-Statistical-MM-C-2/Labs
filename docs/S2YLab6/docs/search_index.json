[["index.html", "Lab 6 - Ordinary least squares 1 Welcome to Lab 6! 1.1 Introduction to Ordinary Least Squares (OLS) 1.2 The simplest regression model: simple linear regression (one covariate) 1.3 The next-simplest regression model: multiple linear regression (multiple predictors) 1.4 Ordinary least squares", " Lab 6 - Ordinary least squares 1 Welcome to Lab 6! Intended Learning Outcomes: Introduce and understand the matrix notation of linear models. Obtain model parameters using ordinary least squares (OLS) and using matrix notation in R. 1.1 Introduction to Ordinary Least Squares (OLS) The primary tool used to model associations among variables is regression (linear or otherwise). Regression analysis is used to model the relationship between a single variable \\(Y\\), called the response or dependent variable, and one or more explanatory variables, also called predictor(s), covariates, or independent variable(s), \\(x_1, x_2,...,x_p\\). As a first step, the response variable will be continuous, but the predictor variables can be either continuous, discrete, or categorical. You can predict non-numeric (continuous) variables using regression models, but they will be called generalised regression models. Here, we stick to the classical regression models. Parameter estimation for linear models can be carried out via OLS or maximum likelihood estimation. In this lab, we will look at OLS. The word “regression” is due to Sir Francis Galton, who in 1885 demonstrated that offspring do not tend toward the size of the parents; rather, offspring size tends toward the mean of the population. That is, there is a “regression toward mediocrity.” What else is Francis famous for? \"Whenever you can, count\", \"Nature vs nurture\" and \"London is the epicenter of female beauty, Aberdeen is the opposite...\". Also, eugenics. Fun guy. 1.2 The simplest regression model: simple linear regression (one covariate) The simple linear regression model is defined as \\[\\begin{equation} Y_i = \\beta_0 + \\beta_1x_i + \\epsilon_i, \\tag{1.1} \\end{equation}\\] where \\(Y_i\\) is the value of the response variable for the \\(i\\)th observation. \\(\\beta_0\\) and \\(\\beta_1\\) are regression parameters. \\(\\beta_0\\) is known as the intercept. \\(\\beta_1\\) is the regression coefficient of \\(x\\) (the effect of \\(x\\) on \\(Y\\)). \\(\\epsilon_i\\) is a random error term that is assumed to have mean \\(0\\) and variance \\(sigma^2\\). The \"modelling\" bit refers to estimating model parameters, \\(\\beta_0\\) and \\(\\beta_1\\) for the \\(n\\) measurements. It looks like \\[ Y_1 = \\beta_0 + \\beta_1x_1 + \\epsilon_1\\\\ Y_2 = \\beta_0 + \\beta_1x_2 + \\epsilon_2\\\\ \\vdots\\\\ Y_n = \\beta_0 + \\beta_1x_n + \\epsilon_n.\\\\ \\] The model can be written in matrix notation as \\[ \\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\] where \\[\\mathbf{Y} = \\left[\\begin{matrix} Y_1\\\\Y_2\\\\ \\vdots \\\\Y_n \\end{matrix}\\right] \\text{ as a } (n \\times 1) \\text{ matrix, } \\mathbf{X} = \\left[\\begin{matrix} 1 &amp; x_1\\\\1 &amp; x_2 \\\\ \\vdots&amp;\\vdots \\\\ 1 &amp; x_n \\end{matrix}\\right] \\text{ as } (n \\times 2) \\text{, } \\boldsymbol{\\beta} =\\left[\\begin{matrix} \\beta_0 \\\\ \\beta_1 \\end{matrix} \\right] \\text{ as } (2 \\times 1) \\text{.} \\] Note that \\(\\text{Var}(\\boldsymbol{\\epsilon}) = \\sigma^2\\mathbf{I}\\) where \\(\\sigma^2 \\mathbf{I}\\) is the variance-covariance matrix of the vector of errors. Given that \\(\\mathbf{I}\\) is the identity matrix, \\(\\sigma^2\\) appears only along the diagonals, indicating that \\(\\epsilon_i\\) are independent from each other. 1.3 The next-simplest regression model: multiple linear regression (multiple predictors) Multiple linear regression is an extension of simple linear regression, where we have more than one coefficient \\(x\\). \\(Y\\) is still the dependent variable, \\(\\beta_0\\) the intercept, but \\(\\beta_1\\) is generalised to \\(\\beta_j\\) which is now the coefficient of \\(x_{ij}\\). The model is written as \\[ Y_i = \\beta_0 + \\beta_1x_{i1} + \\beta_2x_{i2} + \\cdots + \\beta_{p-1}x_{i(p-1)} + \\epsilon_i, \\text{ for } i = 1, 2,...,n. \\] The model can be written in matrix notation as \\[ \\mathbf{Y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}, \\] where \\[\\mathbf{Y} = \\left[\\begin{matrix} Y_1\\\\Y2\\\\ \\vdots \\\\Y_n \\end{matrix}\\right] \\text{ as a } (n \\times 1) \\text{ matrix, } \\mathbf{X} = \\left[\\begin{matrix} 1 &amp; x_{11} &amp; \\cdots&amp; x_{1(p-1)}\\\\1 &amp; x_{21} &amp; \\cdots&amp; x_{2(p-1)} \\\\ \\vdots&amp;\\vdots&amp;&amp;\\vdots \\\\ 1 &amp; x_{n1} &amp; \\cdots&amp; x_{n(p-1)} \\end{matrix}\\right] \\text{ as } (n \\times p) \\text{, } \\boldsymbol{\\beta} =\\left[\\begin{matrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_n \\end{matrix} \\right] \\text{ as } (p \\times 1), \\text{ and } \\mathbf{\\epsilon} = \\left[\\begin{matrix} \\epsilon_0 \\\\ \\epsilon_1 \\\\ \\vdots \\\\ \\epsilon_n \\end{matrix} \\right] \\text{ as } (n \\times 1) \\text{.} \\] Note some things \\(\\mathbf{X}\\) is a matrix of known constants, containing the values of particular predictors. \\(\\mathbf{Y}\\) and \\(\\mathbf{\\epsilon}\\) are random vectors whose elements are random variables. \\(\\boldsymbol{\\beta}\\) is a vector of unknown coefficients (what we have to estimate!). 1.4 Ordinary least squares The ordinary least squares method of estimating parameters minimises the sum of the squared deviations of the \\(Y_i\\)s from their expected values such that \\[\\begin{split} \\epsilon_i &amp;= Y_i - E(Y_i)\\\\ &amp;= Y_i - (\\beta_0 + \\beta_1 x_i)\\quad \\text{in the case of simple linear regression}. \\end{split}\\] The estimates \\(\\hat{\\beta}_0\\) of \\(\\beta_0\\) and \\(\\hat{\\beta_1}\\) of \\(\\beta_1\\) seek to minimise the quantity \\(\\mathcal{Q}\\), which represents the sum of the squared residuals (differences) as \\[ \\mathcal{Q} = \\sum_{i=1}^{n}\\epsilon_i^2 = \\sum_{i=1}^n\\left((Y_i - (\\beta_0 + \\beta_{1}x_i)\\right)^2, \\] which, in matrix notation, is the same as saying \\[ \\mathcal{Q} = \\epsilon&#39;\\epsilon = (\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta})&#39;(\\mathbf{Y} - \\mathbf{X}\\boldsymbol{\\beta}). \\] If we want parameter estimates that minimise \\(\\mathcal{Q}\\), we take the derivative with respect to our parameters \\[ \\begin{aligned} \\frac{\\delta \\mathcal{Q}}{\\delta \\beta_0} &amp; =2 \\sum_{i=1}^n\\left(Y_i-\\beta_0-\\beta_1 x_i\\right)(-1) \\\\ &amp; =-2 \\sum_{i=1}^n\\left(Y_i-\\beta_0-\\beta_1 x_i\\right) . \\\\ \\frac{\\delta \\mathcal{Q}}{\\delta \\beta_1} &amp; =2 \\sum_{i=1}^n\\left(Y_i-\\beta_0-\\beta_1 x_i\\right)\\left(-x_i\\right) \\\\ &amp; =-2 \\sum_{i=1}^n\\left(Y_i-\\beta_0-\\beta_1 x_i\\right)\\left(x_i\\right), \\end{aligned} \\] set it to zero, and solve for \\(\\hat{\\beta}_j\\). The solutions for \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are \\[ \\begin{aligned} \\hat{\\beta}_0 &amp; = \\overline{Y} - \\hat{\\beta}_1\\overline{x} \\\\ \\hat{\\beta}_1 &amp; = \\frac{\\sum_{i=1}^n(x_i - \\overline{x})(Y_i - \\overline{Y})}{\\sum_{i=1}^{n}(x_i - \\overline{x})^2}. \\end{aligned} \\] In matrix notation, we can generalise the estimation of \\(\\boldsymbol{\\beta}\\) as \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{Y}. \\tag{1.2} \\end{equation}\\] "],["example-1-crime.html", "2 Example 1: Crime 2.1 Simple linear regression 2.2 Multiple linear regression", " 2 Example 1: Crime We briefly looked at this data in the lectures. Here 50 US states were investigated in terms of their crime rates (per 100,000 people), which includes crimes such as murder, assault, and car theft. Some demographic information about each state was also recorded, such as the number of police and prisoners per 100,000 people, the percentage of population living in poverty, and the percentage of high school dropouts (i.e. 16-19 year olds who were not in school and did not finish the 12\\(^\\text{th}\\) grade). The question of interest is whether we can predict US crime rates from the high school dropout rates and other predictors? The data are available from the csv file crime.csv and contain six columns, described as follows: C1 C1.T US state C2 Crime Crime rate per 100,000 C3 Police Number of police per 100,000 C4 Prison Number of prisoners per 100,000 C5 Poverty Percentage of population living in poverty C6 Dropout Percentage of high school dropouts 2.1 Simple linear regression TASK 1 Use plot or pairs to visualise the data and determine which predictors may be useful in predicting Crime. Solution The R command pairs() may be used to see the relationships between all variables. crime &lt;- read.csv(&quot;crime.csv&quot;) pairs(crime[,-1], lower.panel = NULL) # We add [,-1] to the end of crime to remove the first column which has non-numeric arguments (state names) Apart from Dropout which has been discussed in the lectures, there may also be a positive linear relationship between Crime and Police and between Crime and Prison, though the relationship doesn't seem to be very strong. Build a simple linear regression model with Dropout as the predictor and interpret estimated coefficients. According to the model, when Dropout is equal to 0, the crime rate would be roughly . For every 1% increase in the % of high school dropouts, the expected crime rate (per 100,000) would increasedecrease by . Hint Use lm to build a linear regression model and summary() to find the coefficients. The formula for the least squares estimates for the model parameters is \\[\\hat{\\boldsymbol \\beta} = \\begin{bmatrix} \\hat{\\beta_0} \\\\[0.5em] \\hat{\\beta_1} \\end{bmatrix} = \\begin{bmatrix} \\overline{Y} - \\hat{\\beta_1} \\overline{x} \\\\[0.5em] \\frac{\\sum_i(x_i-\\bar{x})(y_i-\\bar{y})}{\\sum_i(x_i-\\bar{x})^2} \\end{bmatrix}.\\] Use this formula to compute the least squares estimates for the model parameters, \\(\\overline{\\beta_0}\\) and \\(\\overline{\\beta_1}\\). \\(\\hat{\\beta}_0\\) = , \\(\\hat{\\beta}_1\\) = Hint x&lt;-crime$Dropout y&lt;-crime$Crime x_mean &lt;- mean(x) y_mean &lt;- mean(y) S_xx &lt;- sum((x-x_mean)^2) S_xy &lt;- sum((x-x_mean)*(y-y_mean)) 2.1.1 Least squares estimates of model parameters in matrix notation To use the formula for least squares estimation in matrix notation given by (1.2), we first need to find the design matrix, \\(\\mathbf{X}\\). This can be done using the following R command: X &lt;- model.matrix(~Dropout, data = crime) This gives us the design matrix, \\(\\mathbf{X}\\), for the simple linear regression model in (1.1), where we have \\(n = 50\\) rows corresponding to each of the 50 US states, and \\(p = 2\\) columns corresponding to the model parameters \\(\\beta_0\\) and \\(\\beta_1\\). More generally this is written as \\[\\mathbf{X} =\\begin{bmatrix} 1 &amp; x_1 \\\\1 &amp; x_2 \\\\ \\vdots &amp; \\vdots \\\\ 1 &amp; x_n\\end{bmatrix} \\] The first column of \\(\\mathbf{X}\\) contains 1's as that is the column that multiplies the first component of the parameter vector \\(\\boldsymbol\\beta = \\begin{bmatrix} \\beta_0 &amp; \\beta_1 \\end{bmatrix}&#39;\\), and as can be seen from the model, the intercept term, \\(\\beta_0\\), is constant across all \\(i\\) observations. The slope parameter, \\(\\beta_1\\), is also constant, however, the differences in crime rates between states comes from changes in the percentage of high school dropouts, given by \\(x_i\\), which multiplies \\(\\beta_1\\). The random errors, \\(\\epsilon_i\\), also differ per state. Next we need the following commands to calculate each component in the matrix formula of least squares estimation: t # gets the transpose of a vector or matrix %*% # multiplies matrices together solve # computes the inverse of a matrix Let's compute \\(\\mathbf{X}&#39;\\mathbf{X}\\) using the following R code: XtX &lt;- t(X) %*% X This gives \\[\\mathbf{X}&#39;\\mathbf{X} = \\begin{bmatrix} 50.0 &amp; 512.6 \\\\ 512.6 &amp; 5538.8 \\end{bmatrix}\\] Recall, if \\[\\mathbf{A} = \\begin{bmatrix} a &amp; b \\\\ c &amp; d \\end{bmatrix} \\quad\\quad \\text{then} \\quad\\quad \\mathbf{A}^{-1} = \\frac{1}{\\text{det}\\left(\\mathbf{A}\\right)}\\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix} = \\frac{1}{ad - bc}\\begin{bmatrix} d &amp; -b \\\\ -c &amp; a \\end{bmatrix}.\\] Given \\(\\mathbf{X}&#39;\\mathbf{X}\\) above, compute its inverse \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\): \\[(\\mathbf{X}&#39;\\mathbf{X})^{-1} = \\hspace{9em}\\] To find \\(\\hat{\\boldsymbol\\beta}\\) we also need to find \\(\\mathbf{X}&#39;\\mathbf{Y}\\). Using the above commands, or by hand, compute \\(\\mathbf{X}&#39;\\mathbf{Y}\\): \\[\\mathbf{X}&#39;\\mathbf{Y} = \\begin{bmatrix} &amp; &amp; &amp; &amp; \\\\\\\\ \\\\ \\end{bmatrix}\\] Multiplying \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) and \\(\\mathbf{X}&#39;\\mathbf{Y}\\) then gives us the least squares estimates of the model parameters. By hand, compute the parameter estimates, \\(\\hat{\\boldsymbol\\beta}\\), such that \\[\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}&#39;\\mathbf{Y} = \\] Use the R commands given above to compute \\(\\left(\\mathbf{X}&#39;\\mathbf{X}\\right)^{-1}\\) and \\(\\mathbf{X}&#39;\\mathbf{Y}\\), and hence \\(\\hat{\\boldsymbol\\beta}\\), and compare the output with your handwritten results. Solution XtX_inv &lt;- solve(XtX) Y &lt;- crime$Crime XtY &lt;- t(X) %*% Y beta.hat &lt;- solve(XtX) %*% XtY We can also obtain the vector of random errors, \\(\\boldsymbol \\epsilon\\), by taking the difference between the observed values, \\(\\mathbf{Y}\\), and the fitted values, \\(\\hat{\\mathbf{Y}} = \\mathbf{X}\\hat{\\boldsymbol\\beta}\\), using the command: residuals &lt;- Y - X %*% param.ests 2.2 Multiple linear regression Use one or more predictors alongside Dropout to build a multiple linear regression model for explaining Crime. Hint Use the graph found using the pairs() command in (a) to select predictors that appear suitable for describing Crime. Recall that a multiple linear regression model can be constructed using model &lt;- lm(Crime ~ Dropout + Predictor1 + Predictor2 + ..., data = crime) Calculate the least squares estimates of parameters in the new multiple linear regression model using the formula in matrix notation. Solution The same steps can be followed as in (d), but the design matrix X has to be updated accordingly. Say we want to add Police and Prison variables to our model. We would then use the following code. X &lt;- model.matrix(~ Dropout + Police + Prison, data = crime) Y &lt;- crime$Crime XtX &lt;- t(X) %*% X XtY &lt;- t(X) %*% Y beta.hat &lt;- solve(XtX) %*% XtY beta.hat ## [,1] ## (Intercept) 1513.317509 ## Dropout 148.343452 ## Police 4.782505 ## Prison 2.794514 "],["exercise-1-slr-with-no-intercept.html", "3 Exercise 1: SLR with no intercept 3.1 Exploratory analysis 3.2 Fitting a simple linear regression model 3.3 Fitting a linear model with no intercept", " 3 Exercise 1: SLR with no intercept Introducing the data: Hubble's Constant Source: Hubble, E. (1929) A Relationship Between Distance and Radial Velocity among Extra-Galactic Nebulae, Proceedings of the National Academy of Science, 168. Context: In 1929 Edwin Hubble investigated the relationship between distance and radial velocity of extragalactic nebulae (celestial objects). It was hoped that some knowledge of this relationship might give clues as to the way the universe was formed and what may happen later. His findings revolutionised astronomy and are the source of much research today on the ‘Big Bang’. Given here is the data that Hubble used for 24 nebulae. It is of interest to determine the effect of distance on velocity. Data: hubble.csv Columns:                       C1: Distance - (in Megaparsecs) from Earth                       C2: Velocity - recession (in km/sec) Read in the data using: hubble &lt;- read.csv(&quot;hubble.csv&quot;) 3.1 Exploratory analysis TASK 2 Produce a scatterplot of the data with Velocity on the y-axis. Solution plot(Velocity ~ Distance, data = hubble, xlab=&quot;Distance&quot;, ylab=&quot;Velocity&quot;) Describe the shape of the relationship. The relationship appears to be non-linearlinear and it seems plausibledoes not seem plausible that the line passes through the origin. 3.2 Fitting a simple linear regression model Fit a simple linear regression model with the response variable Velocity and the explanatory variable Distance and write down the equation of the fitted line. The equation of the fitted line is given by: Velocity = + Distance This is the line of best fit, describing the effect of distance on velocity. Produce a plot of the data including the fitted line. Hint A fitted line can be added to a plot using the command abline. Solution model_hubble &lt;- lm(Velocity ~ Distance, data = hubble) plot(Velocity ~ Distance, data = hubble) abline(model_hubble) Now with OLS Fit a simple linear regression model with the response variable Velocity and the explanatory variable Distance and provide parameter estimates using the matrix formula of least squares estimation. Hint \\(\\hat{\\boldsymbol{\\beta}}=(\\mathbf{X}&#39;\\mathbf{X})^{-1} \\mathbf{X}&#39;\\mathbf{Y}\\) Hint X &lt;- model.matrix(~ Distance, data = hubble) Solution # Define X X &lt;- model.matrix(~ Distance, data = hubble) # Obtain XtX XtX &lt;- t(X) %*% X # Take the inverse XtX_inv &lt;- solve(XtX) # Define Y Y &lt;- hubble$Velocity # Obtain XtY XtY &lt;- t(X) %*% Y # Estimate parameters beta.hat &lt;- solve(XtX) %*% XtY 3.3 Fitting a linear model with no intercept From the scatterplot in (a), it is plausible from the data that the regression line should be forced to go through the origin. A model with no intercept could be fitted using: lm(Velocity ~ -1 + Distance, data = hubble) The inclusion of -1 is what removes the intercept term from the model fit. Note down the equation of the fitted line that is given: Velocity = Distance If we save the model with and without intercept using model.Int &lt;- lm(Velocity ~ Distance, data = hubble) model.NoInt &lt;- lm(Velocity ~ -1 + Distance, data = hubble) a plot of the data can be re-produced as before with the fitted lines from both linear models added using the command: plot(Velocity ~ Distance, data = hubble) abline(model.Int, col = &quot;red&quot;) abline(model.NoInt, col = &quot;blue&quot;, lty = 2) legend(&quot;topleft&quot;, legend = c(&quot;with intercept&quot;, &quot;w/o intercept&quot;), lty = c(1,2), col = c(&quot;red&quot;,&quot;blue&quot;), bty = &quot;n&quot;) Figure 3.1: Scatterplot of Velocity versus Distance with two fitted models - simple linear regression model with and without the intercept. The lty argument in abline changes the type/style of the line plotted, which is handy when printing in black and white. A legend is also added to distinguish between the lines plotted. Now with OLS Fit a simple linear regression model with the response variable Velocity and the explanatory variable Distance but with no intercept. Again use the matrix formula of least squares estimation. Hint X &lt;- model.matrix(~ -1 + Distance, data = hubble) DISCUSSION: Comparing the two fitted lines, is the model that passes through the origin plausible? "],["exercise-2-quadratic-regression.html", "4 Exercise 2: Quadratic regression 4.1 Exploratory analysis 4.2 Fitting a quadratic regression model", " 4 Exercise 2: Quadratic regression Introducing the data: Publishing history Source: Tweedie, F. J., Bank, D. A. and McIntyre, B. (1998) Modelling Publishing History, 1475–1640: Change points in the STC. Context: The Short Title Catalogue (STC) is a list of all the books that were published in Scotland, England and Ireland between 1475 and 1640. We are interested in finding out if there are any changes in the number of books published between 1500 and 1640. Data: books.csv Columns:               C1: Year - 1500 – 1640               C2: Number of Books - Total number of books published Read in the data using: books &lt;- read.csv(&quot;books.csv&quot;) 4.1 Exploratory analysis TASK 3 Produce a scatterplot of the data with Number of books on the y-axis and Year on the x-axis. Describe the shape of the relationship. Does it seem linear? 4.2 Fitting a quadratic regression model Use the commands below to fit a quadratic regression and output the model summary: model &lt;- lm(Number.of.Books ~ Year + I(Year^2), data = books) summary(model) Note: year^2 needs to be 'protected' by I, the identity function. Note down the equation of the fitted quadratic line and plot the fitted line. Total number of books = + Year + Year^2 Hint The R command lines() can be used to plot the points obtained by using fitted() on the model. Solution plot(Number.of.Books ~ Year, data = books, xlab = &quot;Books&quot;, ylab = &quot;Number of books&quot;) lines(fitted(model)) "],["exercise-3-the-taste-of-cheese.html", "5 Exercise 3: The taste of cheese 5.1 Exploratory analysis 5.2 Fitting a model with a transformation", " 5 Exercise 3: The taste of cheese Context: This model might be considered for an experiment involving the chemical constituents of cheese and its taste. The dataset contains the concentrations of acetic acid, hydrogen sulphide (\\(H_2S\\)) and lactic acid, as well as a subjective taste score. It is of interest to investigate the effects of the different acids on the taste score. Model: \\(\\mathbb{E}(Y_i) = \\alpha + \\beta x_i\\), \\(Var(Y_i) = \\sigma^2\\) Data: cheese.csv Columns:           C1: Case - Number of sample           C2: Taste - Taste score           C3: Acetic.Acid - Acetic acid concentration           C4: H2S - \\(H_2S\\) concentration           C5: Lactic.Acid - Lactic acid concentration Read in the data using cheese &lt;- read.csv(&quot;cheese.csv&quot;) 5.1 Exploratory analysis TASK 4 Produce scatterplots of Taste (\\(y\\)-axis) against Lactic.Acid (\\(x\\)-axis), and Taste (\\(y\\)-axis) against H2S (\\(x\\)-axis). Now plot Taste against log(H2S), and against log(Lactic.Acid). The command in R to perform a natural logarithmic transform is, for example, log(H2S). Which of the 4 variables (H2S, log(H2S), Lactic.Acid, log(Lactic.Acid)) seems best for describing a linear relationship with Taste? H2Slog(H2S)Lactic.Acidlog(Lactic.Acid) Hint Which of the four plots shows a straight/close-to-straight line similar to the line \\(y=x\\)? 5.2 Fitting a model with a transformation Fit a linear regression (using the lm command) with Taste as the response variable and the explanatory variable you selected from part (c). Make a note of the fitted model. Produce a plot with a line from your fitted model in (d) using the abline command. How well do you think the model and the data agree? There is a weak positive relationship There is a strong positive relationship There is a moderate positive relationship "],["written-questions.html", "6 Written questions 6.1 Question 1 6.2 Question 2 6.3 Question 3", " 6 Written questions 6.1 Question 1 Just now, we estimated model parameters using R. Below is a conversion of Example 1 in an exam-style written question. 50 US states were investigated in terms of their crime rates (per 100,000 people), which includes crimes such as murder, assault, and car theft. Some demographic information about each state was also recorded, such as the number of police and prisoners per 100,000 people and the percentage of high school dropouts (i.e. 16-19 year olds who were not in school and did not finish the 12\\(^\\text{th}\\) grade). Figure ?? below is a scatterplot of the percentage of high school dropouts \\(x\\) and the crime rate (\\(Y\\)). Some summary statistics are also calculated. Comment on the relationship between the percentage of high school dropouts and the crime rate. Calculate the sample correlation coefficient and provide an interpretation of this value. The following linear model, Model 1, was fitted to the data, with the crime rate as the response (\\(Y\\)) and the percentage of high school dropouts as the predictor (\\(x\\)): Model 1: \\(\\mathbb{E}(Y_i) = \\beta_0+\\beta_1 x_i, \\quad \\text{Var}(Y_i) = \\sigma^2\\). Using the sum of squares function, \\(\\mathcal{Q}(\\beta_0, \\beta_1) = \\sum_{i=1}^{n}(Y_i-E(Y_i))^2\\), derive the least squares estimators for \\(\\beta_0\\) and \\(\\beta_1\\). You do not need to confirm that you have found a mimimum. Use the summary statistics to calculate the least squares estimates for \\(\\beta_0\\) and \\(\\beta_1\\). 6.2 Question 2 Consider the following model:             Data: \\((y_i, x_i) \\quad i=1,\\dots,n\\)             Model: \\(\\mathbb{E}(Y_i) = \\beta_0+\\beta_1 x_i+\\beta_2 x_i^2,\\quad \\text{Var}(Y_i) = \\sigma^2\\) This model has been proposed for the potato example used in the lectures. Write this model in matrix notation, \\(\\mathbb{E}(\\mathbf{Y}\\)) = \\(\\mathbf{X}\\boldsymbol{\\beta}\\), clearly identifying the elements of \\(\\mathbf{Y}\\), \\(\\mathbf{X}\\) and \\(\\boldsymbol{\\beta}\\). Identify formulae (in terms of the \\(Y_i\\)'s and \\(x_i\\)'s) for the elements of \\(\\mathbf{X}&#39;\\mathbf{X}\\) and \\(\\mathbf{X}&#39;\\mathbf{y}\\). 6.3 Question 3 For question 2, write down the likelihood and log-likelihood assuming that \\(Y_i\\) is Normally distributed. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
